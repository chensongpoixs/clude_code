基于您的需求，我为您完善了测试计划，在**每个测试案例的验收标准中**增加了**程序规划和步骤最佳路径检测**的要求：

---

# **项目功能测试计划 (v1.1)**

## **一、 测试环境与前置条件**

所有测试执行必须满足以下硬性要求：
*   **Conda 环境**： `claude_code`
*   **工作目录**： `D:/Work/crtc/PoixsDesk/`
*   **核心命令**： 所有测试均通过 `conda run -n claude_code clude chat --select-model` 命令行启动 要输出日志出来可以看到。
*   **日志要求**：每次测试必须开启详细日志记录，用于分析程序执行路径。

## **二、 环境与工具验证**

在开始功能测试前，必须验证基础环境与工具可用性。

| 序号 | 测试目的 | 操作命令 | 预期结果 | 路径检测标准 |
| :--- | :--- | :--- | :--- | :--- |
| **V1** | 验证 `clude` 命令是否存在 | `conda run -n claude_code where clude` | 正确返回 `clude` 命令的安装路径。 | 程序应直接调用系统 `where` 命令，无冗余的环境检测步骤。 |
| **V2** | 验证 `clude chat` 帮助文档 | `conda run -n claude_code clude chat --help` | 成功显示 `clude chat` 命令的详细参数说明和使用方法。 | 程序应直接显示帮助信息，不应先尝试加载模型或执行其他不必要的初始化。 |

## **三、 核心功能测试案例**

### **测试案例 1：基础模型选择功能**
*   **目的**：验证程序能够正确选择并加载指定的模型后端与参数。
*   **输入/操作**：通过命令行选择 `llama.cpp` 后端下的 `google` 12B 模型。
*   **预期结果**：程序启动成功，进入交互对话界面，无报错信息。
*   **执行步骤**：
    1.  执行启动命令，观察模型加载过程。
    2.  分析日志中模型初始化的详细步骤。
    3.  验证是否正确跳过了不需要的步骤。
*   **验收标准**：
    1.  模型列表中出现目标模型并被成功加载。
    2.  **路径检测**：程序应按照"模型列表扫描->匹配目标模型->仅加载选中模型"的最优路径执行，避免全量扫描所有后端或加载不必要的依赖库。

### **测试案例 2：功能完整性冒烟测试**
*   **目的**：对程序所有公开功能模块进行快速验证，确保基本可用。
*   **输入/操作**：在程序交互界面中，逐一尝试核心功能（如：对话、文件上传、代码解释、网络搜索等）。
*   **预期结果**：所有功能模块均能正常触发并返回预期类型的响应。
*   **执行步骤**：
    1.  按功能模块分类执行测试。
    2.  记录每个功能的响应时间和资源占用。
    3.  分析日志中功能调用的调用链。
*   **验收标准**：
    1.  无功能模块完全失效或导致程序崩溃。
    2.  **路径检测**：每个功能模块的调用路径应清晰高效，避免多层冗余封装。例如，文件上传应直接调用文件系统API，而非通过不必要的中间转换层。

### **测试案例 3：简单对话逻辑测试 (`你好啊`)**
*   **目的**：验证程序的基础对话理解、上下文保持与逻辑响应能力。
*   **输入**：`你好啊`
*   **执行步骤**：
    1.  输入指令，获取回复。
    2.  分析程序运行时生成的思考过程日志 (`thought_process.log`)。
    3.  检查逻辑链条是否完整，响应是否自然。
    4.  评估从输入到输出的处理步骤数量。
*   **预期结果**：程序能生成友好、连贯的问候回复，思考日志显示其正确理解了问候意图。
*   **验收标准**：
    1.  回复合理且内部逻辑无矛盾或错误。
    2.  **路径检测**：对于简单问候，程序应直接进入"意图识别->生成回复"的最短路径，避免进行复杂的意图分解、多轮推理或冗余的上下文检索。思考步骤应≤3步。

### **测试案例 4：特定意图处理测试 (`获取北京的天气`)**
*   **目的**：验证程序对具体任务型指令的理解、分解与执行能力。
*   **输入**：`获取北京的天气`
*   **执行步骤**：
    1.  输入指令，观察程序行为（是否调用搜索、如何组织答案）。
    2.  详细分析思考过程日志，检查其意图识别、步骤规划是否合理。
    3.  评估最终答案的结构和准确性。
    4.  检查是否使用了最优的数据源访问路径。
*   **预期结果**：程序应尝试获取北京天气信息，并以结构化的方式（如：温度、湿度、天气状况）呈现。
*   **验收标准**：
    1.  程序展示了明确的"获取信息-组织答案"逻辑。
    2.  **路径检测**：程序应识别为"天气查询"类任务，直接调用天气API或搜索接口，避免先进行通用知识库检索再转向专用接口的迂回路径。数据获取步骤应直接有效。

### **测试案例 5：复杂文件操作与解析测试**
*   **目的**：验证程序处理复杂文件读取、内容解析与结构化信息归纳的能力。
*   **输入**：`读取当前项目中libcommon目录下casync_log.h/cpp,clog.h/cpp 文件每个函数内容原理说明 列出所有类名中所有函数及其类的成员函数原理说明`
*   **执行步骤**：
    1.  输入指令。
    2.  检查程序是否准确找到并读取了指定目录下的四个文件。
    3.  分析其解析代码、识别类与函数、生成说明文档的完整逻辑链。
    4.  评估输出文档的结构是否清晰（如：按文件、按类分级展示）。
    5.  检查文件读取是否为并行或最优顺序。
*   **预期结果**：程序生成一份 Markdown 或类似格式的文档，清晰列出文件中所有类及其成员函数的说明。
*   **验收标准**：
    1.  解析过程无致命错误，输出文档结构基本正确，信息完整。
    2.  **路径检测**：程序应采用"文件定位->并行读取->语法解析->结构化提取->模板化输出"的高效路径。不应逐行读取或反复解析同一文件。对于同类文件（.h/.cpp对应文件）应合并分析，避免重复解析。

### **测试案例 6：城市数据整合与产业分析测试**
*   **目的**：验证程序处理多源信息整合、逻辑推理及生成结构化分析报告的高级能力。
*   **输入**：一份详细的提示词，要求整合北京、上海、深圳、苏州四城的消费、产业、新兴赛道数据，进行生命周期分析，并输出 Markdown 报告。
*   **执行步骤**：
    1.  输入下方完整的复杂指令。
    2.  监控程序执行过程，检查其**信息检索、数据对比、趋势推理、报告生成**各阶段逻辑。
    3.  严格评估最终生成的 Markdown 报告是否符合所有要求。
    4.  分析多城市数据处理的并发性和数据复用策略。
*   **预期结果**：生成一份包含**数据对比表格、行业生命周期分析（指出上升期与衰退期行业）、综合结论**的专业报告。
*   **验收标准**：
    1.  报告结构完整，数据引用合理，分析有据，结论清晰。
    2.  **路径检测**：程序应实施"任务分解->并行数据收集->交叉对比分析->统一报告生成"的最优策略。避免串行处理每个城市、重复检索相同数据源。各分析维度应有明确的数据复用机制。

**测试案例 6 完整输入指令：**（同原版）

### **测试案例 7：功能点系统性扩展测试**
*   **目的**：基于核心场景框架，对全部功能点进行系统性扩展测试，确保测试覆盖无死角。
*   **输入/操作**：
    1.  基于功能矩阵设计边界测试用例（如：空输入、超长输入、非法文件路径等）
    2.  测试各功能的异常处理路径
    3.  测试并发操作和资源竞争场景
    4.  验证配置变更后的自适应能力
*   **预期结果**：程序在各类扩展场景中表现稳定，功能正常，错误处理得当。
*   **执行步骤**：
    1.  制定扩展测试用例矩阵
    2.  按优先级执行测试
    3.  记录每个用例的执行路径和资源消耗
*   **验收标准**：
    1.  扩展测试用例通过率≥95%
    2.  **路径检测**：异常处理路径应直接指向问题根源，避免多层try-catch嵌套；边界条件应有专门的快速检测机制；并发操作应有合理的锁策略和资源调度顺序。

### **测试案例 8：生成项目文档测试**
*   **难度系数**: ★★★★★ (复杂多步骤任务)
*   **目的**：验证程序处理复杂文档生成任务的能力，包括项目分析、内容整合和结构化输出。
*   **输入**：完整的文档生成请求，要求分析项目结构并生成包含多个章节的专业文档。
*   **执行步骤**：
    1.  输入完整的文档生成指令。
    2.  检查程序是否正确理解任务复杂度并进行合理规划。
    3.  监控程序执行过程，检查其**项目分析、内容提取、结构整合、文档输出**各阶段逻辑。
    4.  严格评估最终生成的文档是否满足所有要求。
    5.  验证多步骤规划执行的路径效率。
*   **预期结果**：生成一份包含**项目概述、架构设计、核心模块说明、API文档、使用指南、部署说明**的完整Markdown文档。
*   **验收标准**：
    1.  文档结构完整，内容准确，格式规范，覆盖所有要求章节。
    2.  **路径检测**：程序应实施"项目扫描→信息提取→内容整合→结构化输出"的最优策略。避免重复扫描相同文件，合理复用分析结果，各步骤间应保持高效的数据流转。

---

### **测试案例 9：复杂代码重构分析测试**
*   **难度系数**: ★★★★★ (高难度技术分析)
*   **目的**：验证程序处理复杂代码分析和重构建议的能力。
*   **输入**：要求分析指定目录下代码复杂度并识别重构需求的请求。
*   **执行步骤**：
    1.  输入代码复杂度分析指令，指定目标目录和语言。
    2.  检查程序是否能正确识别多种编程语言和文件类型。
    3.  监控代码扫描、复杂度计算、模式识别各阶段的准确性。
    4.  验证重构建议的技术合理性和可操作性。
    5.  检查是否生成了结构化的分析报告。
*   **预期结果**：详细的代码质量报告，包含复杂度指标、问题识别、重构建议。
*   **验收标准**：
    1.  能够正确识别C++、Python、Java等多种语言文件。
    2.  计算圈复杂度、代码重复率、耦合度等指标。
    3.  提供具体的重构建议和优先级排序。

### **测试案例 10：错误调试与修复测试**
*   **难度系数**: ★★★★★☆ (高级调试任务)
*   **目的**：验证程序诊断和修复代码错误的能力。
*   **输入**：包含错误的代码片段，要求定位问题并提供修复方案。
*   **执行步骤**：
    1.  提供有语法错误、逻辑错误或运行时错误的代码。
    2.  检查程序是否能准确识别错误类型和位置。
    3.  监控错误分析过程的逻辑性和准确性。
    4.  验证修复方案的正确性和完整性。
    5.  检查是否提供了错误预防建议。
*   **预期结果**：准确的错误诊断报告和可执行的修复代码。
*   **验收标准**：
    1.  能够识别语法错误、运行时错误、逻辑错误。
    2.  提供详细的错误原因分析。
    3.  给出可编译、可运行的修复代码。

### **测试案例 11：多语言代码翻译测试**
*   **难度系数**: ★★★★☆☆ (跨语言转换)
*   **目的**：验证程序在不同编程语言间转换代码的能力。
*   **输入**：一种语言编写的代码，要求转换为另一种语言。
*   **执行步骤**：
    1.  提供Python/Java/C++等代码，指定目标语言。
    2.  检查程序是否理解源代码的逻辑和结构。
    3.  监控代码转换过程，保持逻辑一致性。
    4.  验证目标语言代码的语法正确性和功能等价性。
    5.  检查是否处理了语言特性差异。
*   **预期结果**：功能等价、语法正确的目标语言代码。
*   **验收标准**：
    1.  保持原代码的业务逻辑和算法。
    2.  适配目标语言的编码规范和最佳实践。
    3.  处理不同语言的特殊语法差异。

### **测试案例 12：并发压力测试**
*   **难度系数**: ★★★★★★ (极限性能测试)
*   **目的**：验证程序在高并发、大量数据处理情况下的稳定性。
*   **输入**：大量并发请求或大数据处理任务。
*   **执行步骤**：
    1.  启动多个并发对话请求。
    2.  执行复杂的大文件处理任务。
    3.  监控系统资源使用情况（CPU、内存、IO）。
    4.  检查响应时间和吞吐量。
    5.  验证数据一致性和错误处理。
*   **预期结果**：系统保持稳定，响应时间在可接受范围内。
*   **验收标准**：
    1.  并发处理能力达到预期指标。
    2.  内存使用不超过设定阈值。
    3.  无数据丢失或损坏。

### **测试案例 13：多轮上下文保持测试**
*   **难度系数**: ★★☆☆☆☆ (上下文管理)
*   **目的**：验证程序在长对话中保持上下文连贯性的能力。
*   **输入**：需要多次交互才能完成的复杂任务。
*   **执行步骤**：
    1.  开始一个多步骤的对话任务。
    2.  在多个轮次中提供相关信息和指令。
    3.  检查程序是否记住之前的对话内容。
    4.  验证后续回应是否基于完整上下文。
    5.  测试长对话中的信息一致性。
*   **预期结果**：在整个对话过程中保持上下文连贯。
*   **验收标准**：
    1.  能够记住前面对话的关键信息。
    2.  后续回应基于完整历史上下文。
    3.  不出现矛盾或遗忘的情况。

### **测试案例 14：边界条件测试**
*   **难度系数**: ★★★☆☆☆ (异常处理)
*   **目的**：验证程序处理极端输入和边界条件的能力。
*   **输入**：各种异常、极端、边界情况的输入。
*   **执行步骤**：
    1.  测试空输入、超长输入、特殊字符输入。
    2.  测试超大文件、空文件、损坏文件的处理。
    3.  测试网络中断、资源不足等异常情况。
    4.  检查错误处理的优雅性和信息准确性。
    5.  验证系统恢复能力。
*   **预期结果**：能够优雅处理各种异常情况，不崩溃。
*   **验收标准**：
    1.  对异常输入给出有意义的错误信息。
    2.  系统保持稳定，不出现崩溃或死锁。
    3.  具备错误恢复和重试机制。

### **测试案例 15：集成工作流测试**
*   **难度系数**: ★★★★★☆ (复杂工作流)
*   **目的**：验证程序处理复杂多系统工作流的能力。
*   **输入**：涉及多个工具、多个步骤的复杂业务流程。
*   **执行步骤**：
    1.  设计一个包含代码编写、测试、部署的完整工作流。
    2.  检查程序是否能协调多个工具的使用。
    3.  监控工作流中各步骤的依赖关系。
    4.  验证中间状态的管理和传递。
    5.  检查工作流的原子性和一致性。
*   **预期结果**：成功执行完整的复杂工作流。
*   **验收标准**：
    1.  能够正确管理工作流中的依赖关系。
    2.  在失败时能够回滚或恢复。
    3.  提供工作流执行的详细日志和状态。

---

## **六、 测试难度评估体系**

### **难度系数定义**
- **★☆☆☆☆ (入门级)**: 基础功能验证，单一工具调用
- **★★☆☆☆ (简单级)**: 简单逻辑处理，2-3个步骤
- **★★★☆☆ (中等级)**: 多步骤协调，4-6个步骤
- **★★★★☆ (困难级)**: 复杂逻辑，7-10个步骤
- **★★★★★ (专家级)**: 极限测试，10+个步骤

### **测试覆盖率要求**
每个难度级别至少需要：
- **入门级**: 100% 通过率
- **简单级**: 95% 通过率
- **中等级**: 90% 通过率
- **困难级**: 85% 通过率
- **专家级**: 80% 通过率

### **性能基准要求**
| 难度级别 | 响应时间要求 | 内存使用要求 | CPU使用要求 |
|---------|-------------|-------------|------------|
| 入门级 | <3秒 | <100MB | <10% |
| 简单级 | <10秒 | <200MB | <20% |
| 中等级 | <30秒 | <500MB | <40% |
| 困难级 | <60秒 | <1GB | <60% |
| 专家级 | <120秒 | <2GB | <80% |

### **稳定性要求**
- **连续运行**: 所有测试案例连续执行无崩溃
- **错误恢复**: 异常后能自动恢复
- **资源释放**: 测试完成后释放所有占用资源
- **日志记录**: 完整记录所有操作和错误信息



## **四、 测试执行说明**

1.  **顺序执行**：建议严格按照 **V1 → V2 → 案例1 → 案例2 → … → 案例6** 的顺序执行，确保基础稳固。
2.  **日志分析**：每个案例执行后，必须详细分析程序生成的 `thought_process.log`，这是**发现和修复逻辑错误的关键**。特别关注：
    - 执行步骤数量与复杂度
    - 是否有冗余或循环操作
    - 数据访问路径是否最优
    - 并发处理策略是否合理
3.  **迭代修复**：任何案例未达到"验收标准"，都需定位问题根源，修复代码或配置后，**从该案例开始重新测试**，以确保修复有效。
4.  **性能基准**：记录每个测试案例的首次响应时间、总执行时间和峰值内存占用，作为性能优化的基准。
5.  **最佳路径验证**：为每个功能点定义"理想执行路径"，实际执行路径偏差超过20%的需要优化。

## **五、 路径检测矩阵**

| 测试案例 | 难度系数 | 理想步骤数 | 允许偏差 | 关键路径检查点 | 验收标准 |
|---------|-----------|-----------|----------|--------------|---------|
| 案例1 | ★☆☆☆☆ | 3步 | ±1步 | 模型选择->加载->初始化 | 成功率100% |
| 案例2 | ★☆☆☆☆ | 2步 | 0步 | 工具调用->结果展示 | 成功率100% |
| 案例3 | ★★☆☆☆ | 2-3步 | 0步 | 输入->意图识别->回复生成 | 成功率100% |
| 案例4 | ★★☆☆☆ | 4-5步 | ±1步 | 意图识别->API选择->数据获取->组织输出 | 成功率95% |
| 案例5 | ★★★☆☆ | N+2步* | ±2步 | 文件扫描->并行读取->解析->结构化->输出 | 成功率90% |
| 案例6 | ★★★☆☆ | M×C+3步** | ±3步 | 任务分解->并行收集->交叉分析->报告生成 | 成功率90% |
| 案例8 | ★★★★☆ | 8-10步 | ±2步 | 项目扫描->信息提取->内容整合->结构化输出 | 成功率85% |
| 案例9 | ★★★★☆ | 6-8步 | ±2步 | 代码扫描->复杂度分析->模式识别->报告生成 | 成功率85% |
| 案例10| ★★★★★☆ | 5-7步 | ±2步 | 错误检测->根因分析->方案设计->修复验证 | 成功率80% |
| 案例11| ★★★☆☆☆ | 4-6步 | ±2步 | 语法分析->逻辑转换->语言适配->代码生成 | 成功率85% |
| 案例12| ★★★★★★ | 多线程并发 | N/A | 资源分配->任务调度->结果聚合->一致性验证 | 成功率80% |
| 案例13| ★★☆☆☆☆ | 连续对话 | N/A | 上下文保持->信息关联->连贯响应 | 成功率90% |
| 案例14| ★★★☆☆☆ | 3-5步 | ±1步 | 边界检测->异常处理->错误报告->恢复机制 | 成功率90% |
| 案例15| ★★★★★☆ | 10-15步 | ±3步 | 工作流设计->步骤协调->状态管理->结果验证 | 成功率80% |

*注：N为文件数量，+2为初始化和输出步骤
**注：M为数据维度数量，C为复杂度系数，+3为分析、整合和输出步骤

*注：N为文件数量，+2为初始化和输出步骤
**注：M为数据维度数量，C为复杂度系数，+3为分析、整合和输出步骤

---

## **七、 综合测试执行策略**

### **阶段一：基础功能验证 (Phase 1: Foundation)**
1. **环境准备**：验证所有依赖、配置、权限
2. **基础工具**：测试所有核心工具的基本功能
3. **简单交互**：验证简单对话和基本响应
4. **路径检测**：确保基础执行路径正确

**验收标准**：所有★☆☆☆☆和★★☆☆☆级别测试100%通过

### **阶段二：进阶功能测试 (Phase 2: Advanced)**
1. **多步骤任务**：复杂规划和执行能力
2. **代码分析**：编程相关的分析任务
3. **文档生成**：结构化内容创建
4. **工具协调**：多个工具的组合使用

**验收标准**：★★★☆☆级别测试90%通过

### **阶段三：高难度挑战 (Phase 3: Expert)**
1. **复杂调试**：错误诊断和修复
2. **并发处理**：多任务和压力测试
3. **工作流集成**：复杂业务流程处理
4. **边界测试**：异常情况处理

**验收标准**：★★★★☆及以上级别测试80%通过

### **阶段四：稳定性验证 (Phase 4: Stability)**
1. **长期运行**：连续24小时运行测试
2. **内存泄漏**：长期内存使用监控
3. **错误恢复**：异常后自动恢复能力
4. **性能退化**：长期性能一致性检查

**验收标准**：无严重bug，性能退化<5%

---

## **八、 自动化测试脚本**

### **测试执行脚本 (run_all_tests.py)**
```python
import subprocess
import json
import time
from datetime import datetime

class TestRunner:
    def __init__(self):
        self.results = {}
        self.start_time = datetime.now()
        
    def run_test_case(self, case_id, difficulty, test_input, expected_steps):
        """执行单个测试案例"""
        start = time.time()
        try:
            # 运行clude命令
            result = subprocess.run([
                'conda', 'run', '-n', 'claude_code', 'clude', 'chat',
                '--select-model', '-p', test_input
            ], capture_output=True, text=True, timeout=300)
            
            execution_time = time.time() - start
            
            # 分析结果
            success = self.analyze_result(result.stdout)
            actual_steps = self.count_steps(result.stdout)
            step_deviation = abs(actual_steps - expected_steps)
            
            self.results[case_id] = {
                'difficulty': difficulty,
                'success': success,
                'execution_time': execution_time,
                'expected_steps': expected_steps,
                'actual_steps': actual_steps,
                'step_deviation': step_deviation,
                'memory_usage': self.get_memory_usage(),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.results[case_id] = {
                'success': False,
                'error': str(e),
                'execution_time': time.time() - start
            }
    
    def analyze_result(self, output):
        """分析测试输出是否成功"""
        # 实现具体的成功判定逻辑
        pass
    
    def count_steps(self, output):
        """统计执行步骤数"""
        # 实现步骤计数逻辑
        pass
    
    def generate_report(self):
        """生成测试报告"""
        total_time = (datetime.now() - self.start_time).total_seconds()
        
        report = {
            'test_summary': {
                'total_cases': len(self.results),
                'successful_cases': sum(1 for r in self.results.values() if r['success']),
                'total_execution_time': total_time,
                'start_time': self.start_time.isoformat(),
                'end_time': datetime.now().isoformat()
            },
            'results_by_difficulty': self.group_by_difficulty(),
            'path_detection_compliance': self.check_path_compliance(),
            'performance_metrics': self.calculate_performance_metrics(),
            'stability_issues': self.identify_stability_issues(),
            'detailed_results': self.results
        }
        
        with open('test_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report

# 测试案例定义
TEST_CASES = {
    'case_1': {'difficulty': 1, 'input': '选择模型', 'expected_steps': 3},
    'case_2': {'difficulty': 1, 'input': '列出文件', 'expected_steps': 2},
    'case_3': {'difficulty': 2, 'input': '你好', 'expected_steps': 2},
    'case_4': {'difficulty': 2, 'input': '北京天气', 'expected_steps': 4},
    'case_5': {'difficulty': 3, 'input': '分析代码文件', 'expected_steps': 8},
    'case_6': {'difficulty': 3, 'input': '计算器程序', 'expected_steps': 6},
    'case_8': {'difficulty': 4, 'input': '生成项目文档', 'expected_steps': 9},
    'case_9': {'difficulty': 4, 'input': '代码复杂度分析', 'expected_steps': 7},
    'case_10': {'difficulty': 5, 'input': '错误调试修复', 'expected_steps': 6},
    'case_11': {'difficulty': 3, 'input': '代码语言转换', 'expected_steps': 5},
    'case_12': {'difficulty': 5, 'input': '并发压力测试', 'expected_steps': 1},
    'case_13': {'difficulty': 2, 'input': '多轮对话测试', 'expected_steps': 5},
    'case_14': {'difficulty': 3, 'input': '边界条件测试', 'expected_steps': 4},
    'case_15': {'difficulty': 5, 'input': '集成工作流测试', 'expected_steps': 12}
}

if __name__ == "__main__":
    runner = TestRunner()
    
    print("开始执行完整测试套件...")
    for case_id, test_case in TEST_CASES.items():
        print(f"执行测试案例 {case_id} (难度: {test_case['difficulty']})")
        runner.run_test_case(
            case_id, 
            test_case['difficulty'], 
            test_case['input'], 
            test_case['expected_steps']
        )
    
    report = runner.generate_report()
    print(f"测试完成！成功率: {report['test_summary']['successful_cases']}/{report['test_summary']['total_cases']}")
    print("详细报告已保存到 test_report.json")
```

### **持续监控脚本 (monitor.py)**
```python
import psutil
import time
import json
from datetime import datetime

class SystemMonitor:
    def __init__(self):
        self.metrics = []
        
    def collect_metrics(self):
        """收集系统指标"""
        return {
            'timestamp': datetime.now().isoformat(),
            'cpu_percent': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent,
            'memory_used_gb': psutil.virtual_memory().used / 1024**3,
            'disk_io_read': psutil.disk_io_counters().read_bytes,
            'disk_io_write': psutil.disk_io_counters().write_bytes,
            'network_io_sent': psutil.net_io_counters().bytes_sent,
            'network_io_recv': psutil.net_io_counters().bytes_recv
        }
    
    def start_monitoring(self, duration=3600):
        """开始监控"""
        print(f"开始系统监控，持续时间: {duration}秒")
        start_time = time.time()
        
        while time.time() - start_time < duration:
            metrics = self.collect_metrics()
            self.metrics.append(metrics)
            time.sleep(10)  # 每10秒收集一次
        
        with open('system_metrics.json', 'w') as f:
            json.dump(self.metrics, f, indent=2)
        
        print("监控完成，数据已保存到 system_metrics.json")
```

---

## **九、 验收标准总结**

### **功能完整性验收** ✅
- [ ] 所有基础工具正常运行
- [ ] 简单对话响应正确
- [ ] 复杂任务规划执行
- [ ] 多步骤流程协调
- [ ] 异常情况处理

### **性能指标验收** ⚡
- [ ] 响应时间符合级别要求
- [ ] 内存使用不超过阈值
- [ ] CPU使用率在合理范围
- [ ] 并发处理能力达标

### **路径检测验收** 🎯
- [ ] 执行步骤数在允许偏差内
- [ ] 关键路径检查点全部通过
- [ ] 无冗余或循环操作
- [ ] 数据访问路径最优

### **稳定性验收** 🛡️
- [ ] 连续运行无崩溃
- [ ] 错误恢复机制正常
- [ ] 资源释放完整
- [ ] 长期性能不退化

### **用户体验验收** 👥
- [ ] 交互响应友好
- [ ] 错误信息清晰
- [ ] 进度反馈及时
- [ ] 结果格式规范

---

**说明**：本测试计划v2.0版本建立了**完整的难度评估体系和自动化测试框架**，涵盖从入门级到专家级的15个测试案例，确保程序在功能性、性能、稳定性、用户体验各方面都达到生产级标准。