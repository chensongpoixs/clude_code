# å¤šæ¨¡å‹å‚å•†æ¥å…¥è®¾è®¡æ–¹æ¡ˆ

## 1. èƒŒæ™¯ä¸ç›®æ ‡

### 1.1 å½“å‰çŠ¶æ€
- ä»…æ”¯æŒå•ä¸€ `llama_cpp_http` æä¾›å•†
- é…ç½®å†™æ­»åœ¨ `LLMConfig` ä¸­
- æ— æ³•åŠ¨æ€åˆ‡æ¢ä¸åŒå‚å•†

### 1.2 ç›®æ ‡
å‚è€ƒ Dify å¹³å°ï¼Œæ”¯æŒ 50+ æ¨¡å‹æä¾›å•†ï¼š
- å›½é™…ä¸»æµï¼šOpenAIã€Anthropicã€Google Geminiã€Azureã€AWS Bedrock
- å›½å†…å‚å•†ï¼šé€šä¹‰åƒé—®ã€æ–‡å¿ƒä¸€è¨€ã€æ™ºè°±ã€DeepSeekã€æœˆä¹‹æš—é¢
- æ¨ç†å¹³å°ï¼šOllamaã€Xinferenceã€SiliconFlowã€OpenRouter
- è‡ªå®šä¹‰ï¼šä»»æ„ OpenAI å…¼å®¹ API

### 1.3 æ ¸å¿ƒéœ€æ±‚
1. **æ¯ä¸ªå‚å•†ä¸€ä¸ªç‹¬ç«‹æ–‡ä»¶** - ä¾¿äºç»´æŠ¤å’Œæ‰©å±•
2. **å…¨å±€é…ç½®ç®¡ç†** - ç»Ÿä¸€çš„å‚å•†é…ç½®
3. **è¿è¡Œæ—¶åˆ‡æ¢** - æ”¯æŒæŸ¥çœ‹ã€é€‰æ‹©ã€åˆ‡æ¢å‚å•†/æ¨¡å‹
4. **ç»Ÿä¸€æ¥å£** - æ‰€æœ‰å‚å•†æš´éœ²ç›¸åŒçš„è°ƒç”¨æ¥å£

---

## 2. æ¶æ„è®¾è®¡

### 2.1 ç›®å½•ç»“æ„

```
src/clude_code/llm/
â”œâ”€â”€ __init__.py                 # å¯¼å‡ºç»Ÿä¸€æ¥å£
â”œâ”€â”€ base.py                     # æŠ½è±¡åŸºç±» LLMProvider
â”œâ”€â”€ model_manager.py            # å…¨å±€æ¨¡å‹ç®¡ç†å™¨ï¼ˆå·²æœ‰ï¼Œéœ€æ‰©å±•ï¼‰
â”œâ”€â”€ registry.py                 # å‚å•†æ³¨å†Œè¡¨
â”‚
â”œâ”€â”€ providers/                  # å„å‚å•†å®ç°ï¼ˆæ¯ä¸ªå‚å•†ä¸€ä¸ªæ–‡ä»¶ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ openai.py              # OpenAI / GPT
â”‚   â”œâ”€â”€ anthropic.py           # Anthropic / Claude
â”‚   â”œâ”€â”€ azure_openai.py        # Azure OpenAI
â”‚   â”œâ”€â”€ google_gemini.py       # Google Gemini
â”‚   â”œâ”€â”€ aws_bedrock.py         # AWS Bedrock
â”‚   â”œâ”€â”€ ollama.py              # Ollama (æœ¬åœ°)
â”‚   â”œâ”€â”€ llama_cpp.py           # llama.cpp (æœ¬åœ°)
â”‚   â”œâ”€â”€ siliconflow.py         # ç¡…åŸºæµåŠ¨
â”‚   â”œâ”€â”€ deepseek.py            # DeepSeek
â”‚   â”œâ”€â”€ zhipu.py               # æ™ºè°± AI
â”‚   â”œâ”€â”€ moonshot.py            # æœˆä¹‹æš—é¢
â”‚   â”œâ”€â”€ qianwen.py             # é€šä¹‰åƒé—®
â”‚   â”œâ”€â”€ wenxin.py              # æ–‡å¿ƒä¸€è¨€
â”‚   â””â”€â”€ openai_compat.py       # é€šç”¨ OpenAI å…¼å®¹ï¼ˆå…œåº•ï¼‰
â”‚
â””â”€â”€ image_utils.py              # å›¾ç‰‡å¤„ç†ï¼ˆå·²æœ‰ï¼‰
```

### 2.2 æ ¸å¿ƒç±»è®¾è®¡

#### 2.2.1 æŠ½è±¡åŸºç±» `LLMProvider`

```python
# src/clude_code/llm/base.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any

@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    id: str
    name: str
    provider: str
    context_window: int = 4096
    max_output_tokens: int = 4096
    supports_vision: bool = False
    supports_function_call: bool = False
    supports_streaming: bool = True
    pricing: dict | None = None  # {"input": 0.001, "output": 0.002} per 1K tokens

@dataclass
class ProviderConfig:
    """å‚å•†é…ç½®"""
    name: str
    api_key: str = ""
    base_url: str = ""
    api_version: str = ""
    organization: str = ""
    extra: dict[str, Any] = field(default_factory=dict)

class LLMProvider(ABC):
    """LLM å‚å•†æŠ½è±¡åŸºç±»"""
    
    # å‚å•†å…ƒä¿¡æ¯
    PROVIDER_NAME: str = ""
    PROVIDER_TYPE: str = ""  # cloud | local | aggregator
    REGION: str = ""         # æµ·å¤– | å›½å†… | é€šç”¨
    
    def __init__(self, config: ProviderConfig):
        self.config = config
    
    @abstractmethod
    def chat(self, messages: list[ChatMessage], **kwargs) -> str:
        """åŒæ­¥èŠå¤©"""
        pass
    
    @abstractmethod
    async def chat_async(self, messages: list[ChatMessage], **kwargs) -> str:
        """å¼‚æ­¥èŠå¤©"""
        pass
    
    @abstractmethod
    def chat_stream(self, messages: list[ChatMessage], **kwargs):
        """æµå¼èŠå¤©"""
        pass
    
    @abstractmethod
    def list_models(self) -> list[ModelInfo]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        pass
    
    @abstractmethod
    def get_model_info(self, model_id: str) -> ModelInfo | None:
        """è·å–å•ä¸ªæ¨¡å‹ä¿¡æ¯"""
        pass
    
    def validate_config(self) -> tuple[bool, str]:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        return True, "OK"
    
    def test_connection(self) -> tuple[bool, str]:
        """æµ‹è¯•è¿æ¥"""
        pass
```

#### 2.2.2 å‚å•†æ³¨å†Œè¡¨ `ProviderRegistry`

```python
# src/clude_code/llm/registry.py
from typing import Type

class ProviderRegistry:
    """å‚å•†æ³¨å†Œè¡¨ï¼ˆå•ä¾‹ï¼‰"""
    
    _instance = None
    _providers: dict[str, Type[LLMProvider]] = {}
    _instances: dict[str, LLMProvider] = {}
    
    @classmethod
    def register(cls, name: str):
        """è£…é¥°å™¨ï¼šæ³¨å†Œå‚å•†"""
        def decorator(provider_class: Type[LLMProvider]):
            cls._providers[name] = provider_class
            return provider_class
        return decorator
    
    @classmethod
    def get_provider(cls, name: str, config: ProviderConfig) -> LLMProvider:
        """è·å–å‚å•†å®ä¾‹"""
        if name not in cls._providers:
            raise ValueError(f"æœªçŸ¥å‚å•†: {name}")
        
        key = f"{name}:{config.api_key[:8] if config.api_key else 'default'}"
        if key not in cls._instances:
            cls._instances[key] = cls._providers[name](config)
        return cls._instances[key]
    
    @classmethod
    def list_providers(cls) -> list[dict]:
        """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œå‚å•†"""
        return [
            {
                "name": name,
                "type": p.PROVIDER_TYPE,
                "region": p.REGION,
            }
            for name, p in cls._providers.items()
        ]
```

### 2.3 é…ç½®è®¾è®¡

#### 2.3.1 å…¨å±€é…ç½® `~/.clude/.clude.yaml`

```yaml
# å¤šå‚å•†é…ç½®
providers:
  # é»˜è®¤å‚å•†
  default: openai
  
  # OpenAI
  openai:
    enabled: true
    api_key: ${OPENAI_API_KEY}
    base_url: https://api.openai.com/v1
    default_model: gpt-4o
    
  # Anthropic
  anthropic:
    enabled: true
    api_key: ${ANTHROPIC_API_KEY}
    default_model: claude-3-5-sonnet-latest
    
  # Azure OpenAI
  azure_openai:
    enabled: false
    api_key: ${AZURE_OPENAI_API_KEY}
    base_url: https://your-resource.openai.azure.com
    api_version: 2024-02-15-preview
    deployment_map:
      gpt-4o: your-gpt4o-deployment
      
  # DeepSeek
  deepseek:
    enabled: true
    api_key: ${DEEPSEEK_API_KEY}
    base_url: https://api.deepseek.com/v1
    default_model: deepseek-chat
    
  # æœˆä¹‹æš—é¢
  moonshot:
    enabled: true
    api_key: ${MOONSHOT_API_KEY}
    base_url: https://api.moonshot.cn/v1
    default_model: moonshot-v1-8k
    
  # ç¡…åŸºæµåŠ¨
  siliconflow:
    enabled: true
    api_key: ${SILICONFLOW_API_KEY}
    base_url: https://api.siliconflow.cn/v1
    default_model: deepseek-ai/DeepSeek-V3
    
  # Ollama (æœ¬åœ°)
  ollama:
    enabled: true
    base_url: http://127.0.0.1:11434
    default_model: llama3.2
    
  # llama.cpp (æœ¬åœ°)
  llama_cpp:
    enabled: true
    base_url: http://127.0.0.1:8899
    default_model: gemma-3-12b-it
    
  # é€šç”¨ OpenAI å…¼å®¹
  openai_compat:
    enabled: true
    api_key: ${CUSTOM_API_KEY}
    base_url: ${CUSTOM_BASE_URL}
    default_model: ${CUSTOM_MODEL}
```

#### 2.3.2 é…ç½®æ•°æ®æ¨¡å‹

```python
# src/clude_code/config/config.py æ‰©å±•

class ProviderConfigItem(BaseModel):
    """å•ä¸ªå‚å•†é…ç½®"""
    enabled: bool = True
    api_key: str = ""
    base_url: str = ""
    api_version: str = ""
    default_model: str = ""
    extra: dict[str, Any] = Field(default_factory=dict)

class ProvidersConfig(BaseModel):
    """å¤šå‚å•†é…ç½®"""
    default: str = "openai"
    openai: ProviderConfigItem = Field(default_factory=ProviderConfigItem)
    anthropic: ProviderConfigItem = Field(default_factory=ProviderConfigItem)
    azure_openai: ProviderConfigItem = Field(default_factory=ProviderConfigItem)
    deepseek: ProviderConfigItem = Field(default_factory=ProviderConfigItem)
    moonshot: ProviderConfigItem = Field(default_factory=ProviderConfigItem)
    siliconflow: ProviderConfigItem = Field(default_factory=ProviderConfigItem)
    ollama: ProviderConfigItem = Field(default_factory=ProviderConfigItem)
    llama_cpp: ProviderConfigItem = Field(default_factory=ProviderConfigItem)
    openai_compat: ProviderConfigItem = Field(default_factory=ProviderConfigItem)
```

---

## 3. å®ç°æ­¥éª¤

### Phase 1: åŸºç¡€æ¶æ„ï¼ˆP0ï¼‰
| æ­¥éª¤ | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| 1.1 | åˆ›å»ºæŠ½è±¡åŸºç±» `LLMProvider` | `llm/base.py` |
| 1.2 | åˆ›å»ºå‚å•†æ³¨å†Œè¡¨ `ProviderRegistry` | `llm/registry.py` |
| 1.3 | æ‰©å±•é…ç½®æ•°æ®æ¨¡å‹ | `config/config.py` |
| 1.4 | é‡æ„ `ModelManager` æ”¯æŒå¤šå‚å•† | `llm/model_manager.py` |

### Phase 2: æ ¸å¿ƒå‚å•†å®ç°ï¼ˆP1ï¼‰
| æ­¥éª¤ | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| 2.1 | OpenAI æä¾›å•† | `llm/providers/openai.py` |
| 2.2 | Anthropic æä¾›å•† | `llm/providers/anthropic.py` |
| 2.3 | é€šç”¨ OpenAI å…¼å®¹ | `llm/providers/openai_compat.py` |
| 2.4 | Ollama æä¾›å•† | `llm/providers/ollama.py` |
| 2.5 | llama.cpp æä¾›å•†ï¼ˆé‡æ„ç°æœ‰ï¼‰ | `llm/providers/llama_cpp.py` |

### Phase 3: å›½å†…å‚å•†å®ç°ï¼ˆP2ï¼‰
| æ­¥éª¤ | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| 3.1 | DeepSeek | `llm/providers/deepseek.py` |
| 3.2 | æœˆä¹‹æš—é¢ | `llm/providers/moonshot.py` |
| 3.3 | æ™ºè°± AI | `llm/providers/zhipu.py` |
| 3.4 | ç¡…åŸºæµåŠ¨ | `llm/providers/siliconflow.py` |
| 3.5 | é€šä¹‰åƒé—® | `llm/providers/qianwen.py` |

### Phase 4: CLI é›†æˆï¼ˆP3ï¼‰
| æ­¥éª¤ | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| 4.1 | `/providers` å‘½ä»¤ - åˆ—å‡ºå‚å•† | `cli/slash_commands.py` |
| 4.2 | `/provider <name>` - åˆ‡æ¢å‚å•† | `cli/slash_commands.py` |
| 4.3 | `/models` å‘½ä»¤ - åˆ—å‡ºæ¨¡å‹ | `cli/slash_commands.py` |
| 4.4 | `/model <name>` - åˆ‡æ¢æ¨¡å‹ | `cli/slash_commands.py` |
| 4.5 | `clude providers` CLI å‘½ä»¤ | `cli/providers_cmd.py` |

### Phase 5: é«˜çº§åŠŸèƒ½ï¼ˆP4ï¼‰
| æ­¥éª¤ | å†…å®¹ | æ–‡ä»¶ |
|------|------|------|
| 5.1 | æ¨¡å‹èƒ½åŠ›æ£€æµ‹ï¼ˆVision/Function Callï¼‰ | `llm/capabilities.py` |
| 5.2 | æ¨¡å‹è‡ªåŠ¨è·¯ç”±ï¼ˆæŒ‰ä»»åŠ¡ç±»å‹é€‰æ¨¡å‹ï¼‰ | `llm/router.py` |
| 5.3 | æˆæœ¬è¿½è¸ªä¸é¢„ç®—æ§åˆ¶ | `llm/cost_tracker.py` |
| 5.4 | æ•…éšœè½¬ç§»ï¼ˆFailoverï¼‰ | `llm/failover.py` |

---

## 4. å„å‚å•†å®ç°ç¤ºä¾‹

### 4.1 OpenAI æä¾›å•†

```python
# src/clude_code/llm/providers/openai.py
from ..base import LLMProvider, ProviderConfig, ModelInfo, ChatMessage
from ..registry import ProviderRegistry
import httpx

@ProviderRegistry.register("openai")
class OpenAIProvider(LLMProvider):
    PROVIDER_NAME = "OpenAI"
    PROVIDER_TYPE = "cloud"
    REGION = "æµ·å¤–"
    
    MODELS = [
        ModelInfo(id="gpt-4o", name="GPT-4o", provider="openai", 
                  context_window=128000, max_output_tokens=16384,
                  supports_vision=True, supports_function_call=True),
        ModelInfo(id="gpt-4o-mini", name="GPT-4o Mini", provider="openai",
                  context_window=128000, max_output_tokens=16384,
                  supports_vision=True, supports_function_call=True),
        ModelInfo(id="gpt-4-turbo", name="GPT-4 Turbo", provider="openai",
                  context_window=128000, max_output_tokens=4096,
                  supports_vision=True, supports_function_call=True),
        ModelInfo(id="o1-preview", name="o1 Preview", provider="openai",
                  context_window=128000, max_output_tokens=32768),
        ModelInfo(id="o1-mini", name="o1 Mini", provider="openai",
                  context_window=128000, max_output_tokens=65536),
    ]
    
    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self.base_url = config.base_url or "https://api.openai.com/v1"
        self.client = httpx.Client(
            base_url=self.base_url,
            headers={
                "Authorization": f"Bearer {config.api_key}",
                "Content-Type": "application/json",
            },
            timeout=120.0,
        )
    
    def chat(self, messages: list[ChatMessage], **kwargs) -> str:
        model = kwargs.get("model", self.config.extra.get("default_model", "gpt-4o"))
        payload = {
            "model": model,
            "messages": [{"role": m.role, "content": m.content} for m in messages],
            "temperature": kwargs.get("temperature", 0.2),
            "max_tokens": kwargs.get("max_tokens", 4096),
        }
        resp = self.client.post("/chat/completions", json=payload)
        resp.raise_for_status()
        return resp.json()["choices"][0]["message"]["content"]
    
    def list_models(self) -> list[ModelInfo]:
        return self.MODELS
    
    def get_model_info(self, model_id: str) -> ModelInfo | None:
        for m in self.MODELS:
            if m.id == model_id:
                return m
        return None
```

### 4.2 DeepSeek æä¾›å•†

```python
# src/clude_code/llm/providers/deepseek.py
from ..base import LLMProvider, ProviderConfig, ModelInfo
from ..registry import ProviderRegistry

@ProviderRegistry.register("deepseek")
class DeepSeekProvider(LLMProvider):
    PROVIDER_NAME = "DeepSeek"
    PROVIDER_TYPE = "cloud"
    REGION = "å›½å†…"
    
    MODELS = [
        ModelInfo(id="deepseek-chat", name="DeepSeek Chat", provider="deepseek",
                  context_window=64000, max_output_tokens=8192,
                  supports_function_call=True),
        ModelInfo(id="deepseek-coder", name="DeepSeek Coder", provider="deepseek",
                  context_window=64000, max_output_tokens=8192,
                  supports_function_call=True),
        ModelInfo(id="deepseek-reasoner", name="DeepSeek R1", provider="deepseek",
                  context_window=64000, max_output_tokens=8192),
    ]
    
    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self.base_url = config.base_url or "https://api.deepseek.com/v1"
        # å¤ç”¨ OpenAI å…¼å®¹é€»è¾‘...
```

---

## 5. CLI äº¤äº’è®¾è®¡

### 5.1 æŸ¥çœ‹å‚å•†åˆ—è¡¨

```bash
$ clude providers list

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                    å¯ç”¨æ¨¡å‹å‚å•† (8/50)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  åºå·  â”‚  å‚å•†åç§°       â”‚  ç±»å‹    â”‚  åŒºåŸŸ  â”‚  çŠ¶æ€    â”‚  é»˜è®¤æ¨¡å‹              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1     â”‚  â˜… openai       â”‚  cloud   â”‚  æµ·å¤–  â”‚  âœ“ å·²é…ç½® â”‚  gpt-4o               â”‚
â”‚  2     â”‚  anthropic      â”‚  cloud   â”‚  æµ·å¤–  â”‚  âœ“ å·²é…ç½® â”‚  claude-3-5-sonnet    â”‚
â”‚  3     â”‚  deepseek       â”‚  cloud   â”‚  å›½å†…  â”‚  âœ“ å·²é…ç½® â”‚  deepseek-chat        â”‚
â”‚  4     â”‚  moonshot       â”‚  cloud   â”‚  å›½å†…  â”‚  âœ— æœªé…ç½® â”‚  -                    â”‚
â”‚  5     â”‚  â˜… llama_cpp    â”‚  local   â”‚  é€šç”¨  â”‚  âœ“ è¿è¡Œä¸­ â”‚  gemma-3-12b-it       â”‚
â”‚  6     â”‚  ollama         â”‚  local   â”‚  é€šç”¨  â”‚  âœ— æœªè¿è¡Œ â”‚  -                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

â˜… = å½“å‰ä½¿ç”¨

ä½¿ç”¨ `/provider <name>` åˆ‡æ¢å‚å•†
ä½¿ç”¨ `clude providers config <name>` é…ç½®å‚å•†
```

### 5.2 æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨

```bash
$ clude models list --provider openai

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                 OpenAI å¯ç”¨æ¨¡å‹ (5)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ¨¡å‹ ID          â”‚  åç§°           â”‚  ä¸Šä¸‹æ–‡    â”‚  èƒ½åŠ›             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â˜… gpt-4o         â”‚  GPT-4o         â”‚  128K      â”‚  ğŸ–¼ï¸ ğŸ“ ğŸŒŠ          â”‚
â”‚  gpt-4o-mini      â”‚  GPT-4o Mini    â”‚  128K      â”‚  ğŸ–¼ï¸ ğŸ“ ğŸŒŠ          â”‚
â”‚  gpt-4-turbo      â”‚  GPT-4 Turbo    â”‚  128K      â”‚  ğŸ–¼ï¸ ğŸ“ ğŸŒŠ          â”‚
â”‚  o1-preview       â”‚  o1 Preview     â”‚  128K      â”‚  ğŸŒŠ               â”‚
â”‚  o1-mini          â”‚  o1 Mini        â”‚  128K      â”‚  ğŸŒŠ               â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

ğŸ–¼ï¸ = Vision  ğŸ“ = Function Call  ğŸŒŠ = Streaming
â˜… = å½“å‰ä½¿ç”¨

ä½¿ç”¨ `/model <id>` åˆ‡æ¢æ¨¡å‹
```

### 5.3 åœ¨ Chat ä¸­åˆ‡æ¢

```
you (): /providers
[æ˜¾ç¤ºå‚å•†åˆ—è¡¨]

you (): /provider deepseek
âœ“ å·²åˆ‡æ¢åˆ°å‚å•†: DeepSeek (deepseek-chat)

you (): /models
[æ˜¾ç¤º DeepSeek æ¨¡å‹åˆ—è¡¨]

you (): /model deepseek-coder
âœ“ å·²åˆ‡æ¢åˆ°æ¨¡å‹: deepseek-coder
```

---

## 6. éªŒæ”¶æ ‡å‡†

### 6.1 åŠŸèƒ½éªŒæ”¶
- [ ] æ”¯æŒè‡³å°‘ 10 ä¸ªå‚å•†
- [ ] æ¯ä¸ªå‚å•†ç‹¬ç«‹æ–‡ä»¶
- [ ] ç»Ÿä¸€çš„ `LLMProvider` æ¥å£
- [ ] CLI å‘½ä»¤å®Œæ•´
- [ ] é…ç½®çƒ­åŠ è½½

### 6.2 éåŠŸèƒ½éªŒæ”¶
- [ ] å‚å•†åˆ‡æ¢ < 100ms
- [ ] æ¨¡å‹åˆ—è¡¨ç¼“å­˜
- [ ] é”™è¯¯å¤„ç†å®Œå–„
- [ ] æ–‡æ¡£å®Œæ•´

---

## 7. é£é™©ä¸ç¼“è§£

| é£é™© | ç¼“è§£æªæ–½ |
|------|----------|
| API æ ¼å¼å·®å¼‚ | ç»Ÿä¸€æŠ½è±¡å±‚ + é€‚é…å™¨æ¨¡å¼ |
| è®¤è¯æ–¹å¼ä¸åŒ | æ”¯æŒå¤šç§è®¤è¯ç­–ç•¥ |
| æµå¼å“åº”å·®å¼‚ | ç»Ÿä¸€ SSE è§£æ |
| å›½å†…ç½‘ç»œé—®é¢˜ | æ”¯æŒä»£ç†é…ç½® |

---

## 8. çŠ¶æ€

- [x] Phase 1: åŸºç¡€æ¶æ„ï¼ˆLLMProvider, ProviderRegistry, ProvidersConfig, ModelManagerï¼‰
- [x] Phase 2: æ ¸å¿ƒå‚å•†ï¼ˆ21 å®¶å·²å®ç°ï¼‰
- [x] Phase 3: CLI é›†æˆï¼ˆ/providers, /provider, /models å¢å¼ºï¼‰
- [x] Phase 4: é«˜çº§åŠŸèƒ½ï¼ˆæˆæœ¬è¿½è¸ªã€æ•…éšœè½¬ç§»ã€è‡ªåŠ¨è·¯ç”±ï¼‰
- [x] Phase 5: å®Œæ•´å‚å•†æ¥å…¥ï¼ˆæ–°å¢ 25 å®¶ï¼Œæ€»è®¡ 46 å®¶ï¼‰

### å·²å®ç°å‚å•† (46 å®¶)

| ç±»å‹ | å‚å•†æ•°é‡ | å‚å•†åˆ—è¡¨ |
|------|----------|----------|
| å›½é™…ä¸»æµ | 5 | OpenAI, Anthropic, Google Gemini, Mistral, Cohere |
| äº‘å‚å•† | 5 | Azure OpenAI, Google Vertex AI, AWS Bedrock, AWS SageMaker, è…¾è®¯äº‘ |
| NVIDIA | 3 | NVIDIA NIM, NVIDIA Triton, NVIDIA Catalog |
| å›½å†…å‚å•† | 15 | DeepSeek, æœˆä¹‹æš—é¢, æ™ºè°±, é€šä¹‰åƒé—®, æ–‡å¿ƒä¸€è¨€, ç™¾å·, MiniMax, è®¯é£, è…¾è®¯æ··å…ƒ, é˜¶è·ƒæ˜Ÿè¾°, é­”æ­ç¤¾åŒº, ç™¾åº¦åƒå¸†, é˜¿é‡Œäº‘ PAI, è…¾è®¯äº‘ TI, ä¸ƒç‰›äº‘ |
| æ¨ç†å¹³å° | 13 | Ollama, Groq, Together.ai, OpenRouter, ç¡…åŸºæµåŠ¨, Replicate, Hugging Face, Lepton, novita.ai, Jina, GPUStack, PerfXCloud, Xorbits |
| æœ¬åœ°éƒ¨ç½² | 4 | LocalAI, Xinference, OpenLLM, Text Embedding |
| åŸºç¡€ | 1 | OpenAI Compatible |

### é«˜çº§åŠŸèƒ½æ¨¡å—

| æ¨¡å— | æ–‡ä»¶ | åŠŸèƒ½ |
|------|------|------|
| æˆæœ¬è¿½è¸ª | `cost_tracker.py` | Token æ¶ˆè€—è®°å½•ã€è´¹ç”¨è®¡ç®—ã€æŒ‰å‚å•†/æ¨¡å‹ç»Ÿè®¡ |
| æ•…éšœè½¬ç§» | `failover.py` | è‡ªåŠ¨åˆ‡æ¢å¤‡ç”¨å‚å•†ã€å¥åº·æ£€æŸ¥ã€é‡è¯•ç­–ç•¥ |
| è‡ªåŠ¨è·¯ç”± | `auto_router.py` | æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æœ€ä½³æ¨¡å‹ã€ä¼˜å…ˆçº§ç­–ç•¥ |

