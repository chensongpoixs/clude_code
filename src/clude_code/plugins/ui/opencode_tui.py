"""
Textual-based TUIï¼ˆå¯¹æ ‡ OpenCodeï¼‰ï¼š
- å¤šçª—æ ¼å¸ƒå±€ï¼ˆå·¦ä¾§è¾“å‡º/å³ä¾§çŠ¶æ€ä¸æ“ä½œ/åº•éƒ¨äº‹ä»¶ï¼‰
- æ¯ä¸ªçª—æ ¼å¯æ»šåŠ¨ï¼ˆæ”¯æŒé¼ æ ‡æ»šè½®æŸ¥çœ‹å†å²ï¼‰
- ä¸ä¾èµ– rich.Live çš„æ•´å±åˆ·æ–°

æ³¨æ„ï¼šè¯¥æ¨¡å—ä¾èµ–å¯é€‰ä¾èµ– `textual`ï¼ˆè§ pyproject.toml çš„ [project.optional-dependencies].uiï¼‰ã€‚
æœªå®‰è£…æ—¶åº”ç”±è°ƒç”¨æ–¹ä¼˜é›…é™çº§ã€‚
"""

from __future__ import annotations

from queue import Empty, Queue
from threading import Thread
from typing import Any, Callable
import json
import time
import threading
from rich.text import Text
from rich.table import Table
from rich.syntax import Syntax
from collections import deque


def run_opencode_tui(
    *,
    cfg: Any,
    run_turn: Callable[[str, Callable[[str], bool], Callable[[dict[str, Any]], None]], None],
) -> None:
    """
    è¿è¡Œ OpenCode é£æ ¼ Textual TUIï¼ˆåœ¨ä¸»çº¿ç¨‹é˜»å¡è¿è¡Œï¼‰ã€‚

    ä¸ºä»€ä¹ˆè¿™æ ·åšï¼š
    - Textual/TUI æ¡†æ¶é€šå¸¸éœ€è¦åœ¨ä¸»çº¿ç¨‹è¿è¡Œï¼ˆæ‰èƒ½æ­£ç¡®å¤„ç†è¾“å…¥/é¼ æ ‡/ç»ˆç«¯èƒ½åŠ›ï¼‰
    - AgentLoop åœ¨åå°çº¿ç¨‹æ‰§è¡Œï¼Œé€šè¿‡é˜Ÿåˆ—æŠŠäº‹ä»¶æ¨é€å› UI çº¿ç¨‹æ¸²æŸ“
    """

    # å»¶è¿Ÿå¯¼å…¥ï¼šé¿å…çº¯ CLI/doctor/tools ä¹Ÿè¢«è¿«å®‰è£… textual
    # å¯é€‰ä¾èµ–ï¼šTextualï¼ˆè¿è¡Œæ—¶å­˜åœ¨å³å¯ï¼›é™æ€æ£€æŸ¥å…è®¸ç¼ºå¤±ï¼‰
    from textual.app import App, ComposeResult  # type: ignore[import-not-found]
    from textual.containers import Horizontal, Vertical  # type: ignore[import-not-found]
    from textual.widgets import Footer, Header, Input, RichLog  # type: ignore[import-not-found]

    q: Queue[dict[str, Any]] = Queue(maxsize=50_000)
    _confirm_lock = threading.Lock()
    _confirm_seq = 0
    _confirm_waiters: dict[int, dict[str, Any]] = {}

    class _Log(RichLog):
        """RichLog é»˜è®¤å¯æ»šåŠ¨ï¼Œæ”¯æŒé¼ æ ‡æ»šè½®æŸ¥çœ‹å†å²ã€‚"""

        def __init__(self, *args: Any, **kwargs: Any) -> None:
            kwargs.setdefault("wrap", True)
            super().__init__(*args, **kwargs)
            # é»˜è®¤è·Ÿéšå°¾éƒ¨ï¼ˆæ›´åƒ OpenCodeï¼‰ï¼›ç”¨æˆ·å¯æŒ‰ f åˆ‡æ¢â€œæµè§ˆå†å²/è·Ÿéšè¾“å‡ºâ€
            self.auto_scroll = True

    class OpencodeTUI(App):
        TITLE = "clude chat"
        SUB_TITLE = "opencode"
        CSS = """
        Screen { layout: vertical; }
        /* ä¸»åŒºï¼ˆå¯¹è¯/è¾“å‡º + æ“ä½œé¢æ¿ï¼‰ä¸äº‹ä»¶åŒºå°½é‡ç­‰é«˜ï¼Œä¾¿äºæ’æŸ¥é—®é¢˜ */
        #main { height: 1fr; min-height: 10; }
        /* é¡¶éƒ¨ clude chat é¢æ¿ï¼šæŒ‰å†…å®¹è‡ªé€‚åº”ï¼Œé¿å…å¤šä½™ç©ºç™½ */
        #header_panel { height: auto; min-height: 3; }
        #left { width: 3fr; }
        #right { width: 2fr; }
        /* è¾“å…¥æ¡†ï¼šéœ€è¦ç»™è¾¹æ¡†/æç¤ºç•™ç©ºé—´ï¼Œå¦åˆ™ä¼šå¯¼è‡´æ— æ³•è¾“å…¥æˆ–ä¸å¯è§ */
        #input_row { height: 3; min-height: 3; }
        /* äº‹ä»¶åŒºï¼šæé«˜é«˜åº¦ï¼ˆä¸ä¸»åŒºæ¥è¿‘ç­‰é«˜ï¼‰ï¼›å¹¶ç»™æœ€å°é«˜åº¦é¿å…å¤ªæ‰ */
        #events { height: 1fr; min-height: 10; }
        _Log { border: solid $primary; }
        /* æ‰€æœ‰å°çª—å£æ ‡é¢˜å±…ä¸­ï¼ˆå¯¹é½ä½ çš„è¦æ±‚ï¼‰ */
        #header_panel, #conversation, #ops, #events, #input { border-title-align: center; }
        /* å³ä¾§ï¼šæ“ä½œé¢æ¿åƒæ‰å‰©ä½™é«˜åº¦ */
        #ops { height: 1fr; min-height: 8; }
        """

        BINDINGS = [
            ("q", "quit", "Quit"),
            ("ctrl+c", "quit", "Quit"),
            ("f", "toggle_follow", "Follow/Scroll"),
            ("end", "jump_bottom", "Bottom"),
        ]

        def __init__(self) -> None:
            super().__init__()
            self._busy = False
            self._follow = True
            self._model = str(getattr(getattr(cfg, "llm", None), "model", "") or "auto")
            self._base_url = str(getattr(getattr(cfg, "llm", None), "base_url", "") or "")

            # å¯¹é½ enhanced çš„â€œçŠ¶æ€/æ“ä½œâ€å­—æ®µ
            self._state = "IDLE"
            self._operation = "ç­‰å¾…ä¸­"
            self._last_step: int | str = "-"
            self._last_event: str = "waiting"
            self._last_llm_messages: int | None = None
            self._last_tool: str | None = None
            self._last_tool_args: str | None = None
            self._last_tool_result: str | None = None
            self._active_tasks: int = 0
            self._recent_completed: deque[str] = deque(maxlen=5)
            self._project_memory_shown: bool = False

            self._max_tokens = int(getattr(getattr(cfg, "llm", None), "max_tokens", 0) or 0)
            self._llm_prompt = 0
            self._llm_completion = 0
            self._tps = 0.0
            self._verbosity: str = "compact"  # compact|verbose|debugï¼ˆä»…å½±å“â€œå¯¹è¯/è¾“å‡ºâ€çš„å—å†…å®¹ï¼‰
            # å¯¹è¯çª—æ ¼é£æ ¼ï¼šlog = å¤åˆ» chat é»˜è®¤â€œæ‰§è¡Œæ—¥å¿—æµâ€ï¼›block = ç»“æ„åŒ–å—
            self._conversation_mode: str = "log"
            self._llm_round: int = 0

            # äº¤äº’ç¡®è®¤ï¼ˆconfirmï¼‰çŠ¶æ€
            self._pending_confirm_id: int | None = None
            self._pending_confirm_msg: str | None = None
            self._input_placeholder_normal: str = "è¾“å…¥å†…å®¹ï¼Œå›è½¦å‘é€ï¼ˆq é€€å‡ºï¼‰"

            # LLM è¯·æ±‚å†å²ï¼ˆç”¨äºâ€œæ“ä½œé¢æ¿â€çš„å¤šæ¡è¿›åº¦æ¡å±•ç¤ºï¼‰
            # æ¯æ¡ï¼š{id, idx, kind, step_id, start_ts, elapsed_ms, status, model, prompt_tokens, completion_tokens}
            self._llm_req_seq: int = 0
            self._llm_requests: deque[dict[str, Any]] = deque(maxlen=12)
            self._active_llm_id: int | None = None
            self._spinner_frames: tuple[str, ...] = ("â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â ")
            self._spinner_idx: int = 0

        def _now_hhmmss(self) -> str:
            try:
                return time.strftime("%H:%M:%S", time.localtime())
            except Exception:
                return ""

        def _short_trace(self, trace_id: str | None) -> str:
            t = (trace_id or "").strip()
            return t[:8] if t else "-"

        def _level_style(self, level: str) -> str:
            lv = (level or "").strip().lower()
            return {
                "info": "cyan",
                "progress": "blue",
                "warning": "yellow",
                "warn": "yellow",
                "error": "red",
                "success": "green",
            }.get(lv, "cyan")

        def _event_phase(self) -> str:
            """æŠŠå†…éƒ¨çŠ¶æ€æ˜ å°„åˆ°æ›´ç¨³å®šçš„ phase å±•ç¤ºå­—æ®µï¼ˆç”¨äºäº‹ä»¶çª—æ ¼æ‘˜è¦ï¼‰ã€‚"""
            st = (self._state or "").strip().upper()
            if st in {"INTAKE", "PLANNING", "EXECUTING", "VERIFY", "DONE"}:
                return st
            if st in {"IDLE"}:
                return "IDLE"
            return st or "UNK"

        def _event_level_code(self, et: str, data: dict[str, Any]) -> str:
            """
            äº‹ä»¶çº§åˆ«ï¼ˆç”¨äºäº‹ä»¶çª—æ ¼æ‘˜è¦ï¼‰ï¼š
            - E: Error
            - W: Warning
            - I: Info
            - D: Debug
            """
            et = (et or "").strip()
            if et in {"plan_parse_failed", "policy_deny_tool", "policy_deny_cmd", "denied_by_user"}:
                return "E"
            if et in {"stuttering_detected"}:
                return "W"
            if et == "tool_result":
                ok = bool((data or {}).get("ok"))
                return "I" if ok else "E"
            if et in {"llm_request_params", "llm_usage"}:
                return "D"
            return "I"

        def _event_summary(self, et: str, data: dict[str, Any]) -> str:
            """æŠŠä¸åŒäº‹ä»¶å‹ç¼©æˆä¸€æ®µâ€œäººèƒ½æ‰«è¯»â€çš„æ‘˜è¦ï¼ˆä¸€çº§èŠ‚ç‚¹ç”¨ï¼‰ã€‚"""
            data = data or {}
            if et == "state":
                return f"state={data.get('state')} reason={data.get('reason') or data.get('step') or data.get('mode')}"
            if et == "intent_classified":
                return f"category={data.get('category')} conf={data.get('confidence')}"
            if et == "planning_llm_request":
                return f"attempt={data.get('attempt')}"
            if et == "plan_generated":
                return f"title={data.get('title')} steps={data.get('steps')}"
            if et == "plan_step_start":
                return f"{data.get('idx')}/{data.get('total')} step_id={data.get('step_id')}"
            if et == "llm_request_params":
                return (
                    f"model={data.get('model')} prompt={data.get('prompt_tokens_est')} "
                    f"max={data.get('max_tokens')} temp={data.get('temperature')}"
                )
            if et == "llm_usage":
                return (
                    f"prompt={data.get('prompt_tokens_est')} completion={data.get('completion_tokens_est')} "
                    f"elapsed_ms={data.get('elapsed_ms')}"
                )
            if et == "tool_call_parsed":
                tool = str(data.get("tool") or "")
                args = data.get("args") or {}
                evs = self._summarize_tool_args(tool, args if isinstance(args, dict) else {})
                evs_s = " ".join(evs[:3]) if evs else ""
                return f"tool={tool}" + (f" {evs_s}" if evs_s else "")
            if et == "tool_result":
                tool = str(data.get("tool") or "")
                ok = bool(data.get("ok"))
                err = data.get("error")
                code = ""
                msg = ""
                if isinstance(err, dict):
                    code = str(err.get("code") or "")
                    msg = str(err.get("message") or "")
                else:
                    msg = str(err or "")
                if msg and len(msg) > 80:
                    msg = msg[:79] + "â€¦"
                return f"tool={tool} ok={ok}" + (f" code={code}" if code else "") + (f" err={msg}" if msg and not ok else "")
            if et == "display":
                title = str(data.get("title") or "display")
                content = str(data.get("content") or "")
                content = content.strip().replace("\n", " ")
                if len(content) > 80:
                    content = content[:79] + "â€¦"
                return f"title={title} {content}".strip()
            if et in {"policy_deny_tool"}:
                return f"tool={data.get('tool')} reason={data.get('reason')}"
            if et in {"policy_deny_cmd"}:
                c = str(data.get("command") or "")
                c = c.replace("\n", " ")
                if len(c) > 80:
                    c = c[:79] + "â€¦"
                return f"cmd={c} reason={data.get('reason')}"
            if et == "denied_by_user":
                return f"tool={data.get('tool')}"
            return ""

        def _event_icon(self, lvl: str, et: str, data: dict[str, Any]) -> tuple[str, str]:
            """äº‹ä»¶æ‘˜è¦è¡Œçš„å°å›¾æ ‡ï¼ˆæ›´å¥½æ‰«è¯»ï¼‰ã€‚è¿”å› (icon, style)ã€‚"""
            if et == "tool_result":
                ok = bool((data or {}).get("ok"))
                return ("âœ“" if ok else "âœ—", "bold green" if ok else "bold red")
            if lvl == "E":
                return ("âœ—", "bold red")
            if lvl == "W":
                return ("âš ", "bold yellow")
            if lvl == "D":
                return ("Â·", "dim")
            return ("â€¢", "bold white")

        def _event_name_style(self, et: str) -> str:
            et = (et or "").strip()
            if et.startswith("llm_"):
                return "bold magenta"
            if et.startswith("tool_"):
                return "bold yellow"
            if et.startswith("plan_") or et.startswith("planning_"):
                return "bold cyan"
            if et.startswith("policy_") or et in {"denied_by_user"}:
                return "bold red"
            if et == "state":
                return "bold blue"
            if et == "display":
                return "bold cyan"
            return "bold"

        def _one_line(self, s: Any, limit: int = 140) -> str:
            try:
                t = str(s or "").replace("\n", " ").strip()
            except Exception:
                return ""
            if len(t) > limit:
                t = t[: limit - 1] + "â€¦"
            return t

        def _fmt_duration(self, seconds: float) -> str:
            try:
                s = max(0, int(seconds))
                m, s = divmod(s, 60)
                h, m = divmod(m, 60)
                if h > 0:
                    return f"{h}:{m:02d}:{s:02d}"
                return f"0:{m:02d}:{s:02d}"
            except Exception:
                return "0:00:00"

        def _format_llm_progress_line(self, *, idx: int, purpose: str, done: bool, elapsed_s: float) -> Text:
            """
            æ§åˆ¶é¢æ¿é‡Œçš„ LLM è¯·æ±‚è¿›åº¦è¡Œï¼Œå½¢æ€å¯¹é½ä½ ç»™çš„å‚è€ƒï¼š
            1. LLM è¯·æ±‚ [ç›®çš„]   0% -:--:--
            """
            mid = (purpose or "åˆ†æ").strip()
            if len(mid) > 24:
                mid = mid[:23] + "â€¦"
            # æ ·å¼å¯¹é½ï¼šLLM è¯·æ±‚ [xxxxxxx]   0% -:--:--
            mid = f"[{mid}]"
            pct = "100%" if done else "0%"
            tail = self._fmt_duration(elapsed_s) if done else "-:--:--"
            spin = "" if done else (self._spinner_frames[self._spinner_idx % len(self._spinner_frames)] + " ")

            t = Text()
            t.append(f"{idx}. ", style="dim")
            t.append(spin, style="yellow" if not done else "dim")
            t.append("LLM è¯·æ±‚ ", style="bold")
            t.append(mid, style="bold cyan" if done else "bold yellow")
            t.append("   ", style="dim")
            t.append(f"{pct} ", style="bold green" if done else "yellow")
            t.append(tail, style="white" if done else "dim")
            return t

        def _llm_start(self, *, kind: str, step_id: str | None = None, purpose: str | None = None) -> None:
            self._llm_req_seq += 1
            p = (purpose or "").strip()
            if not p:
                # å…ˆç»™ä¸€ä¸ªå ä½ç›®çš„ï¼›æ›´å‡†ç¡®çš„ç›®çš„ä¼šåœ¨ llm_request_params é‡Œæ ¹æ® stage/step_id è‡ªåŠ¨ä¿®æ­£
                p = "å‡†å¤‡è¯·æ±‚"
            rec: dict[str, Any] = {
                "id": self._llm_req_seq,
                "idx": self._llm_req_seq,
                "kind": kind,  # planning|execute_step|react|unknown
                "step_id": step_id,
                "purpose": p,
                "start_ts": time.time(),
                "elapsed_ms": None,
                "status": "running",
                "model": None,
                "prompt_tokens": None,
                "completion_tokens": None,
            }
            self._llm_requests.append(rec)
            self._active_llm_id = rec["id"]

        def _llm_attach_params(self, data: dict[str, Any]) -> None:
            if self._active_llm_id is None:
                return
            try:
                rec = next((r for r in reversed(self._llm_requests) if r.get("id") == self._active_llm_id), None)
                if not rec:
                    return
                # ç”¨ stage/step_id æ¨å¯¼â€œæœ¬æ¬¡è¯·æ±‚ç›®çš„â€ï¼ˆè¿™æ¯”ä»…é  kind/step_id æ›´å‡†ç¡®ï¼‰
                stage = str(data.get("stage") or "").strip()
                sid = str(data.get("step_id") or "").strip() or None
                if stage:
                    rec["kind"] = stage
                    rec["step_id"] = sid
                    if stage == "planning":
                        rec["purpose"] = "ç”Ÿæˆæ•´ä½“è®¡åˆ’"
                    elif stage == "replan":
                        rec["purpose"] = f"é‡è§„åˆ’ {sid}" if sid else "é‡è§„åˆ’æ­¥éª¤"
                    elif stage == "execute_step":
                        # execute_step çš„æ ¸å¿ƒç›®æ ‡ï¼šåˆ†æå½“å‰æ­¥éª¤å¹¶å†³å®šä¸‹ä¸€åŠ¨ä½œï¼ˆå·¥å…·/å®Œæˆ/é‡è§„åˆ’ï¼‰
                        rec["purpose"] = f"åˆ†ææ­¥éª¤ {sid}" if sid else "åˆ†æå¹¶å†³å®šä¸‹ä¸€æ­¥"
                    elif stage == "react_fallback":
                        rec["purpose"] = "ReAct å†³ç­–/ç›´æ¥å›ç­”"
                    else:
                        # å…¶å®ƒé˜¶æ®µï¼šç»™ä¸€ä¸ªå¯è¯»å…œåº•
                        rec["purpose"] = stage
                if data.get("model"):
                    rec["model"] = data.get("model")
                if isinstance(data.get("prompt_tokens_est"), int):
                    rec["prompt_tokens"] = data.get("prompt_tokens_est")
            except Exception:
                pass

        def _llm_finish(self, data: dict[str, Any]) -> None:
            if self._active_llm_id is None:
                return
            try:
                rec = next((r for r in reversed(self._llm_requests) if r.get("id") == self._active_llm_id), None)
                if not rec:
                    return
                elapsed_ms = data.get("elapsed_ms")
                if isinstance(elapsed_ms, int) and elapsed_ms >= 0:
                    rec["elapsed_ms"] = elapsed_ms
                if isinstance(data.get("completion_tokens_est"), int):
                    rec["completion_tokens"] = data.get("completion_tokens_est")
                rec["status"] = "done"
            finally:
                self._active_llm_id = None

        def _push_structured_block(
            self,
            *,
            title: str,
            level: str = "info",
            step: int | str | None = None,
            ev: str | None = None,
            trace_id: str | None = None,
            summary: str | None = None,
            decision: str | None = None,
            evidence: list[str] | None = None,
            hint: str | None = None,
            force_show_decision: bool = False,
        ) -> None:
            """
            åœ¨â€œå¯¹è¯/è¾“å‡ºâ€çª—æ ¼è¾“å‡ºç»“æ„åŒ–å—ï¼š
            - å¤´éƒ¨ï¼štime/LEVEL/step/ev/trace
            - æ­£æ–‡ï¼šSummary / Why / Evidenceï¼ˆæ‘˜è¦ä¼˜å…ˆï¼‰
            """
            conversation = self.query_one("#conversation", _Log)
            ttl = (title or "").strip()
            if not ttl:
                return
            lv = (level or "info").strip().upper()
            t = self._now_hhmmss()
            st = "-" if step is None else str(step)
            et = (ev or "").strip() or "-"
            tr = self._short_trace(trace_id)
            head = f"[{t}] [{lv}] step={st} ev={et} trace={tr}  {ttl}".strip()

            color = self._level_style(level)
            conversation.write(Text(f"â”Œâ”€ {head}", style=color))

            def _w(prefix: str, txt: str | None) -> None:
                s = (txt or "").strip()
                if not s:
                    return
                # é˜²æ­¢çˆ†å±ï¼šå¯¹è¯åŒºæ¯è¡Œå°½é‡çŸ­ä¸€äº›
                if len(s) > 500 and self._verbosity != "debug":
                    s = s[:499] + "â€¦"
                conversation.write(Text(f"â”‚ {prefix}{s}", style=color))

            _w("Summary: ", summary)
            if force_show_decision or self._verbosity in {"verbose", "debug"}:
                _w("Why: ", decision)
            if evidence:
                if self._verbosity == "compact":
                    ev_lines = evidence[:6]
                else:
                    ev_lines = evidence[:12]
                for ln in ev_lines:
                    ln = (ln or "").strip()
                    if not ln:
                        continue
                    if len(ln) > 520 and self._verbosity != "debug":
                        ln = ln[:519] + "â€¦"
                    conversation.write(Text(f"â”‚ Evidence: {ln}", style=color))
                if len(evidence) > len(ev_lines):
                    conversation.write(Text("â”‚ Evidence: â€¦(æ›´å¤šè¯æ®è§â€œäº‹ä»¶/æ“ä½œé¢æ¿â€)", style=color))
            if hint:
                _w("Hint: ", hint)
            conversation.write(Text("â””â”€", style=color))
            if self._follow:
                try:
                    conversation.scroll_end(animate=False)
                except Exception:
                    pass

        def _push_chat_log(self, text: str, *, style: str = "white") -> None:
            """åœ¨å¯¹è¯çª—æ ¼è¾“å‡ºâ€œchat é»˜è®¤æ—¥å¿—æµâ€çš„ä¸€è¡Œã€‚"""
            t = (text or "").rstrip()
            if not t:
                return
            conv = self.query_one("#conversation", _Log)
            conv.write(Text(t, style=style))
            if self._follow:
                try:
                    conv.scroll_end(animate=False)
                except Exception:
                    pass

        def _format_args_one_line(self, args: Any, *, limit: int = 220) -> str:
            try:
                s = json.dumps(args, ensure_ascii=False, default=str)
            except Exception:
                s = str(args)
            s = s.replace("\n", " ").strip()
            if len(s) > limit:
                s = s[: limit - 1] + "â€¦"
            return s

        def _format_tool_result_summary(self, tool: str, ok: bool, data: dict[str, Any]) -> str:
            """å°½é‡ç”¨äººèƒ½ç†è§£çš„æ–¹å¼æ¦‚æ‹¬ tool_resultã€‚"""
            err = data.get("error")
            if not ok:
                if isinstance(err, dict):
                    code = err.get("code")
                    msg = err.get("message")
                    return f"å¤±è´¥: {code} {self._one_line(msg, 140)}".strip()
                return f"å¤±è´¥: {self._one_line(err, 140)}".strip()
            payload = data.get("payload") or {}
            if tool == "list_dir" and isinstance(payload, dict):
                # å…¼å®¹ä¸åŒå®ç°ï¼šitems/entries å¯èƒ½å­˜åœ¨
                items = payload.get("items") or payload.get("entries")
                if isinstance(items, list):
                    return f"æˆåŠŸ: {len(items)} é¡¹"
            if tool == "grep" and isinstance(payload, dict):
                hits = payload.get("hits") or payload.get("matches") or payload.get("count")
                if isinstance(hits, int):
                    return f"æˆåŠŸ: æ‰¾åˆ° {hits} ä¸ªåŒ¹é…"
            if tool == "read_file" and isinstance(payload, dict):
                content = payload.get("content") or payload.get("text")
                if isinstance(content, str):
                    return f"æˆåŠŸ: è¯»å– {len(content)} å­—ç¬¦"
            return "æˆåŠŸ"

        def _summarize_tool_args(self, tool: str, args: dict[str, Any]) -> list[str]:
            """ä» args ä¸­æç‚¼â€œå¯¹è¯åŒºå¯è¯»è¯æ®â€ï¼Œé¿å… dump å…¨é‡ JSONã€‚"""
            tool = (tool or "").strip()
            args = args or {}
            evs: list[str] = []
            if tool == "read_file":
                evs.append(f"path={args.get('target_file')}")
                if args.get("offset") is not None:
                    evs.append(f"offset={args.get('offset')}")
                if args.get("limit") is not None:
                    evs.append(f"limit={args.get('limit')}")
            elif tool == "grep":
                evs.append(f"pattern={args.get('pattern')}")
                if args.get("path"):
                    evs.append(f"path={args.get('path')}")
                if args.get("glob"):
                    evs.append(f"glob={args.get('glob')}")
            elif tool == "apply_patch":
                # patch å†…å®¹å¯èƒ½å¾ˆé•¿ï¼šåªæç¤ºâ€œå·²æäº¤ patchâ€ï¼Œè¯¦æƒ…çœ‹äº‹ä»¶çª—æ ¼
                evs.append("patch=*** Begin Patch â€¦")
            elif tool in {"run_terminal_cmd", "run_cmd"}:
                cmd = args.get("command") or args.get("cmd")
                evs.append(f"command={cmd}")
                if args.get("is_background") is not None:
                    evs.append(f"is_background={args.get('is_background')}")
            elif tool in {"web_search", "webfetch"}:
                q = args.get("search_term") or args.get("url")
                evs.append(f"q={q}")
            else:
                # é»˜è®¤æç‚¼å°‘é‡å…³é”®å­—æ®µ
                for k in ("target_file", "target_directory", "glob_pattern", "query", "explanation", "name"):
                    if k in args and args.get(k) is not None:
                        evs.append(f"{k}={args.get(k)}")
            return [str(x) for x in evs if x not in ("None", "")]

        def compose(self) -> ComposeResult:
            yield Header(show_clock=True)
            yield _Log(id="header_panel")
            with Horizontal(id="main"):
                with Vertical(id="left"):
                    yield _Log(id="conversation")
                with Vertical(id="right"):
                    yield _Log(id="ops")
            with Horizontal(id="input_row"):
                yield Input(placeholder=self._input_placeholder_normal, id="input")
            # äº‹ä»¶çª—æ ¼ï¼šçº¯æ—¥å¿—è¾“å‡ºï¼ˆä¸æ”¯æŒæŠ˜å /å±•å¼€ï¼Œä¾¿äºâ€œé¡ºåºå›æ”¾ + å¤åˆ¶ç²˜è´´â€æ’éšœï¼‰
            yield _Log(id="events")
            yield Footer()

        def on_mount(self) -> None:
            # ä¸ºæ¯ä¸ªâ€œçª—å£â€è®¾ç½®è¾¹æ¡†æ ‡é¢˜ï¼ˆå¯¹é½ enhanced çš„åˆ†åŒºå‘½åï¼‰
            header_panel = self.query_one("#header_panel", _Log)
            conversation = self.query_one("#conversation", _Log)
            ops_panel = self.query_one("#ops", _Log)
            events_panel = self.query_one("#events", _Log)
            input_box = self.query_one("#input", Input)

            header_panel.border_title = "clude chat"
            conversation.border_title = "å¯¹è¯/è¾“å‡º"
            ops_panel.border_title = "æ“ä½œé¢æ¿"
            events_panel.border_title = "äº‹ä»¶"
            input_box.border_title = "you"

            # æ ‡é¢˜å±…ä¸­ï¼ˆTextual æ”¯æŒ border_title_alignï¼‰
            for w in (header_panel, conversation, ops_panel, events_panel, input_box):
                try:
                    w.border_title_align = "center"  # type: ignore[attr-defined]
                except Exception:
                    pass

            # ç¡®ä¿è¾“å…¥æ¡†å¯ç”¨ä¸”é»˜è®¤è·å¾—ç„¦ç‚¹
            try:
                input_box.focus()
            except Exception:
                pass

            # åˆå§‹çŠ¶æ€ï¼šè¿›å…¥ç•Œé¢å³â€œç­‰å¾…è¾“å…¥â€ï¼Œä¸è¦æ²¿ç”¨ä¸Šæ¬¡çš„ DONE/æ‰§è¡Œæ€
            self._state = "IDLE"
            self._operation = "ç­‰å¾…ä¸­"
            self._last_step = "-"
            self._last_event = "ready"
            self._active_tasks = 0
            self._recent_completed.clear()

            # è¿›å…¥ç•Œé¢å³è¾“å‡ºé¡¹ç›®è®°å¿†åŠ è½½å—ï¼ˆå¯¹é½ enhancedï¼šæ— éœ€ç­‰åˆ°ç¬¬ä¸€è½® run_turnï¼‰
            try:
                from clude_code.orchestrator.agent_loop.prompts import load_project_memory

                _txt, meta = load_project_memory(getattr(cfg, "workspace_root", "."))
                loaded = bool(meta.get("loaded"))
                path = str(meta.get("path", ""))
                truncated = bool(meta.get("truncated", False))
                length = meta.get("length")
                legacy = bool(meta.get("legacy_name", False))
                if loaded:
                    conversation.write(Text("â”Œâ”€ é¡¹ç›®è®°å¿†å·²åŠ è½½ï¼ˆCLUDE.mdï¼‰", style="cyan"))
                    conversation.write(Text(f"â”‚ path={path}", style="cyan"))
                    conversation.write(Text(f"â”‚ length={length}", style="cyan"))
                    conversation.write(Text(f"â”‚ truncated={truncated}", style="cyan"))
                    conversation.write(Text(f"â”‚ legacy_name={legacy}", style="cyan"))
                    conversation.write(Text("â””â”€", style="cyan"))
                else:
                    conversation.write(Text("â”Œâ”€ æœªåŠ è½½é¡¹ç›®è®°å¿†ï¼ˆCLUDE.mdï¼‰", style="cyan"))
                    conversation.write(Text(f"â”‚ path={path}", style="cyan"))
                    conversation.write(Text("â”‚ åŸå› ï¼šæ–‡ä»¶ä¸å­˜åœ¨/ä¸ºç©º/è¯»å–å¤±è´¥", style="cyan"))
                    conversation.write(Text("â””â”€", style="cyan"))
                self._project_memory_shown = True
            except Exception:
                # å¤±è´¥ä¸é˜»å¡ UI
                pass

            self._refresh_header_panel()
            self._refresh_ops()

            self.query_one("#events", _Log).write(
                Text("æç¤ºï¼šäº‹ä»¶çª—æ ¼ä¸ºé¡ºåºæ—¥å¿—ï¼ˆä¸æ”¯æŒæŠ˜å /å±•å¼€ï¼‰ï¼›å…³é”®äº‹ä»¶ä¼šè¾“å‡ºæ‘˜è¦ + JSONã€‚", style="dim")
            )
            self.set_interval(0.05, self._drain_events)
            # ä»…ç”¨äºâ€œæ­£åœ¨è¯·æ±‚ä¸­â€çš„ spinner åŠ¨ç”»ï¼šè½»é‡åˆ·æ–° ops
            self.set_interval(0.15, self._tick_ops_spinner)

        def _tick_ops_spinner(self) -> None:
            """è¯·æ±‚è¿›è¡Œä¸­æ—¶åˆ·æ–° spinnerï¼ˆä¸ä¾èµ–æ–°äº‹ä»¶åˆ°æ¥ï¼‰ã€‚"""
            if self._active_llm_id is None and not self._busy:
                return
            self._spinner_idx = (self._spinner_idx + 1) % 10_000
            try:
                self._refresh_ops()
            except Exception:
                pass

        def _refresh_header_panel(self) -> None:
            """é¡¶éƒ¨ `clude chat` çª—å£ï¼šæ‰¿è½½ enhanced é¡¶æ é‡Œçš„å…³é”®è¿è¡Œæ€ä¿¡æ¯ã€‚"""
            hp = self.query_one("#header_panel", _Log)
            hp.clear()

            # ç¬¬ä¸€è¡Œï¼šå¯¹é½ enhancedï¼ˆæ¨¡å¼/çŠ¶æ€/æ“ä½œ/è¿è¡Œä¿¡æ¯æ¨ªå‘æ’å¸ƒï¼‰
            row1 = Table.grid(expand=True)
            row1.add_column(justify="left", ratio=5, no_wrap=True)
            row1.add_column(justify="left", ratio=2, no_wrap=True)
            row1.add_column(justify="left", ratio=3, no_wrap=True)
            row1.add_column(justify="right", ratio=4, no_wrap=True)

            t_mode = Text("æ¨¡å¼: Clude Code é£æ ¼ï¼ˆopencodeï¼‰")
            t_state = Text(f"çŠ¶æ€: {self._state}")
            t_op = Text(f"æ“ä½œ: {self._operation}")
            t_run = Text(f"è¿è¡Œ: step={self._last_step}  ev={self._last_event}", style="dim")

            row1.add_row(t_mode, t_state, t_op, t_run)
            hp.write(row1)

            # ç¬¬äºŒè¡Œï¼šContext/Output/TPS
            hp.write(Text(f"  {self._render_top_metrics()}", style="dim"))

            # ç¬¬ä¸‰è¡Œï¼šæŠŠåŸâ€œçŠ¶æ€â€çª—æ ¼ä¿¡æ¯åˆå¹¶è¿›æ¥ï¼ˆæ¨¡å‹/åœ°å€/ä»»åŠ¡ï¼‰
            row3 = Table.grid(expand=True)
            row3.add_column(justify="left", ratio=3, no_wrap=True)
            row3.add_column(justify="left", ratio=5, no_wrap=True)
            row3.add_column(justify="right", ratio=4, no_wrap=True)
            model = (self._model or "auto")[:48]
            base = (self._base_url or "")[:80]
            t_model = Text(f"æ¨¡å‹: {model}", style="dim")
            t_base = Text(f"åœ°å€: {base}" if base else "åœ°å€: -", style="dim")
            t_tasks = Text(f"ä»»åŠ¡: {self._active_tasks} æ´»è·ƒ / {len(self._recent_completed)} æœ€è¿‘å®Œæˆ", style="dim")
            row3.add_row(t_model, t_base, t_tasks)
            hp.write(row3)

        def _refresh_ops(self) -> None:
            """åˆ·æ–°å³ä¾§â€œæ“ä½œé¢æ¿â€çª—æ ¼ï¼ˆå¯¹é½ enhanced çš„å¿«ç…§ä¿¡æ¯ï¼‰ã€‚"""
            ops = self.query_one("#ops", _Log)
            ops.clear()
            # ç»“æ„åŒ–ï¼šLLM + Tool å¿«ç…§ï¼ˆä¾¿äºæ’æŸ¥ï¼‰
            llm_t = Table(show_header=False, box=None, pad_edge=False)
            llm_t.add_column(justify="left", style="bold", width=8)
            llm_t.add_column(justify="left")
            if self._last_llm_messages is not None:
                llm_t.add_row("LLM", f"messages={self._last_llm_messages}")
            ops.write(llm_t)

            ops.write(Text(""))

            # LLM è¯·æ±‚å†å²ï¼ˆå¤šæ¡ï¼‰
            if self._llm_requests:
                ops.write(Text("LLM è¯·æ±‚", style="bold dim"))
                for r in list(self._llm_requests)[-6:]:
                    done = (r.get("status") == "done")
                    start_ts = float(r.get("start_ts") or time.time())
                    if done and isinstance(r.get("elapsed_ms"), int):
                        elapsed_s = float(r["elapsed_ms"]) / 1000.0
                    else:
                        elapsed_s = time.time() - start_ts
                    purpose = str(r.get("purpose") or "") or str(r.get("kind") or "åˆ†æ")
                    ops.write(
                        self._format_llm_progress_line(
                            idx=int(r.get("idx") or 0),
                            purpose=purpose,
                            done=done,
                            elapsed_s=elapsed_s,
                        )
                    )
                ops.write(Text(""))

            if self._last_tool:
                args = f" {self._last_tool_args}" if self._last_tool_args else ""
                t = Text()
                t.append("tool: ", style="bold yellow")
                t.append(f"{self._last_tool}{args}")
                ops.write(t)
            if self._last_tool_result:
                ops.write(self._last_tool_result)
            if self._busy:
                ops.write(Text("â€¦æ‰§è¡Œä¸­ï¼ˆopencode TUIï¼‰", style="dim"))

        def _set_follow(self, follow: bool) -> None:
            self._follow = bool(follow)
            # RichLog panes æ”¯æŒ auto_scroll
            for wid in ("#conversation", "#ops", "#events"):
                try:
                    self.query_one(wid, _Log).auto_scroll = self._follow
                except Exception:
                    pass

        def action_toggle_follow(self) -> None:
            self._set_follow(not self._follow)
            try:
                self.query_one("#events", _Log).write(Text(f"follow={'on' if self._follow else 'off'}", style="dim"))
            except Exception:
                pass

        def action_jump_bottom(self) -> None:
            # å›åˆ°åº•éƒ¨å¹¶å¼€å¯ follow
            self._set_follow(True)
            for wid in ("#conversation", "#ops", "#events"):
                try:
                    self.query_one(wid, _Log).scroll_end(animate=False)
                except Exception:
                    pass

        def _render_top_metrics(self) -> str:
            if self._max_tokens > 0:
                pct = (self._llm_prompt / self._max_tokens) * 100 if self._max_tokens else 0.0
                ctx = f"Context: {self._llm_prompt}/{self._max_tokens} ({pct:.0f}%)"
            else:
                ctx = f"Context: {self._llm_prompt}"
            return f"{ctx}    Output: {self._llm_completion}/âˆ    {self._tps:.1f} tokens/sec"

        def _append_event_line(
            self,
            et: str,
            data: dict[str, Any],
            *,
            step: int | str | None = None,
            trace_id: str | None = None,
        ) -> None:
            """
            äº‹ä»¶çª—æ ¼ï¼ˆæ—¥å¿—ï¼‰ï¼š
            - æ¯æ¡äº‹ä»¶è¾“å‡ºä¸€è¡Œå¼ºæ‘˜è¦ï¼ˆå¯æ‰«è¯»ï¼‰
            - å…³é”®äº‹ä»¶è¾“å‡ºæ ¼å¼åŒ– JSONï¼ˆå¯å¤åˆ¶ç²˜è´´æ’éšœï¼‰
            """
            events = self.query_one("#events", _Log)

            ts = self._now_hhmmss()
            st = "-" if step is None else str(step)
            tr = (trace_id or "").strip()
            tr8 = tr[:8] if tr else "-"
            phase = self._event_phase()
            lvl = self._event_level_code(et, data)
            icon, icon_style = self._event_icon(lvl, et, data)
            summary = self._one_line(self._event_summary(et, data), 150)

            label_text = Text()
            label_text.append(f"[{ts}] ", style="dim")
            label_text.append(f"{icon} ", style=icon_style)
            label_text.append(f"{phase} ", style="dim")
            label_text.append(f"step={st} ", style="dim")
            label_text.append(et, style=self._event_name_style(et))
            label_text.append("  ", style="dim")
            label_text.append(f"trace={tr8}", style="dim cyan")
            if summary:
                label_text.append("  |  ", style="dim")
                label_text.append(summary, style="white" if lvl != "D" else "dim")

            events.write(label_text)

            # è¾“å‡º JSONï¼ˆåªå¯¹å…³é”®äº‹ä»¶ï¼Œé¿å…åˆ·å±ï¼‰
            if et in {
                "llm_request_params",
                "llm_usage",
                "tool_call_parsed",
                "tool_result",
                "plan_generated",
                "replan_generated",
                "plan_parse_failed",
                "policy_deny_tool",
                "policy_deny_cmd",
            }:
                try:
                    s = json.dumps(data, ensure_ascii=False, default=str, indent=2)
                except Exception:
                    s = str(data)
                if len(s) > 8000:
                    s = s[:7999] + "â€¦"
                events.write(Syntax(s, "json", word_wrap=True, line_numbers=False))

            if self._follow:
                try:
                    events.scroll_end(animate=False)
                except Exception:
                    pass

        def _apply_event(self, ev: dict[str, Any]) -> None:
            et = str(ev.get("event", ""))
            data = ev.get("data", {}) or {}
            if "step" in ev and ev.get("step") is not None:
                self._last_step = ev.get("step")  # type: ignore[assignment]
            self._last_event = et or self._last_event
            trace_id = ev.get("trace_id")

            conversation = self.query_one("#conversation", _Log)
            ops = self.query_one("#ops", _Log)

            self._append_event_line(et, data, step=ev.get("step"), trace_id=trace_id)

            # --- å¯¹è¯/è¾“å‡ºï¼šchat é»˜è®¤æ—¥å¿—æµï¼ˆä½ è¦æ±‚çš„æ ¼å¼ï¼‰ ---
            if self._conversation_mode == "log":
                if et == "turn_start":
                    self._llm_round = 0
                    self._push_chat_log(f"å¼€å§‹æ–°çš„ä¸€è½®å¯¹è¯ trace_id={trace_id}", style="bold cyan")
                elif et == "user_message":
                    self._push_chat_log(f"ç”¨æˆ·è¾“å…¥: {data.get('text')}", style="white")
                elif et == "intent_classified":
                    cat = data.get("category")
                    conf = data.get("confidence")
                    # category å¯èƒ½æ˜¯ {value: "..."} æˆ–å­—ç¬¦ä¸²
                    if isinstance(cat, dict) and "value" in cat:
                        cat = cat.get("value")
                    self._push_chat_log(f"æ„å›¾è¯†åˆ«ç»“æœ: {cat} (ç½®ä¿¡åº¦: {conf})", style="dim")
                elif et == "planning_skipped":
                    self._push_chat_log("æ£€æµ‹åˆ°èƒ½åŠ›è¯¢é—®æˆ–é€šç”¨å¯¹è¯ï¼Œè·³è¿‡æ˜¾å¼è§„åˆ’é˜¶æ®µã€‚", style="dim")
                elif et == "user_content_built":
                    prev = str(data.get("preview") or "")
                    trunc = bool(data.get("truncated"))
                    if trunc:
                        prev = prev + "â€¦"
                    self._push_chat_log(f"user input LLM  user_content={prev}", style="dim")
                elif et == "llm_request_params":
                    # ä»¥ params äº‹ä»¶ä½œä¸ºâ€œæœ¬è½®è¯·æ±‚â€è®¡æ•°å…¥å£ï¼ˆåŒ…å« stage/messagesï¼‰
                    self._llm_round += 1
                    mc = data.get("messages_count")
                    self._push_chat_log(f"â†’ ç¬¬ {self._llm_round} è½®ï¼šè¯·æ±‚ LLMï¼ˆæ¶ˆæ¯æ•°={mc}ï¼‰", style="bold")
                    self._push_chat_log(
                        f"LLM è¯·æ±‚å‚æ•°: model={data.get('model')} api_mode={data.get('api_mode')} messages={data.get('messages_count')}",
                        style="dim",
                    )
                elif et == "llm_response_data":
                    self._push_chat_log(f"LLM è¿”å›æ‘˜è¦: text_length={data.get('text_length')}", style="dim")
                elif et == "tool_call_parsed":
                    tool = str(data.get("tool") or "")
                    args = data.get("args") or {}
                    self._push_chat_log(
                        f"ğŸ”§ è§£æåˆ°å·¥å…·è°ƒç”¨: {tool} [è½®æ¬¡] {self._llm_round} [å‚æ•°] {self._format_args_one_line(args)}",
                        style="yellow",
                    )
                    self._push_chat_log(f"â–¶ æ‰§è¡Œå·¥å…·: {tool}", style="yellow")
                elif et == "display":
                    # display å±äº Agent ä¸»åŠ¨è¾“å‡º
                    content = str(data.get("content") or "").strip()
                    if content:
                        self._push_chat_log(f"â„¹ï¸ {self._one_line(content, 240)}", style="cyan")
                elif et == "tool_result":
                    tool = str(data.get("tool") or "")
                    ok = bool(data.get("ok"))
                    icon = "âœ“" if ok else "âœ—"
                    style = "green" if ok else "red"
                    summary = self._format_tool_result_summary(tool, ok, data)
                    self._push_chat_log(f"{icon} å·¥å…·æ‰§è¡Œ{'æˆåŠŸ' if ok else 'å¤±è´¥'}: {tool} [ç»“æœ] {summary}", style=style)
                elif et == "confirm_request":
                    # äº¤äº’ç¡®è®¤æç¤ºï¼ˆç”±åå°çº¿ç¨‹å‘èµ·ï¼‰
                    cid = data.get("id")
                    msg = str(data.get("message") or "").strip()
                    if isinstance(cid, int):
                        self._pending_confirm_id = cid
                        self._pending_confirm_msg = msg
                        self._push_chat_log("âš  éœ€è¦ç¡®è®¤ï¼ˆè¾“å…¥ y/n åå›è½¦ï¼‰ï¼š", style="bold yellow")
                        if msg:
                            for ln in msg.splitlines()[:12]:
                                self._push_chat_log(f"  {ln}", style="yellow")
                        inp = self.query_one("#input", Input)
                        inp.placeholder = "ç¡®è®¤ï¼šè¾“å…¥ y/n åå›è½¦ï¼ˆy=å…è®¸ï¼Œn=æ‹’ç»ï¼‰"
                        try:
                            inp.focus()
                        except Exception:
                            pass
                        self._refresh_header_panel()

            def _push_block(title: str, lines: list[str], *, color: str = "cyan") -> None:
                """åœ¨å¯¹è¯çª—æ ¼è¾“å‡º Claude Code é£æ ¼é˜¶æ®µå—ï¼ˆå¯¹é½ enhanced çš„è§†è§‰è¯­è¨€ï¼‰ã€‚"""
                title = (title or "").strip()
                if not title:
                    return
                conversation.write(Text(f"â”Œâ”€ {title}", style=color))
                for ln in lines:
                    ln = (ln or "").strip()
                    if not ln:
                        continue
                    conversation.write(Text(f"â”‚ {ln}", style=color))
                conversation.write(Text("â””â”€", style=color))
                if self._follow:
                    conversation.scroll_end(animate=False)

            # --- çŠ¶æ€æœº ---
            if et == "state":
                st = str(data.get("state", "")).strip()
                if st:
                    self._state = st
                self._operation = str(data.get("reason") or data.get("step") or data.get("mode") or "è¿è¡Œä¸­")
                self._active_tasks = 1 if self._busy else 0
                self._refresh_header_panel()
                if self._conversation_mode != "log":
                    # å¯¹è¯åŒºï¼šç»“æ„åŒ–å—
                    self._push_structured_block(
                        title="çŠ¶æ€åˆ‡æ¢",
                        level="progress" if self._state in {"PLANNING", "EXECUTING"} else "info",
                        step=ev.get("step"),
                        ev=et,
                        trace_id=trace_id,
                        summary=f"state={self._state}",
                        decision=str(data.get("reason") or data.get("step") or data.get("mode") or ""),
                    )
                return

            # å¼€åœºï¼šé¡¹ç›®è®°å¿†åŠ è½½çŠ¶æ€ï¼ˆå¯¹é½ enhanced çš„â€œé¡¹ç›®è®°å¿†å·²åŠ è½½ï¼ˆCLUDE.mdï¼‰â€å—ï¼‰
            if et == "project_memory":
                # å·²åœ¨ on_mount è¾“å‡ºè¿‡ä¸€æ¬¡ï¼Œé¿å…ç”¨æˆ·è¾“å…¥åå†æ¬¡é‡å¤åˆ·å±
                if self._project_memory_shown:
                    return
                loaded = bool(data.get("loaded"))
                path = str(data.get("path", ""))
                truncated = bool(data.get("truncated", False))
                length = data.get("length")
                legacy = bool(data.get("legacy_name", False))
                if loaded:
                    _push_block(
                        "é¡¹ç›®è®°å¿†å·²åŠ è½½ï¼ˆCLUDE.mdï¼‰",
                        [
                            f"path={path}",
                            f"length={length}",
                            f"truncated={truncated}",
                            f"legacy_name={legacy}",
                        ],
                        color="cyan",
                    )
                else:
                    _push_block(
                        "æœªåŠ è½½é¡¹ç›®è®°å¿†ï¼ˆCLUDE.mdï¼‰",
                        [f"path={path}", "åŸå› ï¼šæ–‡ä»¶ä¸å­˜åœ¨/ä¸ºç©º/è¯»å–å¤±è´¥"],
                        color="cyan",
                    )
                self._project_memory_shown = True
                return

            # è§„åˆ’/æ‰§è¡Œé˜¶æ®µï¼ˆå¯¹é½ enhancedï¼‰
            if et == "plan_step_start":
                idx = data.get("idx")
                total = data.get("total")
                step_id = data.get("step_id")
                self._state = "EXECUTING"
                self._operation = f"æ‰§è¡Œæ­¥éª¤ {idx}/{total}: {step_id}"
                self._active_tasks = 1 if self._busy else 0
                self._refresh_header_panel()
                if self._conversation_mode != "log":
                    self._push_structured_block(
                        title="æ‰§è¡Œæ­¥éª¤å¼€å§‹",
                        level="progress",
                        step=ev.get("step"),
                        ev=et,
                        trace_id=trace_id,
                        summary=f"{idx}/{total}  step_id={step_id}",
                        decision="å¼€å§‹æ‰§è¡Œæœ¬æ­¥éª¤ï¼›åç»­å°†æ ¹æ®æ¨¡å‹è¾“å‡ºè°ƒç”¨å·¥å…·å¹¶å›å–‚ç»“æœã€‚",
                        evidence=[f"step_id={step_id}"],
                    )
                return

            if et == "user_message":
                if self._conversation_mode != "log":
                    txt = str(data.get("text", "")).strip()
                    if txt:
                        t = Text()
                        t.append("you: ", style="bold blue")
                        t.append(txt)
                        conversation.write(t)
                        if self._follow:
                            conversation.scroll_end(animate=False)
                return

            if et in {"intent_classified"}:
                cat = data.get("category")
                conf = data.get("confidence")
                self._push_structured_block(
                    title="æ„å›¾è¯†åˆ«",
                    level="info",
                    step=ev.get("step"),
                    ev=et,
                    trace_id=trace_id,
                    summary=f"category={cat} confidence={conf}",
                    decision="ç”¨äºå†³å®šæ˜¯å¦è¿›å…¥ planning é˜¶æ®µï¼Œä»¥åŠå·¥å…·/éªŒè¯ç­–ç•¥çš„ä¼˜å…ˆçº§ã€‚",
                )
                return

            if et in {"planning_llm_request"}:
                self._state = "PLANNING"
                self._operation = "è§„åˆ’ï¼šLLM è¯·æ±‚"
                self._active_tasks = 1
                self._refresh_header_panel()
                # è®°å½•ä¸€æ¬¡ LLM è¯·æ±‚ï¼ˆè§„åˆ’ï¼‰
                if self._active_llm_id is None:
                    self._llm_start(kind="planning", step_id=None, purpose="ç”Ÿæˆè®¡åˆ’")
                    self._refresh_ops()
                self._push_structured_block(
                    title="è¿›å…¥è§„åˆ’é˜¶æ®µï¼ˆç”Ÿæˆ Planï¼‰",
                    level="progress",
                    step=ev.get("step"),
                    ev=et,
                    trace_id=trace_id,
                    summary=f"attempt={data.get('attempt')}",
                    decision="å°†ä»»åŠ¡æ‹†æˆå¯æ‰§è¡Œæ­¥éª¤ï¼Œé™ä½ä¸€æ¬¡æ€§é•¿ä¸Šä¸‹æ–‡å¤±è´¥æ¦‚ç‡ï¼Œå¹¶æé«˜å¯è¿½æº¯æ€§ã€‚",
                )
                return

            if et in {"plan_generated"}:
                title = str(data.get("title") or "").strip()
                steps = data.get("steps")
                preview = data.get("steps_preview") or []
                evs: list[str] = []
                if isinstance(preview, list):
                    for p in preview[:8]:
                        evs.append(str(p))
                self._push_structured_block(
                    title="è®¡åˆ’å·²ç”Ÿæˆï¼ˆPlanï¼‰",
                    level="success",
                    step=ev.get("step"),
                    ev=et,
                    trace_id=trace_id,
                    summary=f"title={title} steps={steps}",
                    decision="æ¥ä¸‹æ¥ä¼šæŒ‰æ­¥éª¤æ‰§è¡Œï¼šæ¯æ­¥ä¼šè§¦å‘ LLMâ†’å·¥å…·â†’å›å–‚â†’ï¼ˆå¯é€‰ï¼‰éªŒè¯çš„é—­ç¯ã€‚",
                    evidence=evs,
                    hint="æ›´å¤šç»“æ„åŒ–ç»†èŠ‚è§â€œäº‹ä»¶â€çª—æ ¼ï¼ˆplan_generated JSONï¼‰ã€‚",
                )
                return

            if et in {"plan_parse_failed"}:
                self._push_structured_block(
                    title="è®¡åˆ’è§£æå¤±è´¥",
                    level="error",
                    step=ev.get("step"),
                    ev=et,
                    trace_id=trace_id,
                    summary=f"attempt={data.get('attempt')} error={data.get('error')}",
                    decision="å°†è¦æ±‚æ¨¡å‹ä»…è¾“å‡ºä¸¥æ ¼ JSONï¼Œè§¦å‘é‡è¯•ï¼ˆæˆ–é™çº§åˆ° ReActï¼‰ã€‚",
                    hint="å»ºè®®ç¼©å°ä»»åŠ¡ã€æé«˜ç»“æ„åŒ–çº¦æŸï¼Œæˆ–æŒ‡å®šå…¥å£æ–‡ä»¶ã€‚",
                )
                return

            if et in {"assistant_text", "assistant"}:
                if self._conversation_mode != "log":
                    txt = str(data.get("text", "")).strip()
                    if txt:
                        t = Text()
                        t.append("assistant: ", style="bold magenta")
                        t.append(txt)
                        conversation.write(t)
                        if self._follow:
                            conversation.scroll_end(animate=False)
                # å¯¹é½ enhancedï¼šassistant_text è§†ä¸ºæœ¬è½®å·²ç»“æŸ
                self._state = "DONE"
                self._operation = "æœ¬è½®ç»“æŸ"
                self._active_tasks = 0
                self._refresh_header_panel()
                self._refresh_ops()
                return

            if et == "display":
                if self._conversation_mode != "log":
                    content = str(data.get("content", "")).strip()
                    level = str(data.get("level") or "info")
                    title = str(data.get("title") or "Agent è¾“å‡º").strip()
                    thought = str(data.get("thought") or "").strip()
                    explanation = str(data.get("explanation") or "").strip()
                    ev_lines = data.get("evidence")
                    evidence: list[str] | None = None
                    if isinstance(ev_lines, list):
                        evidence = [str(x) for x in ev_lines if str(x).strip()]
                    if content:
                        self._push_structured_block(
                            title=title,
                            level=level,
                            step=ev.get("step"),
                            ev=et,
                            trace_id=trace_id,
                            summary=content,
                            decision=(thought or explanation),
                            evidence=evidence,
                            hint="ï¼ˆdisplay å·¥å…·è¾“å‡ºï¼‰",
                            force_show_decision=True,
                        )
                return

            if et == "llm_request":
                self._state = "EXECUTING"
                self._operation = "LLM è¯·æ±‚"
                mc = data.get("messages")
                if isinstance(mc, int) and mc >= 0:
                    self._last_llm_messages = mc
                self._active_tasks = 1
                self._refresh_header_panel()
                # è®°å½•ä¸€æ¬¡ LLM è¯·æ±‚ï¼ˆæ‰§è¡Œæ­¥éª¤ï¼‰
                if self._active_llm_id is None:
                    sid = str(data.get("step_id") or "") or None
                    self._llm_start(kind="execute_step", step_id=sid, purpose=(f"æ‰§è¡Œ {sid}" if sid else "æ‰§è¡Œæ­¥éª¤"))
                self._refresh_ops()
                if self._conversation_mode != "log":
                    conversation.write(Text("ğŸ¤– LLM è¯·æ±‚ä¸­...", style="dim"))
                    if self._follow:
                        conversation.scroll_end(animate=False)
                return

            if et == "llm_request_params":
                pt = data.get("prompt_tokens_est")
                if isinstance(pt, int) and pt >= 0:
                    self._llm_prompt = pt
                # é™„åŠ æœ¬æ¬¡è¯·æ±‚çš„å‚æ•°ï¼ˆç”¨äº ops å†å²å±•ç¤ºï¼‰
                try:
                    self._llm_attach_params(data)
                except Exception:
                    pass
                mc = data.get("messages_count")
                if isinstance(mc, int) and mc >= 0:
                    self._last_llm_messages = mc
                self._state = "EXECUTING"
                self._operation = "LLM è¯·æ±‚"
                self._active_tasks = 1
                self._refresh_header_panel()
                self._refresh_ops()
                self._push_structured_block(
                    title="LLM è¯·æ±‚å‚æ•°",
                    level="info",
                    step=ev.get("step"),
                    ev=et,
                    trace_id=trace_id,
                    summary=f"model={data.get('model')} api={data.get('api_mode')} base_url={data.get('base_url')}",
                    decision=f"temperature={data.get('temperature')} max_tokens={data.get('max_tokens')}",
                    evidence=[
                        f"prompt_tokens_est={data.get('prompt_tokens_est')}",
                        f"messages_count={data.get('messages_count')}",
                    ],
                    hint="å®Œæ•´å‚æ•°è§â€œäº‹ä»¶â€çª—æ ¼ï¼ˆllm_request_params JSONï¼‰ã€‚",
                )
                return

            if et == "llm_usage":
                pt = data.get("prompt_tokens_est")
                if isinstance(pt, int) and pt >= 0:
                    self._llm_prompt = pt
                ct = data.get("completion_tokens_est")
                if isinstance(ct, int) and ct >= 0:
                    self._llm_completion = ct
                elapsed_ms = data.get("elapsed_ms") or 0
                self._tps = (self._llm_completion / elapsed_ms) * 1000 if elapsed_ms else 0.0
                self._state = "EXECUTING"
                self._operation = "LLM è¿”å›"
                self._active_tasks = 0
                self._recent_completed.append("LLM")
                # ç»“ç®—æœ¬æ¬¡ LLM è¯·æ±‚è€—æ—¶
                try:
                    self._llm_finish(data)
                except Exception:
                    pass
                self._refresh_header_panel()
                self._refresh_ops()
                return

            if et == "tool_call_parsed":
                tool = str(data.get("tool", ""))
                args_str = str(data.get("args", {}) or {})
                if len(args_str) > 180:
                    args_str = args_str[:179] + "â€¦"
                self._last_tool = tool
                self._last_tool_args = args_str
                self._state = "EXECUTING"
                self._operation = f"å·¥å…·: {tool}"
                self._active_tasks = 1
                self._refresh_header_panel()
                self._refresh_ops()
                args = data.get("args", {}) or {}
                evs = self._summarize_tool_args(tool, args if isinstance(args, dict) else {})
                self._push_structured_block(
                    title=f"å·¥å…·è°ƒç”¨: {tool}",
                    level="progress",
                    step=ev.get("step"),
                    ev=et,
                    trace_id=trace_id,
                    summary="æ¨¡å‹å·²è§£æå‡ºå·¥å…·è°ƒç”¨ï¼Œå°†è¿›è¡Œç­–ç•¥æ ¡éªŒå¹¶æ‰§è¡Œå·¥å…·ã€‚",
                    decision="å¯¹è¯åŒºä»…å±•ç¤ºå…³é”®å‚æ•°æ‘˜è¦ï¼›è¯¦æƒ…è§â€œäº‹ä»¶/æ“ä½œé¢æ¿â€ã€‚",
                    evidence=evs or [f"args={args_str}"],
                )
                return

            if et == "tool_result":
                tool = str(data.get("tool", ""))
                ok = bool(data.get("ok"))
                err_obj = data.get("error")
                err = str(err_obj or "")
                if len(err) > 160:
                    err = err[:159] + "â€¦"
                icon = "âœ“ " if ok else "âœ— "
                icon_style = "green" if ok else "red"
                tr = Text()
                tr.append(icon, style=icon_style)
                tr.append(tool)
                if err:
                    tr.append("  ")
                    tr.append(err, style="dim")
                self._last_tool_result = tr
                self._state = "EXECUTING"
                self._operation = f"å·¥å…·å®Œæˆ: {tool}"
                self._active_tasks = 0
                self._recent_completed.append(f"{'âœ“' if ok else 'âœ—'} {tool}")
                self._refresh_header_panel()
                self._refresh_ops()
                level = "success" if ok else "error"
                code = ""
                if isinstance(err_obj, dict):
                    code = str(err_obj.get("code") or "")
                self._push_structured_block(
                    title=f"å·¥å…·ç»“æœ: {tool}",
                    level=level,
                    step=ev.get("step"),
                    ev=et,
                    trace_id=trace_id,
                    summary=f"ok={ok}" + (f" code={code}" if code else "") + (f" err={err}" if err and not ok else ""),
                    decision="å·¥å…·ç»“æœå·²å›å–‚ç»™æ¨¡å‹ï¼ˆä½œä¸ºåç»­æ¨ç†ä¾æ®ï¼‰ã€‚",
                    hint="æ›´å®Œæ•´çš„ payload/åŸå§‹é”™è¯¯è§â€œäº‹ä»¶/æ“ä½œé¢æ¿â€ã€‚",
                )
                return

            if et in {"policy_deny_tool", "policy_deny_cmd", "denied_by_user"}:
                self._push_structured_block(
                    title="ç­–ç•¥/ç¡®è®¤æ‹¦æˆª",
                    level="error",
                    step=ev.get("step"),
                    ev=et,
                    trace_id=trace_id,
                    summary=str(data),
                    decision="ä¸ºé¿å…å±é™©æ“ä½œï¼Œæœ¬æ¬¡è°ƒç”¨è¢«ç­–ç•¥æˆ–ç”¨æˆ·ç¡®è®¤æ‹’ç»ã€‚",
                    hint="å¦‚éœ€ç»§ç»­ï¼šè°ƒæ•´ allowed_tools/disallowed_tools æˆ–å…³é—­ confirm_write/confirm_execï¼ˆä¸æ¨èåœ¨ä¸å¯ä¿¡é¡¹ç›®ä¸­å…³é—­ï¼‰ã€‚",
                )
                return

        def _drain_events(self) -> None:
            drained = 0
            while drained < 200:
                try:
                    ev = q.get_nowait()
                except Empty:
                    break
                self._apply_event(ev)
                drained += 1

        # äº‹ä»¶çª—æ ¼å·²æ”¹ä¸ºçº¯æ—¥å¿—è¾“å‡ºï¼šä¸å†æ”¯æŒ Tree çš„å±•å¼€/æ”¶èµ·äº¤äº’

        def on_input_submitted(self, event: Input.Submitted) -> None:
            txt = (event.value or "").strip()
            self.query_one("#input", Input).value = ""
            if not txt:
                return

            # confirm æ¨¡å¼ï¼šä¼˜å…ˆæ¶ˆè´¹è¾“å…¥ï¼ˆå³ä½¿ _busy=Trueï¼‰
            if self._pending_confirm_id is not None:
                v = txt.strip().lower()
                allow = v in {"y", "yes", "å…è®¸", "æ˜¯", "ç¡®è®¤", "ok"}
                deny = v in {"n", "no", "æ‹’ç»", "å¦", "å–æ¶ˆ", "cancel"}
                if not (allow or deny):
                    self.query_one("#events", _Log).write(Text("è¯·è¾“å…¥ y æˆ– nï¼ˆç¡®è®¤æ¨¡å¼ï¼‰", style="yellow"))
                    return
                cid = self._pending_confirm_id
                with _confirm_lock:
                    waiter = _confirm_waiters.get(cid)
                    if waiter is not None:
                        waiter["allow"] = bool(allow)
                        ev0: threading.Event = waiter["event"]
                        ev0.set()
                self._push_chat_log(f"confirm: {'å…è®¸' if allow else 'æ‹’ç»'}", style="green" if allow else "red")
                self._pending_confirm_id = None
                self._pending_confirm_msg = None
                inp = self.query_one("#input", Input)
                inp.placeholder = self._input_placeholder_normal
                try:
                    inp.focus()
                except Exception:
                    pass
                self._refresh_header_panel()
                return

            if self._busy:
                self.query_one("#events", _Log).write("[yellow]å½“å‰æ­£åœ¨æ‰§è¡Œä¸Šä¸€æ¡è¯·æ±‚ï¼Œè¯·ç¨å€™â€¦[/yellow]")
                return
            if txt.lower() in {"exit", "quit", "/exit", "/quit"}:
                self.exit()
                return

            # å…ˆå†™å…¥æœ¬åœ°å¯¹è¯æ¡†ï¼ˆä¸ AgentLoop çš„ user_message äº‹ä»¶ä¿æŒä¸€è‡´ï¼‰
            try:
                q.put_nowait({"event": "user_message", "data": {"text": txt}})
            except Exception:
                pass

            self._busy = True

            def _worker() -> None:
                def _confirm(_msg: str) -> bool:
                    # äº¤äº’ç¡®è®¤ï¼šåå°çº¿ç¨‹é˜»å¡ç­‰å¾… UI è¾“å…¥ y/n
                    nonlocal _confirm_seq
                    with _confirm_lock:
                        cid = int(_confirm_seq)
                        _confirm_seq += 1
                        ev0 = threading.Event()
                        _confirm_waiters[cid] = {"event": ev0, "allow": None, "msg": _msg}
                    try:
                        q.put_nowait({"event": "confirm_request", "data": {"id": cid, "message": _msg}})
                    except Exception:
                        # UI é˜Ÿåˆ—å†™å¤±è´¥ï¼šå…œåº•æ‹’ç»
                        with _confirm_lock:
                            _confirm_waiters.pop(cid, None)
                        return False
                    # ç­‰å¾…ç”¨æˆ·è¾“å…¥ï¼ˆé»˜è®¤ä¸è¶…æ—¶ï¼›é˜²æ­¢å¡æ­»å¯åŠ ä¸Šé™ï¼‰
                    ok = ev0.wait(timeout=60 * 30)
                    with _confirm_lock:
                        allow = bool((_confirm_waiters.get(cid) or {}).get("allow")) if ok else False
                        _confirm_waiters.pop(cid, None)
                    return bool(allow)

                def _on_event(e: dict[str, Any]) -> None:
                    try:
                        q.put_nowait({"event": e.get("event"), "data": e.get("data", {}) or {}, "step": e.get("step")})
                    except Exception:
                        pass

                try:
                    run_turn(txt, _confirm, _on_event)
                finally:
                    self._busy = False

            Thread(target=_worker, daemon=True).start()

    OpencodeTUI().run()



