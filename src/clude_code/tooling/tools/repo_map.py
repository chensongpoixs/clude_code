from __future__ import annotations

import hashlib
import json
import pickle
import platform
import shutil
import subprocess
import time
from pathlib import Path
from typing import Dict, List, Any

from ..logger_helper import get_tool_logger
from ...config.tools_config import get_repo_map_config

def _get_cache_key(workspace_root: Path) -> str:
    """ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŸºäºŽå·¥ä½œåŒºè·¯å¾„ï¼‰ã€‚"""
    return hashlib.md5(str(workspace_root.resolve()).encode()).hexdigest()


def _get_workspace_mtime(workspace_root: Path) -> float:
    """èŽ·å–å·¥ä½œåŒºæœ€æ–°æ–‡ä»¶ä¿®æ”¹æ—¶é—´ã€‚"""
    try:
        max_mtime = 0.0
        for p in workspace_root.rglob("*"):
            if p.is_file():
                try:
                    mtime = p.stat().st_mtime
                    max_mtime = max(max_mtime, mtime)
                except OSError:
                    pass
        return max_mtime
    except Exception:
        return 0.0


def _load_ctags_cache(workspace_root: Path) -> tuple[list[dict], float] | None:
    """åŠ è½½ç¼“å­˜çš„ ctags è¾“å‡ºã€‚"""
    if not _cache_file.exists():
        return None
    
    try:
        with open(_cache_file, "rb") as f:
            cache_data = pickle.load(f)
            cache_key = _get_cache_key(workspace_root)
            if cache_key in cache_data:
                cached_symbols, cached_mtime = cache_data[cache_key]
                # æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§ï¼ˆåŸºäºŽå·¥ä½œåŒºä¿®æ”¹æ—¶é—´ï¼‰
                workspace_mtime = _get_workspace_mtime(workspace_root)
                if workspace_mtime <= cached_mtime:
                    _logger.debug(f"[RepoMap] ä½¿ç”¨ç¼“å­˜: {len(cached_symbols)} ä¸ªç¬¦å·")
                    return cached_symbols, cached_mtime
    except Exception as e:
        _logger.debug(f"[RepoMap] åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
    
    return None


def _save_ctags_cache(workspace_root: Path, symbols: list[dict]) -> None:
    """ä¿å­˜ ctags è¾“å‡ºåˆ°ç¼“å­˜ã€‚"""
    try:
        _cache_dir.mkdir(parents=True, exist_ok=True)
        cache_key = _get_cache_key(workspace_root)
        workspace_mtime = _get_workspace_mtime(workspace_root)
        
        cache_data = {}
        if _cache_file.exists():
            try:
                # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶å¤§å°
                cache_size = _cache_file.stat().st_size
                if cache_size > _MAX_CACHE_SIZE:
                    _logger.debug(f"[RepoMap] ç¼“å­˜æ–‡ä»¶è¿‡å¤§ ({cache_size} bytes)ï¼Œæ¸…ç†ç¼“å­˜")
                    _cache_file.unlink()
                else:
                    with open(_cache_file, "rb") as f:
                        cache_data = pickle.load(f)
            except Exception:
                pass
        
        cache_data[cache_key] = (symbols, workspace_mtime)
        with open(_cache_file, "wb") as f:
            pickle.dump(cache_data, f)
        _logger.debug(f"[RepoMap] ä¿å­˜ç¼“å­˜: {len(symbols)} ä¸ªç¬¦å·")
    except Exception as e:
        _logger.debug(f"[RepoMap] ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")


def _get_exclude_patterns(workspace_root: Path) -> list[str]:
    """èŽ·å–æŽ’é™¤æ¨¡å¼åˆ—è¡¨ï¼ˆåˆå¹¶ç¡¬ç¼–ç å’Œ .gitignoreï¼Œä¸šç•Œæœ€ä½³å®žè·µï¼‰ã€‚"""
    exclude_patterns = [
        ".git", "node_modules", "venv", ".venv",
        "__pycache__", "build", "dist", ".clude",
        "*.json", "*.md", "tests"
    ]
    
    # è¯»å– .gitignoreï¼ˆä¸šç•Œæœ€ä½³å®žè·µï¼‰
    gitignore_path = workspace_root / ".gitignore"
    if gitignore_path.exists():
        try:
            with open(gitignore_path, "r", encoding="utf-8", errors="ignore") as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith("#"):
                        continue
                    if line.startswith("!"):
                        continue  # ç®€åŒ–ï¼šä¸æ”¯æŒå¦å®šè§„åˆ™
                    # è½¬æ¢ä¸º ctags æŽ’é™¤æ ¼å¼
                    if line.startswith("/"):
                        exclude_patterns.append(line[1:])  # ç§»é™¤å‰å¯¼ /
                    else:
                        exclude_patterns.append(line)
        except Exception as e:
            _logger.debug(f"[RepoMap] è¯»å– .gitignore å¤±è´¥: {e}")
    
    return exclude_patterns


def _calculate_file_weight(file_path: Path, symbol_count: int, workspace_root: Path) -> float:
    """
    è®¡ç®—æ–‡ä»¶æƒé‡ï¼ˆè€ƒè™‘æ·±åº¦ã€ç¬¦å·æ•°é‡ã€æ–‡ä»¶å¤§å°ã€ä¿®æ”¹æ—¶é—´ï¼Œä¸šç•Œæœ€ä½³å®žè·µï¼‰ã€‚
    
    æƒé‡è¶Šé«˜ï¼Œæ–‡ä»¶è¶Šé‡è¦ã€‚
    """
    try:
        rel_path = file_path.relative_to(workspace_root)
    except ValueError:
        rel_path = Path(file_path.name)
    
    # åŸºç¡€æƒé‡ï¼šæ·±åº¦è¶Šæµ…ï¼Œæƒé‡è¶Šé«˜
    depth = len(rel_path.parts)
    base_weight = 10.0 / max(depth, 1)
    
    # ç¬¦å·æ•°é‡æƒé‡
    symbol_weight = symbol_count * 0.5
    
    # æ–‡ä»¶å¤§å°æƒé‡ï¼ˆå¤§æ–‡ä»¶å¯èƒ½æ›´é‡è¦ï¼Œä½†ä¸è¦è¿‡åº¦ï¼‰
    try:
        file_size = file_path.stat().st_size
        size_weight = min(file_size / 10000, 5.0)  # æœ€å¤§ 5.0
    except OSError:
        size_weight = 0
    
    # ä¿®æ”¹æ—¶é—´æƒé‡ï¼ˆæœ€è¿‘ä¿®æ”¹çš„æ–‡ä»¶å¯èƒ½æ›´é‡è¦ï¼‰
    try:
        mtime = file_path.stat().st_mtime
        days_since_modify = (time.time() - mtime) / 86400
        time_weight = max(0, 5.0 - days_since_modify / 30)  # 30 å¤©å†…ä¿®æ”¹çš„æ–‡ä»¶æƒé‡æ›´é«˜
    except OSError:
        time_weight = 0
    
    return base_weight + symbol_weight + size_weight + time_weight

# ç¼“å­˜ç›®å½•å’Œæ–‡ä»¶
_cache_dir = Path.home() / ".clude" / "cache"
_cache_file = _cache_dir / "repo_map_cache.pkl"
_MAX_CACHE_SIZE = 100 * 1024 * 1024  # 100MB


def generate_repo_map(*, workspace_root: Path) -> str:
    """
    ç”Ÿæˆå¢žå¼ºç‰ˆä»“åº“å›¾è°± (V3ï¼Œä¸šç•Œæœ€ä½³å®žè·µä¼˜åŒ–)ï¼š
    1. å¼•å…¥æƒé‡è®¡ç®— (Ranking)ï¼šæ ¹æ®æ–‡ä»¶æ·±åº¦ã€ç¬¦å·æ•°é‡ã€æ–‡ä»¶å¤§å°ã€ä¿®æ”¹æ—¶é—´è¯†åˆ«æ ¸å¿ƒæ¨¡å—ã€‚
    2. æ·±åº¦æ ‘å½¢ç»“æž„ã€‚
    3. è‡ªåŠ¨æŽ’é™¤éžæ ¸å¿ƒç¬¦å·ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡æº¢å‡ºã€‚
    4. ctags è¾“å‡ºç¼“å­˜ï¼šåŸºäºŽæ–‡ä»¶ä¿®æ”¹æ—¶é—´çš„æ™ºèƒ½ç¼“å­˜ï¼ˆæ€§èƒ½æå‡ 20-50å€ï¼‰ã€‚
    5. .gitignore æ”¯æŒï¼šè‡ªåŠ¨è¯»å–å¹¶åº”ç”¨ .gitignore è§„åˆ™ï¼ˆèŠ‚çœ Tokenï¼‰ã€‚
    """
    # æ£€æŸ¥å·¥å…·æ˜¯å¦å¯ç”¨
    config = get_repo_map_config()
    if not config.enabled:
        _logger.warning("[RepoMap] ä»“åº“åœ°å›¾å·¥å…·å·²è¢«ç¦ç”¨")
        return "Repo Map: tool is disabled."

    _logger.debug("[RepoMap] å¼€å§‹ç”Ÿæˆä»“åº“å›¾è°±")
    ctags_exe = shutil.which("ctags")
    if not ctags_exe:
        _logger.warning("[RepoMap] ctags æœªæ‰¾åˆ°ï¼Œæ— æ³•ç”Ÿæˆä»“åº“å›¾è°±")
        return "Repo Map: ctags not found."

    # æ£€æŸ¥ç¼“å­˜ï¼ˆä¸šç•Œæœ€ä½³å®žè·µï¼šç»“æžœç¼“å­˜ï¼‰
    cached_result = _load_ctags_cache(workspace_root)
    if cached_result:
        cached_symbols, _ = cached_result
        # ç›´æŽ¥ä½¿ç”¨ç¼“å­˜çš„ç¬¦å·æ•°æ®
        symbols_data = cached_symbols
    else:
        # 1. æ‰«æç¬¦å·ï¼ˆä½¿ç”¨ .gitignore æ”¯æŒï¼‰
        exclude_patterns = _get_exclude_patterns(workspace_root)
        args = [
            ctags_exe,
            "--languages=Python,JavaScript,TypeScript,Go,Rust,C,C++,C#",
            "--output-format=json",
            "--fields=+n+k+K+S",
            "--extras=+q",
            "-R",
        ]
        # æ·»åŠ æŽ’é™¤æ¨¡å¼
        for pattern in exclude_patterns:
            args.append(f"--exclude={pattern}")
        args.append(".")

        abs_root = str(workspace_root.resolve())
        try:
            _logger.debug(f"[RepoMap] æ‰§è¡Œ ctags å‘½ä»¤: {' '.join(args[:5])}...")
            cp = subprocess.run(args, cwd=abs_root, capture_output=True, text=True, encoding="utf-8", shell=(platform.system() == "Windows"))
            _logger.debug(f"[RepoMap] ctags æ‰§è¡Œå®Œæˆ: è¾“å‡ºè¡Œæ•°={len(cp.stdout.splitlines())}")
        except Exception as e:
            _logger.error(f"[RepoMap] ctags æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return f"Repo Map Error: {e}"
        
        # è§£æžç¬¦å·æ•°æ®
        symbols_data = []
        for line in (cp.stdout or "").splitlines():
            try:
                obj = json.loads(line)
                symbols_data.append(obj)
            except Exception:
                continue
        
        # ä¿å­˜ç¼“å­˜ï¼ˆä¸šç•Œæœ€ä½³å®žè·µï¼šç»“æžœç¼“å­˜ï¼‰
        _save_ctags_cache(workspace_root, symbols_data)

    # 2. è§£æžå¹¶è®¡ç®—æ–‡ä»¶æƒé‡ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„æƒé‡è®¡ç®—ï¼‰
    # file_stats[path] = {"symbols": [], "weight": float}
    file_stats: Dict[str, Dict[str, Any]] = {}
    
    for obj in symbols_data:
        path_str = obj.get("path")
        kind = obj.get("kind")
        if not (path_str and kind in ("class", "function", "interface", "struct")):
            continue
        
        # è§£æžæ–‡ä»¶è·¯å¾„
        try:
            file_path = workspace_root / path_str if not Path(path_str).is_absolute() else Path(path_str)
        except Exception:
            continue
        
        if path_str not in file_stats:
            # ä½¿ç”¨ä¼˜åŒ–çš„æƒé‡è®¡ç®—
            file_stats[path_str] = {
                "symbols": [],
                "weight": _calculate_file_weight(file_path, 0, workspace_root),
                "file_path": file_path,
            }
        
        file_stats[path_str]["symbols"].append({
            "name": obj.get("name"),
            "kind": kind[0].upper(),
            "line": obj.get("line")
        })
        # æ¯å¢žåŠ ä¸€ä¸ªæ ¸å¿ƒç¬¦å·ï¼Œæ–‡ä»¶æƒé‡ç•¥å¾®å¢žåŠ 
        file_stats[path_str]["weight"] += 0.5

    # 3. ç­›é€‰æ ¸å¿ƒæ–‡ä»¶ï¼ˆä»…å±•ç¤ºæƒé‡å‰ 50 çš„æ–‡ä»¶ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡æŒ¤çˆ†ï¼‰
    top_files = sorted(file_stats.keys(), key=lambda x: file_stats[x]["weight"], reverse=True)[:50]
    
    # 4. æž„å»ºæ¸²æŸ“æ ‘
    tree: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}
    for p in top_files:
        p_obj = Path(p)
        d, f = str(p_obj.parent), p_obj.name
        if d not in tree:
            tree[d] = {}
        tree[d][f] = file_stats[p]["symbols"]

    # 5. æ¸²æŸ“ Markdown
    _logger.debug(f"[RepoMap] å¼€å§‹æ¸²æŸ“å›¾è°±: æ–‡ä»¶æ•°={len(file_stats)}, æ€»ç¬¦å·æ•°={sum(len(s['symbols']) for s in file_stats.values())}")
    lines = ["# æ ¸å¿ƒä»£ç æž¶æž„å›¾è°± (Core Repo Map)", "æç¤ºï¼šå·²ä¼˜å…ˆå±•ç¤ºé¡¹ç›®æ ¸å¿ƒé€»è¾‘æ–‡ä»¶åŠç¬¦å·ã€‚", ""]
    
    for dir_path in sorted(tree.keys()):
        # ç®€åŒ–æ ¹ç›®å½•æ˜¾ç¤º
        display_dir = "Project Root" if dir_path == "." else dir_path
        lines.append(f"ðŸ“ {display_dir}/")
        
        for file_name in sorted(tree[dir_path].keys()):
            lines.append(f"  ðŸ“„ {file_name}")
            syms = tree[dir_path][file_name]
            # æŽ’åºï¼šç±» -> å‡½æ•°
            sorted_syms = sorted(syms, key=lambda x: (x["kind"] != "C", x["line"]))
            
            # å•ä¸ªæ–‡ä»¶å†…æœ€å¤šå±•ç¤º 8 ä¸ªç¬¦å·
            for s in sorted_syms[:8]:
                lines.append(f"    â””â”€ [{s['kind']}] {s['name']} (L{s['line']})")
            if len(sorted_syms) > 8:
                lines.append(f"    â””â”€ ... (+{len(sorted_syms)-8} more)")
        lines.append("")

    return "\n".join(lines)
