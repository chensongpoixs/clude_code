import json
import re
import uuid
from pathlib import Path
from typing import Any, Callable, List, Dict, Optional

from clude_code.config.config import CludeConfig
from clude_code.llm.http_client import ChatMessage, ContentPart, LlamaCppHttpClient
from clude_code.llm.model_manager import ModelManager, get_model_manager
from clude_code.observability.audit import AuditLogger
from clude_code.observability.trace import TraceLogger
from clude_code.observability.usage import SessionUsage
from clude_code.observability.logger import get_logger
from clude_code.policy.command_policy import evaluate_command
from clude_code.tooling.feedback import format_feedback_message
from clude_code.tooling.local_tools import LocalTools, ToolResult
from clude_code.knowledge.indexer_service import IndexerService
from clude_code.knowledge.embedder import CodeEmbedder
from clude_code.knowledge.vector_store import VectorStore
from clude_code.verification.runner import Verifier
from clude_code.orchestrator.planner import parse_plan_from_text, render_plan_markdown, Plan
from clude_code.context.claude_standard import get_claude_context_manager
from clude_code.orchestrator.classifier import (
    IntentCategory,
    ClassificationResult,
    IntentClassifier,
)
from clude_code.orchestrator.registry import (
    get_default_registry,
    PromptProfile,
    RiskLevel,
    get_default_profile_for_category
)
# Import AgentState when needed to avoid circular imports
from clude_code.orchestrator.state_m import AgentState

from .models import AgentTurn
from .parsing import try_parse_tool_call
from .prompts import SYSTEM_PROMPT, load_project_memory
from clude_code.prompts import read_prompt, render_prompt
from .tool_lifecycle import run_tool_lifecycle
from .planning import execute_planning_phase
from .execution import (
    check_step_dependencies as _exec_check_step_dependencies,
    handle_tool_call_in_step as _exec_handle_tool_call_in_step,
    execute_single_step_iteration as _exec_execute_single_step_iteration,
    handle_replanning as _exec_handle_replanning,
    execute_final_verification as _exec_execute_final_verification,
    execute_plan_steps as _exec_execute_plan_steps,
)
from .llm_io import (
    llm_chat as _io_llm_chat,
    log_llm_request_params_to_file as _io_log_llm_request_params_to_file,
    log_llm_response_data_to_file as _io_log_llm_response_data_to_file,
    normalize_messages_for_llama as _io_normalize_messages_for_llama,
)
from .react import execute_react_fallback_loop as _react_execute_react_fallback_loop
from .semantic_search import semantic_search as _semantic_search_fn
from .tool_dispatch import dispatch_tool as _dispatch_tool_fn, iter_tool_specs as _iter_tool_specs, render_tools_for_system_prompt


def _try_parse_tool_call(text: str) -> dict[str, Any] | None:
    """
    å…¼å®¹å±‚ï¼šæ—§å‡½æ•°å `_try_parse_tool_call`ã€‚
    # æ–°å®ç°å·²è¿ç§»åˆ° `agent_loop/parsing.py`ï¼Œä¿ç•™æ­¤å…¥å£é¿å…å¤§èŒƒå›´æ”¹åŠ¨ã€‚
    #
    # ä½¿ç”¨ç¤ºä¾‹ï¼š
    # ```python
    # text = "Call tool: read_file path=./file.txt"
    # result = _try_parse_tool_call(text)
    # print(result)
    # ```
    """
    obj = try_parse_tool_call(text)
    if obj is None:
        return None
    if "tool" not in obj or "args" not in obj:
        return None
    if not isinstance(obj["tool"], str) or not isinstance(obj["args"], dict):
        return None
    return obj


def _tool_result_to_message(name: str, tr: ToolResult, keywords: set[str] | None = None) -> str:
    """
    å°†å·¥å…·æ‰§è¡Œç»“æœè½¬æ¢ä¸ºå‘é€ç»™ LLM çš„ç»“æ„åŒ–æ¶ˆæ¯ã€‚
    
    æœ¬å‡½æ•°é‡‡ç”¨ä¸šç•Œæœ€ä½³å®è·µï¼šåªä¿ç•™å†³ç­–å…³é”®å­—æ®µå’Œå¼•ç”¨ï¼Œé¿å…å°†å®Œæ•´ payload å›å–‚ç»™æ¨¡å‹ï¼Œ
    ä»è€Œå‡å°‘ Token æ¶ˆè€—å¹¶æå‡æ¨¡å‹èšç„¦åº¦ã€‚
    
    å‚æ•°:
        name: å·¥å…·åç§°ï¼ˆå¦‚ "read_file", "grep"ï¼‰
        tr: å·¥å…·æ‰§è¡Œç»“æœï¼ˆToolResult å¯¹è±¡ï¼‰
        keywords: å¯é€‰çš„å…³é”®è¯é›†åˆï¼Œç”¨äºè¯­ä¹‰çª—å£é‡‡æ ·ï¼ˆä¼˜å…ˆä¿ç•™åŒ…å«å…³é”®è¯çš„ä»£ç ç‰‡æ®µï¼‰
    
    è¿”å›:
        æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²æ¶ˆæ¯ï¼Œå°†è¢«ä½œä¸º "user" è§’è‰²çš„æ¶ˆæ¯å‘é€ç»™ LLM
    
    æµç¨‹å›¾: è§ `agent_loop_tool_result_to_message_flow.svg`
    """
    # Centralized structured feedback (industry-grade stability):
    # keep decision-critical fields + references, avoid dumping full payload.
    return format_feedback_message(name, tr, keywords=keywords)


class AgentLoop:
    """
    Agent æ ¸å¿ƒå¾ªç¯ç±»ï¼Œå®ç° ReAct (Reasoning + Acting) æ¨¡å¼ã€‚
    
    è´Ÿè´£ï¼š
    - ç®¡ç† LLM å¯¹è¯ä¸Šä¸‹æ–‡
    - è§£æå·¥å…·è°ƒç”¨å¹¶æ‰§è¡Œ
    - ç­–ç•¥æ ¡éªŒï¼ˆç¡®è®¤ã€å‘½ä»¤é»‘åå•ï¼‰
    - å®¡è®¡æ—¥å¿—å’Œè°ƒè¯•è¿½è¸ª
    - ä¸Šä¸‹æ–‡çª—å£ç®¡ç†ï¼ˆå†å²è£å‰ªï¼‰
    - RAG è¯­ä¹‰æœç´¢é›†æˆ
    """
    
    def __init__(self, cfg: CludeConfig, *, session_id: str | None = None) -> None:
        """
        åˆå§‹åŒ– AgentLoop å®ä¾‹ã€‚
        
        åˆå§‹åŒ–æµç¨‹ï¼š
        1. åˆ›å»º LLM å®¢æˆ·ç«¯ï¼ˆllama.cpp HTTPï¼‰
        2. åˆå§‹åŒ–å·¥å…·é›†ï¼ˆLocalToolsï¼‰
        3. åˆå§‹åŒ–å®¡è®¡å’Œè¿½è¸ªæ—¥å¿—
        4. å¯åŠ¨åå°ç´¢å¼•æœåŠ¡ï¼ˆLanceDB RAGï¼‰
        5. ç”Ÿæˆ Repo Mapï¼ˆctagsï¼‰å¹¶æ³¨å…¥ç³»ç»Ÿæç¤ºè¯
        6. æ„å»ºåˆå§‹æ¶ˆæ¯å†å²ï¼ˆä»…åŒ…å« system æ¶ˆæ¯ï¼‰
        
        å‚æ•°:
            cfg: é…ç½®å¯¹è±¡ï¼ˆåŒ…å« LLMã€å·¥ä½œåŒºã€ç­–ç•¥ç­‰é…ç½®ï¼‰
        
        æµç¨‹å›¾: è§ `agent_loop_init_flow.svg`
        """
        self.cfg = cfg
        self.logger = get_logger(
            __name__,
            workspace_root=cfg.workspace_root,
            log_to_console=cfg.logging.log_to_console,
            level=cfg.logging.level,
            log_format=cfg.logging.log_format,
            date_format=cfg.logging.date_format,
        )
        # åˆ›å»ºåªå†™å…¥æ–‡ä»¶çš„ loggerï¼ˆç”¨äºè®°å½• LLM è¯·æ±‚/å“åº”è¯¦æƒ…ï¼‰
        self.file_only_logger = get_logger(
            f"{__name__}.llm_detail",
            workspace_root=cfg.workspace_root,
            log_to_console=False,  # åªå†™å…¥æ–‡ä»¶ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°
            level=cfg.logging.level,
            log_file=cfg.logging.file_path,
            max_bytes=cfg.logging.max_bytes,
            backup_count=cfg.logging.backup_count,
            log_format=cfg.logging.log_format,
            date_format=cfg.logging.date_format,
        )
        # ä¼šè¯ IDï¼šç”¨äº trace/audit å…³è”ã€‚æ”¯æŒä» CLI æ¢å¤ä¼šè¯æ—¶å¤ç”¨æ—§ session_id
        self.session_id = session_id or f"sess_{id(self)}"
        self.logger.info(f"[dim]åˆå§‹åŒ– AgentLoopï¼Œsession_id={self.session_id}[/dim]")
        self.llm = LlamaCppHttpClient(
            base_url=cfg.llm.base_url,
            api_mode=cfg.llm.api_mode,  # type: ignore[arg-type]
            model=cfg.llm.model,
            temperature=cfg.llm.temperature,
            max_tokens=cfg.llm.max_tokens,
            timeout_s=cfg.llm.timeout_s,
            api_key=cfg.llm.api_key,  # æ”¯æŒ OpenAI/Azure ç­‰éœ€è¦è®¤è¯çš„ API
        )
        
        # ç»‘å®šæ¨¡å‹ç®¡ç†å™¨ï¼ˆæ”¯æŒåŠ¨æ€æ¨¡å‹åˆ‡æ¢ï¼‰
        self._model_manager = get_model_manager()
        self._model_manager.bind(self.llm)
        
        self.tools = LocalTools(
            cfg.workspace_root,
            max_file_read_bytes=cfg.limits.max_file_read_bytes,
            max_output_bytes=cfg.limits.max_output_bytes,
        )
        
        # åˆå§‹åŒ–å·¥å…·é…ç½®ï¼ˆç»Ÿä¸€ç®¡ç†ï¼Œä»å…¨å±€é…ç½®æ³¨å…¥ï¼‰
        try:
            from clude_code.config import set_tool_configs
            set_tool_configs(cfg)
            # åˆå§‹åŒ–å¤©æ°”å·¥å…·é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
            from clude_code.tooling.tools.weather import set_weather_config
            set_weather_config(cfg)
        except ImportError:
            pass  # å·¥å…·æ¨¡å—å¯é€‰
        
        self.audit = AuditLogger(cfg.workspace_root, self.session_id)
        self.trace = TraceLogger(cfg.workspace_root, self.session_id)
        self.usage = SessionUsage()
        
        # Knowledge / RAG systems
        self.indexer = IndexerService(cfg)
        self.indexer.start()  # Start background indexing (best-effort)
        if str(getattr(self.indexer, "status", "")).startswith("disabled:"):
            self.logger.info(f"[dim]åå°ç´¢å¼•å·²ç¦ç”¨: {self.indexer.status}[/dim]")
        else:
            self.logger.info("[dim]å¯åŠ¨åå°ç´¢å¼•æœåŠ¡ï¼ˆLanceDB RAGï¼‰[/dim]")
        self.embedder = CodeEmbedder(cfg)
        self.vector_store = VectorStore(cfg)
        self.verifier = Verifier(cfg)
        self.classifier = IntentClassifier(self.llm, file_only_logger=self.file_only_logger)
        
        # Profile Registryï¼ˆæ„å›¾ â†’ Prompt Profile æ˜ å°„ï¼‰
        self.profile_registry = get_default_registry(cfg.workspace_root)
        self._current_profile: PromptProfile | None = None
        self._current_risk_level: RiskLevel = RiskLevel.MEDIUM

        # é˜¶æ®µ C: è¿½è¸ªæœ¬è½®ä¿®æ”¹è¿‡çš„æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºé€‰æ‹©æ€§æµ‹è¯•
        self._turn_modified_paths: set[Path] = set()

        # P0-2: Question å·¥å…·é˜»å¡åè®®çŠ¶æ€
        self._waiting_user_input: bool = False
        self._pending_question: dict[str, Any] | None = None

        # display å·¥å…·éœ€è¦çš„è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼ˆåœ¨ run_turn ä¸­è®¾ç½®ï¼‰
        self._current_ev: Callable[[str, dict[str, Any]], None] | None = None
        self._current_trace_id: str | None = None

        # Initialize with Repo Map for better global context (Aider-style)
        import platform
        raw_repo_map = self.tools.generate_repo_map()
        
        # ç³»ç»Ÿæç¤ºè¯å¤§å°ä¿æŠ¤ï¼šrepo_map æœ€å¤šå ç”¨ 20% çš„ token é¢„ç®—
        MAX_REPO_MAP_CHARS = int(self.llm.max_tokens * 0.2 * 3.5)  # çº¦ 20% token é¢„ç®—
        if len(raw_repo_map) > MAX_REPO_MAP_CHARS:
            # æˆªæ–­ repo_mapï¼Œä¿ç•™å¤´éƒ¨ï¼ˆæ ¸å¿ƒæ–‡ä»¶ï¼‰
            self._repo_map = raw_repo_map[:MAX_REPO_MAP_CHARS] + f"\n... [repo_map å·²æˆªæ–­ï¼ŒåŸé•¿åº¦: {len(raw_repo_map)} chars]"
            self.logger.warning(f"[yellow]repo_map è¿‡å¤§ï¼Œå·²æˆªæ–­: {len(raw_repo_map)} â†’ {MAX_REPO_MAP_CHARS} chars[/yellow]")
        else:
            self._repo_map = raw_repo_map
        
        self._env_info = f"æ“ä½œç³»ç»Ÿ: {platform.system()} ({platform.release()})\nå½“å‰ç»å¯¹è·¯å¾„: {self.cfg.workspace_root}"
        self._tools_section = render_tools_for_system_prompt(include_schema=False)

        # Claude Code å¯¹æ ‡ï¼šè‡ªåŠ¨åŠ è½½ CLUDE.md ä½œä¸ºé¡¹ç›®è®°å¿†ï¼ˆåªè¯»ã€å¤±è´¥ä¸é˜»å¡ï¼‰
        self._project_memory_text, project_memory_meta = load_project_memory(self.cfg.workspace_root)
        self._project_memory_meta: dict[str, object] = project_memory_meta
        self._project_memory_emitted: bool = False

        # åˆå§‹åŒ–æ—¶ä½¿ç”¨é»˜è®¤ System Promptï¼ˆåç»­ä¼šæ ¹æ® Profile åŠ¨æ€æ›´æ–°ï¼‰
        combined_system_prompt = self._build_system_prompt_from_profile(None)
        
        self.messages: list[ChatMessage] = [
            ChatMessage(role="system", content=combined_system_prompt),
        ]
        if bool(project_memory_meta.get("loaded")):
            self.logger.info(f"[dim]å·²åŠ è½½ CLUDE.md é¡¹ç›®è®°å¿†: {project_memory_meta}[/dim]")
        else:
            self.logger.info("[dim]æœªåŠ è½½ CLUDE.mdï¼ˆæœªæ‰¾åˆ°æˆ–ä¸ºç©ºï¼‰[/dim]")
        self.logger.info("[dim]åˆå§‹åŒ–ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å« Repo Map/ç¯å¢ƒä¿¡æ¯/å¯é€‰é¡¹ç›®è®°å¿†ï¼‰[/dim]")
        
        # åˆå§‹åŒ– LLM è¿½è¸ªå±æ€§ï¼ˆç”¨äº observabilityï¼‰
        self._last_llm_stage: str | None = None
        self._last_llm_step_id: str | None = None
        self._last_provider_id: str | None = None
        self._last_provider_base_url: str | None = None
        self._last_provider_model: str | None = None
        self._active_provider_id: str | None = None
        self._active_provider_base_url: str | None = None
        self._active_provider_model: str | None = None
        self._last_logged_system_prompt_hash: str | None = None

    def run_turn(
        self,
        user_text: str,
        *,
        confirm: Callable[[str], bool],
        debug: bool = False,
        on_event: Callable[[dict[str, Any]], None] | None = None,
        images: list[dict[str, Any]] | None = None,
        image_paths: list[str] | None = None,
    ) -> AgentTurn:
        """
        æ‰§è¡Œä¸€è½®å®Œæ•´çš„ Agent å¯¹è¯å¾ªç¯ï¼ˆReAct æ¨¡å¼ï¼‰ã€‚
        
        æ ¸å¿ƒæµç¨‹ï¼š
        1. æ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œæå–å…³é”®è¯ï¼ˆç”¨äºè¯­ä¹‰çª—å£é‡‡æ ·ï¼‰
        2. è¿›å…¥æœ€å¤š 20 æ¬¡çš„å·¥å…·è°ƒç”¨å¾ªç¯ï¼š
           a. è°ƒç”¨ LLM è·å–å“åº”
           b. æ£€æµ‹è¾“å‡ºå¼‚å¸¸ï¼ˆå¤è¯»å­—ç¬¦ï¼‰
           c. è§£æå·¥å…·è°ƒç”¨ JSON
           d. å¦‚æœæ— å·¥å…·è°ƒç”¨ â†’ è¿”å›æœ€ç»ˆæ–‡æœ¬
           e. å¦‚æœæœ‰å·¥å…·è°ƒç”¨ â†’ æ‰§è¡Œç­–ç•¥æ ¡éªŒï¼ˆç¡®è®¤/é»‘åå•ï¼‰
           f. æ‰§è¡Œå·¥å…·å¹¶è·å–ç»“æœ
           g. å°†ç»“æœå›å–‚ç»™ LLMï¼ˆä½œä¸º user æ¶ˆæ¯ï¼‰
           h. è£å‰ªå†å²æ¶ˆæ¯ï¼ˆä¿æŒä¸Šä¸‹æ–‡çª—å£ï¼‰
        3. å¦‚æœè¾¾åˆ°æœ€å¤§å¾ªç¯æ¬¡æ•° â†’ è¿”å›åœæ­¢æ¶ˆæ¯
        
        å‚æ•°:
            user_text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            confirm: ç¡®è®¤å›è°ƒå‡½æ•°ï¼ˆç”¨äºå†™æ–‡ä»¶/æ‰§è¡Œå‘½ä»¤å‰çš„ç”¨æˆ·ç¡®è®¤ï¼‰
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆå†™å…¥ trace.jsonlï¼‰
            on_event: å¯é€‰çš„äº‹ä»¶å›è°ƒï¼ˆç”¨äºå®æ—¶ UI æ›´æ–°ï¼Œå¦‚ --live æ¨¡å¼ï¼‰
            images: å¯é€‰çš„å›¾ç‰‡åˆ—è¡¨ï¼ˆOpenAI Vision API æ ¼å¼ï¼‰
        
        è¿”å›:
            AgentTurn å¯¹è±¡ï¼ŒåŒ…å«æœ€ç»ˆå›å¤ã€å·¥å…·ä½¿ç”¨æ ‡å¿—ã€è¿½è¸ªIDå’Œäº‹ä»¶åˆ—è¡¨
        
        æµç¨‹å›¾: è§ `agent_loop_run_turn_flow.svg`
        """
        # é˜¶æ®µ C: Trace ID å¿…é¡»è·¨è¿›ç¨‹ç¨³å®šä¸”å…¨å±€å”¯ä¸€
        trace_id = f"trace_{uuid.uuid4().hex}"
        self.logger.info(f"[bold cyan]å¼€å§‹æ–°çš„ä¸€è½®å¯¹è¯[/bold cyan] trace_id={trace_id}")
        self.logger.info(f"[dim]ç”¨æˆ·è¾“å…¥: {user_text[:100]}{'...' if len(user_text) > 100 else ''}[/dim]")

        # é˜¶æ®µ C: æ¸…ç©ºæœ¬è½®ä¿®æ”¹è¿½è¸ª
        self._turn_modified_paths.clear()
        # æ³¨ï¼š_llm_log_cursor å·²å¼ƒç”¨ï¼ˆllm_io.py ç°åœ¨ä½¿ç”¨åŸºäºå†…å®¹çš„æ¶ˆæ¯æŸ¥æ‰¾ï¼Œä¸å†ä¾èµ–ç´¢å¼•ï¼‰

        events: list[dict[str, Any]] = []
        step_idx = 0

        def _ev(event: str, data: dict[str, Any]) -> None:
            nonlocal step_idx
            step_idx += 1
            e = {"step": step_idx, "event": event, "data": data}
            events.append(e)
            if debug:
                self.trace.write(trace_id=trace_id, step=step_idx, event=event, data=data)
            if on_event is not None:
                try:
                    # é€ä¼  trace_idï¼šlive UI / TUI / bug report éœ€è¦å¯¹é½åŒä¸€è½® turn çš„å¯è¿½æº¯æ ‡è¯†
                    on_event({**e, "trace_id": trace_id})
                except Exception as ex:
                    self.file_only_logger.warning(f"on_event å›è°ƒå¼‚å¸¸: {ex}", exc_info=True)

        # è®© live UI/TUI èƒ½å±•ç¤º"é»˜è®¤ chat æ—¥å¿—"çš„å¼€åœºè¡Œ
        _ev("turn_start", {"trace_id": trace_id})

        # æå–å…³é”®è¯å¹¶ä¸ŠæŠ¥ï¼ˆç”¨äº UI æ˜¾ç¤º"åˆ†è¯"ï¼‰â€”â€” å¿…é¡»åœ¨ _ev å®šä¹‰ä¹‹åè°ƒç”¨
        keywords = self._extract_keywords(user_text)
        _ev("keywords_extracted", {"keywords": list(keywords)})


        # è®¾ç½®è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ï¼Œä¾› display å·¥å…·ä½¿ç”¨
        self._current_ev = _ev
        self._current_trace_id = trace_id

        # è®°å½•æœ€è¿‘ä¸€æ¬¡ç”¨æˆ·è¾“å…¥çš„å›¾ç‰‡è·¯å¾„ï¼Œä¾›å·¥å…·å›é€€ä½¿ç”¨
        if image_paths:
            self._last_image_paths = list(image_paths)

        # ä»…åœ¨æœ¬ä¼šè¯é¦–æ¬¡ turn å‘å‡ºâ€œé¡¹ç›®è®°å¿†åŠ è½½çŠ¶æ€â€äº‹ä»¶ï¼Œä¾› live UI å±•ç¤º
        if not getattr(self, "_project_memory_emitted", False):
            try:
                _ev("project_memory", dict(getattr(self, "_project_memory_meta", {}) or {}))
            finally:
                self._project_memory_emitted = True

        current_state: AgentState = AgentState.INTAKE

        def _set_state(state: AgentState, info: dict[str, Any] | None = None) -> None:
            nonlocal current_state
            current_state = state
            payload = {"state": state.value}
            if info:
                payload.update(info)
            _ev("state", payload)

        # 1) Intake + Intent åˆ†ç±»ï¼ˆå†³ç­–é—¨ï¼‰
        _set_state(AgentState.INTAKE, {"step": "classifying"})
        enable_planning = self._classify_intent_and_decide_planning(user_text, _ev)
        planning_prompt = self._build_planning_prompt(user_text) if enable_planning else None

        # 2) è®°å½•ç”¨æˆ·è¾“å…¥ï¼ˆå¿…è¦æ—¶æŠŠè§„åˆ’æç¤ºå¹¶å…¥åŒä¸€æ¡ user æ¶ˆæ¯ï¼Œé¿å… role ä¸äº¤æ›¿ï¼‰
        self.audit.write(trace_id=trace_id, event="user_message", data={"text": user_text})
        _ev("user_message", {"text": user_text})
        
        # P0-2: ä½¿ç”¨ Profile æ¸²æŸ“ User Prompt
        # - å¦‚æœæœ‰ planning_promptï¼Œä½¿ç”¨ planning_promptï¼ˆé˜¶æ®µæ¨¡æ¿ï¼‰
        # - å¦åˆ™ä½¿ç”¨ Profile çš„æ„å›¾æ¨¡æ¿æ¸²æŸ“ç”¨æˆ·è¾“å…¥
        if planning_prompt is not None:
            user_content = planning_prompt
        else:
            user_content = self._build_user_prompt_from_profile(
                user_text=user_text,
                planning_prompt="",
            )

        self.logger.info(f"[bold cyan]å‘é€ç»™ LLM çš„ user_content[/bold cyan] len={len(user_content)}")
        # é€ä¼  user_contentï¼ˆç”¨äº"å¯¹è¯/è¾“å‡º"çª—æ ¼å¤åˆ» chat é»˜è®¤æ—¥å¿—ï¼‰
        _ev(
            "user_content_built",
            {
                "preview": user_content[:2000],
                "truncated": len(user_content) > 2000,
                "messages_count": len(self.messages) + 1,  # å³å°† append
                "planning_prompt_included": bool(planning_prompt),
                "has_images": bool(images),
                "images_count": len(images) if images else 0,
            },
        )
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
        if images:
            # å¤šæ¨¡æ€æ¶ˆæ¯ï¼šæ–‡æœ¬ + å›¾ç‰‡
            multimodal_content: list[dict[str, Any]] = [{"type": "text", "text": user_content}]
            multimodal_content.extend(images)
            self.messages.append(ChatMessage(role="user", content=multimodal_content))
            self.logger.info(f"[dim]å·²é™„åŠ  {len(images)} å¼ å›¾ç‰‡åˆ°ç”¨æˆ·æ¶ˆæ¯[/dim]")
        else:
            # çº¯æ–‡æœ¬æ¶ˆæ¯
            self.messages.append(ChatMessage(role="user", content=user_content))
        self._trim_history(max_messages=30)
        self.logger.debug(f"[dim]å½“å‰æ¶ˆæ¯å†å²é•¿åº¦: {len(self.messages)}[/dim]")

        llm_chat = (lambda stage, step_id=None: self._llm_chat(stage, step_id=step_id, _ev=_ev))

        # 3) è§„åˆ’é˜¶æ®µ
        plan: Plan | None = None
        if enable_planning:
            _set_state(AgentState.PLANNING, {"reason": "enable_planning"})
            # ğŸš¨ æ ¸å¿ƒä¿®å¤ï¼šå¢å¼ºè§„åˆ’é˜¶æ®µå¤„ç†
            try:
                plan = execute_planning_phase(self, user_text, planning_prompt, trace_id, _ev, llm_chat)
            except ValueError as planning_error:
                # å¦‚æœè§„åˆ’å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                self.logger.warning(f"[yellow]âš ï¸ æ ‡å‡†è§„åˆ’å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬: {planning_error}[/yellow]")
                
                # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„fallbackè§„åˆ’
                fallback_plan = Plan(
                    title="ç®€åŒ–ä»£ç åˆ†æè®¡åˆ’",
                    steps=[
                        Step(
                            id="step_1",
                            description="ä½¿ç”¨ list_dir å·¥å…·æ‰«æ libcommon ç›®å½•",
                            expected_output="libcommon ç›®å½•ç»“æ„å’Œæ–‡ä»¶åˆ—è¡¨",
                            dependencies=[],
                            tools_expected=["list_dir"]
                        ),
                        Step(
                            id="step_2", 
                            description="ä½¿ç”¨ grep å·¥å…·æœç´¢ libcommon ç›®å½•ä¸­çš„ä»£ç æ–‡ä»¶",
                            expected_output="æ‰¾åˆ°çš„ä»£ç æ–‡ä»¶å’Œå…³é”®ä¿¡æ¯",
                            dependencies=["step_1"],
                            tools_expected=["grep"]
                        ),
                        Step(
                            id="step_3",
                            description="ä½¿ç”¨ read_file å·¥å…·è¯»å–å…³é”®ä»£ç æ–‡ä»¶å†…å®¹",
                            expected_output="ä»£ç æ–‡ä»¶çš„è¯¦ç»†å†…å®¹",
                            dependencies=["step_2"],
                            tools_expected=["read_file"]
                        ),
                        Step(
                            id="step_4",
                            description="åˆ†æä»£ç å¤æ‚åº¦å’Œé‡æ„éœ€æ±‚",
                            expected_output="ä»£ç å¤æ‚åº¦åˆ†æå’Œé‡æ„å»ºè®®",
                            dependencies=["step_3"],
                            tools_expected=["display"]
                        )
                    ],
                    assumptions=["libcommonç›®å½•å­˜åœ¨ä¸”å¯è®¿é—®"],
                    constraints=["åŸºäºå¯ç”¨æ–‡ä»¶è¿›è¡Œåˆ†æ"],
                    risks=["ä»£ç åˆ†æå¯èƒ½ä¸å®Œæ•´"],
                    verification_policy="run_verify"
                )
                
                plan = fallback_plan
                self.logger.info("[green]âœ“ ä½¿ç”¨ç®€åŒ–fallbackè§„åˆ’æˆåŠŸ[/green]")
                plan_summary = render_plan_markdown(plan)
                self.logger.info(f"[dim]ç®€åŒ–è®¡åˆ’æ‘˜è¦:\n{plan_summary}[/dim]")

        # 4) æ‰§è¡Œé˜¶æ®µ
        if plan is not None:
            plan, tool_used, did_modify_code = self._execute_plan_steps(
                plan,
                trace_id,
                keywords,
                confirm,
                events,
                _ev,
                llm_chat,
                _try_parse_tool_call,
                _tool_result_to_message,
                _set_state,
            )

            if plan is None:
                stop_reason = None
                for e in reversed(events):
                    if e.get("event") == "stop_reason":
                        stop_reason = e.get("data", {}).get("reason")
                        break

                if stop_reason == "max_replans_reached":
                    text = "è¾¾åˆ°æœ€å¤§é‡è§„åˆ’æ¬¡æ•°ï¼Œå·²åœæ­¢ã€‚è¯·ç¼©å°ä»»åŠ¡æˆ–æä¾›æ›´æ˜ç¡®çš„å…¥å£æ–‡ä»¶/ç›®æ ‡ã€‚"
                elif stop_reason == "dependency_deadlock":
                    text = "æ£€æµ‹åˆ°ä¾èµ–æ­»é”ï¼šæ‰€æœ‰æœªå®Œæˆæ­¥éª¤éƒ½å¤„äº blocked çŠ¶æ€ã€‚è¯·æ£€æŸ¥è®¡åˆ’ä¸­çš„ä¾èµ–å…³ç³»ã€‚"
                elif stop_reason == "step_not_completed":
                    text = "æ­¥éª¤æœªèƒ½å®Œæˆä¸”æœªè§¦å‘é‡è§„åˆ’ã€‚è¯·ç¼©å°è¯¥æ­¥éª¤æˆ–æä¾›æ›´å¤šçº¦æŸã€‚"
                elif stop_reason == "replan_parse_failed":
                    text = "é‡è§„åˆ’å¤±è´¥ï¼ˆæ— æ³•è§£æ Plan JSONï¼‰ã€‚è¯·æ‰‹åŠ¨æä¾›æ›´æ˜ç¡®çš„æ‹†åˆ†æ­¥éª¤æˆ–å…¥å£æ–‡ä»¶ã€‚"
                else:
                    text = "æ‰§è¡Œé˜¶æ®µæå‰é€€å‡ºã€‚"

                return AgentTurn(assistant_text=text, tool_used=tool_used, trace_id=trace_id, events=events)

            final_result = self._execute_final_verification(plan, did_modify_code, trace_id, tool_used, _ev, _set_state)
            if final_result is not None:
                final_result.events = events
                return final_result

            _set_state(AgentState.DONE, {"ok": True})
            return AgentTurn(
                assistant_text=f"è®¡åˆ’æ‰§è¡Œå®Œæˆï¼š{plan.title}\nï¼ˆå·²æŒ‰æ­¥éª¤æ‰§è¡Œå¹¶å®Œæˆè‡ªæ£€ï¼‰",
                tool_used=tool_used,
                trace_id=trace_id,
                events=events,
            )

        # 5) ReAct fallback
        assistant_text = self._execute_react_fallback_loop(
            trace_id=trace_id,
            keywords=keywords,
            confirm=confirm,
            events=events,
            _ev=_ev,
            _llm_chat=llm_chat,
            _try_parse_tool_call=_try_parse_tool_call,
            _tool_result_to_message=_tool_result_to_message,
            _set_state=_set_state,
        ).assistant_text # è·å– fallback å¾ªç¯çš„æœ€ç»ˆæ–‡æœ¬

        # LLM ç©ºç™½å“åº”çš„æ™ºèƒ½å¤„ç†ï¼šå¦‚æœ LLM è¿”å›ç©ºç™½ï¼Œè¿”å›ä¸€ä¸ªé¢„è®¾çš„å‹å¥½æç¤º
        cleaned_text = assistant_text.strip()
        if not cleaned_text or len(re.sub(r'\s+', '', cleaned_text)) < 5: # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦åå°‘äº5ä¸ªæœ‰æ•ˆå­—ç¬¦
            self.logger.warning(f"[yellow]LLM è¿”å›ç©ºç™½æˆ–è¿‡çŸ­çš„æœ‰æ•ˆå†…å®¹ï¼Œå°†ä½¿ç”¨é¢„è®¾å›å¤ã€‚[/yellow] trace_id={trace_id}")
            assistant_text = "ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ åšçš„å—ï¼Ÿ"
            tool_used = False # é¿å…å°†é¢„è®¾å›å¤æ ‡è®°ä¸ºå·¥å…·ä½¿ç”¨
        else:
            tool_used = False # åœ¨ ReAct fallback ä¹‹å¤–ï¼Œassistant_text ä¸ä»£è¡¨å·¥å…·ä½¿ç”¨

        # 5) è®°å½• LLM æœ€ç»ˆå“åº”
        self.audit.write(trace_id=trace_id, event="assistant_text", data={"text": assistant_text})
        _ev("assistant_text", {"text": assistant_text})
        self.messages.append(ChatMessage(role="assistant", content=assistant_text))
        self._trim_history(max_messages=30)

        return AgentTurn(
            assistant_text=assistant_text,
            tool_used=tool_used,
            trace_id=trace_id,
            events=events,
        )

    def _extract_keywords(self, user_text: str) -> set[str]:
        """æå–ç”¨æˆ·è¾“å…¥ä¸­çš„å…³é”®è¯ï¼ˆç”¨äºè¯­ä¹‰çª—å£é‡‡æ ·ï¼‰ã€‚"""
        keywords = set(re.findall(r'\w{4,}', user_text.lower()))
        keywords -= {"please", "help", "find", "where", "change", "file", "code", "repo", "make"}
        if keywords:
            self.logger.debug(f"[dim]æå–å…³é”®è¯: {keywords}[/dim]")
        return keywords

    def _normalize_messages_for_llama(self, stage: str, *, step_id: str | None = None, _ev: Callable[[str, dict[str, Any]], None] | None = None) -> None:
        return _io_normalize_messages_for_llama(self, stage, step_id=step_id, _ev=_ev)

    def _llm_chat(self, stage: str, *, step_id: str | None = None, _ev: Callable[[str, dict[str, Any]], None] | None = None) -> str:
        return _io_llm_chat(self, stage, step_id=step_id, _ev=_ev)

    """
    è·å–å¯ç”¨äº prompt çš„å·¥å…·ååˆ—è¡¨ï¼ˆPrompt Tool Namesï¼‰
    @author chensongï¼ˆchensongï¼‰
    @date 2026-01-20
    @brief ä» ToolSpec æ³¨å†Œè¡¨æå–â€œå¯è§ä¸”å¯è°ƒç”¨â€çš„å·¥å…·åï¼Œç”¨äº planning prompt çš„ tools_expected æç¤º
    """
    def _get_prompt_tool_names(self) -> list[str]:
        # è¯´æ˜ï¼š
        # - ç»Ÿä¸€ä»¥ tool_dispatch.iter_tool_specs() ä¸ºå•ä¸€äº‹å®æ¥æºï¼ˆé¿å…æ‰‹å†™æ¼å·¥å…·ï¼‰
        # - ä»…æ”¶é›† visible_in_prompt ä¸” callable_by_model çš„å·¥å…·
        # - åšå»é‡ + ç¨³å®šé¡ºåºï¼ˆä¿æŒæ³¨å†Œè¡¨é¡ºåºï¼‰
        try:
            names = [
                s.name
                for s in _iter_tool_specs()
                if getattr(s, "visible_in_prompt", True) and getattr(s, "callable_by_model", True)
            ]
        except Exception:
            # å…œåº•ï¼šå¦‚æœ registry å¼‚å¸¸ï¼Œé¿å… planning é˜¶æ®µå´©æºƒ
            names = ["read_file", "grep", "apply_patch"]

        seen: set[str] = set()
        out: list[str] = []
        for n in names:
            if not n or not isinstance(n, str):
                continue
            if n in seen:
                continue
            seen.add(n)
            out.append(n)
        return out

    """
    æ„å»ºè§„åˆ’é˜¶æ®µæç¤ºè¯ï¼ˆPlanning Prompt Builderï¼‰
    @author chensongï¼ˆchensongï¼‰
    @date 2026-01-20
    @brief ç”Ÿæˆ planning é˜¶æ®µçš„ JSON è§„åˆ’æç¤ºï¼Œå¹¶å°† tools_expected ç¤ºä¾‹è‡ªåŠ¨è¦†ç›–æ‰€æœ‰ç°æœ‰å·¥å…·
    
    æ³¨æ„ï¼ˆNotesï¼‰ï¼š
    - è¿™é‡Œè¿”å›çš„æ˜¯â€œæç¤ºè¯æ–‡æœ¬â€ï¼Œä¸æ˜¯æ¶ˆæ¯å¯¹è±¡ï¼›`run_turn` ä¼šæŠŠå®ƒæ‹¼åˆ°ç”¨æˆ·è¾“å…¥åé¢ä½œä¸ºåŒä¸€æ¡ user æ¶ˆæ¯å‘é€ã€‚
    - tools_expected çš„ç¤ºä¾‹å¿…é¡»åŒ…å«å½“å‰å·¥ç¨‹çš„å…¨éƒ¨å·¥å…·åï¼ˆä»æ³¨å†Œè¡¨æå–ï¼Œé¿å…æ¼é¡¹ï¼‰ã€‚
    """
    def _build_planning_prompt(self, input_text: str) -> str:
        tool_names = self._get_prompt_tool_names()
        tools_expected_example = json.dumps(tool_names, ensure_ascii=False)
        tools_expected_hint = ", ".join(tool_names)
        return render_prompt(
            "user/stage/planning.j2",
            max_plan_steps=int(self.cfg.orchestrator.max_plan_steps),
            tools_expected_example=tools_expected_example,
            tools_expected_hint=tools_expected_hint,
            input_text=input_text,
        )

    def _log_llm_request_params_to_file(self) -> None:
        return _io_log_llm_request_params_to_file(self)

    def _log_llm_response_data_to_file(self, assistant_text: str, tool_call: dict[str, Any] | None) -> None:
        return _io_log_llm_response_data_to_file(self, assistant_text, tool_call)

    def _run_tool_lifecycle(
        self,
        name: str,
        args: dict[str, Any],
        trace_id: str,
        confirm: Callable[[str], bool],
        _ev: Callable[[str, dict[str, Any]], None],
    ) -> ToolResult:
        result = run_tool_lifecycle(self, name, args, trace_id, confirm, _ev)
        
        # P0-2: Question å·¥å…·é˜»å¡åè®®æ£€æµ‹
        # å¦‚æœå·¥å…·è¿”å› type="question" ä¸” status="pending"ï¼Œè®¾ç½®ç­‰å¾…ç”¨æˆ·è¾“å…¥æ ‡å¿—
        if result.ok and isinstance(result.payload, dict):
            payload_type = result.payload.get("type")
            payload_status = result.payload.get("status")
            if payload_type == "question" and payload_status == "pending":
                self._waiting_user_input = True
                self._pending_question = result.payload.get("data")
                self.logger.info("[yellow]â¸ Question å·¥å…·è§¦å‘é˜»å¡ï¼šç­‰å¾…ç”¨æˆ·è¾“å…¥[/yellow]")
                _ev("question_pending", {"question": self._pending_question})
        
        return result

    def _build_system_prompt_from_profile(self, profile: PromptProfile | None) -> str:
        """
        æ ¹æ® Profile åŠ¨æ€æ„å»º System Promptã€‚
        
        å¯¹é½ agent_design_v_1.0.md è®¾è®¡è§„èŒƒï¼š
        - Profile å†³å®š System Prompt ç»„åˆï¼ˆCore + Role + Policy + Contextï¼‰
        - æ”¯æŒé™çº§åˆ°é»˜è®¤ SYSTEM_PROMPT
        
        å‚æ•°:
            profile: Prompt Profileï¼ŒNone è¡¨ç¤ºä½¿ç”¨é»˜è®¤ Prompt
        
        è¿”å›:
            ç»„åˆåçš„ System Prompt æ–‡æœ¬
        """
        if profile is not None:
            try:
                # ä½¿ç”¨ Profile çš„å››å±‚ç»„åˆ
                system_prompt = profile.get_system_prompt(
                    tools_section=self._tools_section,
                    project_memory=self._project_memory_text.strip() if self._project_memory_text else "",
                    env_info=f"{self._env_info}\n\n=== ä»£ç ä»“åº“ç¬¦å·æ¦‚è§ˆ ===\n{self._repo_map}",
                )
                self.logger.debug(f"[dim]ä½¿ç”¨ Profile '{profile.name}' æ„å»º System Prompt[/dim]")
                return system_prompt
            except Exception as e:
                self.logger.warning(f"[yellow]Profile System Prompt æ„å»ºå¤±è´¥: {e}ï¼Œé™çº§ä½¿ç”¨é»˜è®¤[/yellow]")
        
        # é™çº§ï¼šä½¿ç”¨é»˜è®¤ SYSTEM_PROMPT
        combined = (
            f"{SYSTEM_PROMPT}"
            f"{self._project_memory_text}"
            f"\n\n=== ç¯å¢ƒä¿¡æ¯ ===\n{self._env_info}\n\n=== ä»£ç ä»“åº“ç¬¦å·æ¦‚è§ˆ ===\n{self._repo_map}"
        )
        
        # ç³»ç»Ÿæç¤ºè¯æ€»é•¿åº¦ä¿æŠ¤ï¼šæœ€å¤šå ç”¨ 50% çš„ token é¢„ç®—
        MAX_SYSTEM_CHARS = int(self.llm.max_tokens * 0.5 * 3.5)  # çº¦ 50% token é¢„ç®—
        if len(combined) > MAX_SYSTEM_CHARS:
            self.logger.warning(
                f"[yellow]âš  ç³»ç»Ÿæç¤ºè¯è¿‡å¤§: {len(combined)} chars > {MAX_SYSTEM_CHARS} chars (50% token budget)[/yellow]"
            )
            # æˆªæ–­ï¼šä¿ç•™å¤´éƒ¨ + åŸºæœ¬ç¯å¢ƒä¿¡æ¯
            combined = combined[:MAX_SYSTEM_CHARS] + "\n... [ç³»ç»Ÿæç¤ºè¯å·²æˆªæ–­]"
        
        return combined
    
    def _update_system_prompt_for_profile(self, profile: PromptProfile | None) -> None:
        """
        æ›´æ–°æ¶ˆæ¯å†å²ä¸­çš„ System Promptã€‚
        
        å½“ Profile å˜åŒ–æ—¶è°ƒç”¨ï¼Œç¡®ä¿ System Prompt ä¸å½“å‰ Profile ä¸€è‡´ã€‚
        """
        if not self.messages or self.messages[0].role != "system":
            return
        
        new_system_prompt = self._build_system_prompt_from_profile(profile)
        self.messages[0] = ChatMessage(role="system", content=new_system_prompt)
        self.logger.debug("[dim]å·²æ›´æ–° System Prompt[/dim]")
    
    def _build_user_prompt_from_profile(
        self,
        user_text: str,
        planning_prompt: str = "",
    ) -> str:
        """
        æ ¹æ® Profile æ¸²æŸ“ User Promptã€‚
        
        å¯¹é½ agent_design_v_1.0.md è®¾è®¡è§„èŒƒï¼š
        - ç¦æ­¢ç›´æ¥ä½¿ç”¨åŸå§‹ç”¨æˆ·è¾“å…¥ä½œä¸ºæœ€ç»ˆ User Prompt
        - ä½¿ç”¨ Profile çš„æ„å›¾æ¨¡æ¿æ¸²æŸ“ç”¨æˆ·è¾“å…¥
        
        å‚æ•°:
            user_text: åŸå§‹ç”¨æˆ·è¾“å…¥
            planning_prompt: è§„åˆ’åè®®æç¤ºè¯ï¼ˆå¯é€‰ï¼‰
        
        è¿”å›:
            æ¸²æŸ“åçš„ User Prompt æ–‡æœ¬
        """
        profile = self._current_profile
        
        if profile is not None:
            try:
                # è·å–å½“å‰æ„å›¾åç§°
                intent_name = ""
                if hasattr(self, 'classifier') and hasattr(self.classifier, '_last_category'):
                    intent_name = self.classifier._last_category.value if self.classifier._last_category else ""
                
                # ä½¿ç”¨ Profile çš„æ„å›¾æ¨¡æ¿æ¸²æŸ“
                rendered = profile.render_user_prompt(
                    user_text=user_text,
                    planning_prompt=planning_prompt,
                    project_id=getattr(self.cfg, 'project_id', 'default'),
                    intent_name=intent_name or profile.name,
                    risk_level=self._current_risk_level.value,
                )
                self.logger.debug(f"[dim]ä½¿ç”¨ Profile '{profile.name}' æ¸²æŸ“ User Prompt[/dim]")
                return rendered
            except Exception as e:
                self.logger.warning(f"[yellow]Profile User Prompt æ¸²æŸ“å¤±è´¥: {e}ï¼Œé™çº§ä½¿ç”¨åŸå§‹è¾“å…¥[/yellow]")
        
        # é™çº§ï¼šç›´æ¥è¿”å›åŸå§‹ç”¨æˆ·è¾“å…¥
        return user_text

    def _select_profile(self, category: IntentCategory, _ev: Callable[[str, dict[str, Any]], None]) -> PromptProfile | None:
        """
        æ ¹æ®æ„å›¾åˆ†ç±»é€‰æ‹© Prompt Profileã€‚
        
        å¯¹é½ agent_design_v_1.0.md è®¾è®¡è§„èŒƒï¼š
        - Intent â†’ prompt_profile â†’ System/User Prompt ç»„åˆ
        """
        profile_name = get_default_profile_for_category(category)
        profile = self.profile_registry.get(profile_name)
        
        if profile:
            self._current_profile = profile
            self._current_risk_level = profile.risk_level
            self.logger.debug(f"[dim]é€‰æ‹© Profile: {profile_name} (é£é™©ç­‰çº§: {profile.risk_level.value})[/dim]")
            _ev("profile_selected", {
                "profile_name": profile_name,
                "risk_level": profile.risk_level.value,
                "intent_category": category.value,
            })
            # P0-1: æ ¹æ® Profile åŠ¨æ€æ›´æ–° System Prompt
            self._update_system_prompt_for_profile(profile)
        else:
            self.logger.debug(f"[dim]æœªæ‰¾åˆ° Profile: {profile_name}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®[/dim]")
            self._current_profile = None
            self._current_risk_level = RiskLevel.MEDIUM
        
        return profile

    def _classify_intent_and_decide_planning(self, user_text: str, _ev: Callable[[str, dict[str, Any]], None]) -> bool:
        """æ„å›¾åˆ†ç±»å’Œå†³ç­–é—¨ï¼šæ ¹æ®ç”¨æˆ·æ„å›¾å†³å®šæ˜¯å¦å¯ç”¨è§„åˆ’ã€‚"""
        classification = self.classifier.classify(user_text)
        self.logger.info(f"[bold cyan]æ„å›¾è¯†åˆ«ç»“æœ: {classification.category.value}[/bold cyan] (ç½®ä¿¡åº¦: {classification.confidence})")
        _ev("intent_classified", classification.model_dump())
        
        # é€‰æ‹©å¯¹åº”çš„ Prompt Profile
        self._select_profile(classification.category, _ev)

        enable_planning = self.cfg.orchestrator.enable_planning

        # æ–°å¢ï¼šå¤æ‚åº¦æ£€æŸ¥ï¼Œé˜²æ­¢å¤æ‚ä»»åŠ¡è¢«è¯¯åˆ¤ä¸ºGENERAL_CHAT
        if (classification.category == IntentCategory.GENERAL_CHAT and 
            classification.confidence > 0.8 and 
            len(user_text) > 30):
            
# ç®€å•å¤æ‚åº¦è¯„ä¼°ï¼šåŸºäºé•¿åº¦å’Œå…³é”®è¯
            complexity_indicators = ['åˆ›å»º', 'åˆ†æ', 'ä¿®å¤', 'å®ç°', 'è®¾è®¡', 'è°ƒè¯•', 'ç¼–è¯‘', 'éƒ¨ç½²']
            complexity_score = sum(1 for indicator in complexity_indicators if indicator in user_text) / len(complexity_indicators)
            
            # é•¿åº¦å¤æ‚åº¦ï¼šæ¯50ä¸ªå­—ç¬¦å¢åŠ 0.1åˆ†ï¼Œæœ€é«˜0.5åˆ†
            length_complexity = min(len(user_text) / 500.0, 0.5)
            total_complexity = complexity_score + length_complexity
            
            if total_complexity > 0.3:
                self.logger.info(f"[yellow]æ£€æµ‹åˆ°é«˜å¤æ‚åº¦ä»»åŠ¡({total_complexity:.2f})ï¼Œå¼ºåˆ¶å¯ç”¨è§„åˆ’[/yellow]")
                enable_planning = True  # å¼ºåˆ¶å¯ç”¨è§„åˆ’

        if classification.category in (IntentCategory.CAPABILITY_QUERY, IntentCategory.GENERAL_CHAT):
            if enable_planning:
                self.logger.info("[dim]æ£€æµ‹åˆ°èƒ½åŠ›è¯¢é—®æˆ–é€šç”¨å¯¹è¯ï¼Œè·³è¿‡æ˜¾å¼è§„åˆ’é˜¶æ®µã€‚[/dim]")
                _ev(
                    "planning_skipped",
                    {
                        "reason": "capability_query_or_general_chat",
                        "category": classification.category.value,
                        "confidence": classification.confidence,
                    },
                )
                # æ¨¡å¼åˆ¤å®šï¼šèŠå¤©/è§„åˆ’
                enable_planning = False
        # ä¸šç•Œå…œåº•ï¼šçŸ­æ–‡æœ¬ + UNCERTAIN å¾€å¾€æ˜¯é—®å€™/å¯’æš„/æ— ä»»åŠ¡è¾“å…¥ï¼Œä¸åº”è¿›å…¥è§„åˆ’
        # if classification.category == IntentCategory.UNCERTAIN:
        #     txt = (user_text or "").strip()
        #     if len(txt) <= 12 and any(k in txt for k in ("ä½ å¥½", "æ‚¨å¥½", "å“ˆå–½", "å—¨", "hi", "hello", "åœ¨å—")):
        #         self.logger.info("[dim]çŸ­æ–‡æœ¬ç–‘ä¼¼é—®å€™ï¼ˆUNCERTAIN å…œåº•ï¼‰ï¼Œè·³è¿‡æ˜¾å¼è§„åˆ’é˜¶æ®µã€‚[/dim]")
        #         enable_planning = False
        return enable_planning

    def _execute_planning_phase(self, user_text: str, planning_prompt: str | None, trace_id: str, _ev: Callable[[str, dict[str, Any]], None], _llm_chat: Callable[[str, str | None], str]) -> Plan | None:
        return execute_planning_phase(self, user_text, planning_prompt, trace_id, _ev, llm_chat)

    def _check_step_dependencies(self, step, plan: Plan, trace_id: str, _ev: Callable[[str, dict[str, Any]], None]) -> list[str]:
        return _exec_check_step_dependencies(self, step, plan, trace_id, _ev)

    def _handle_tool_call_in_step(
        self,
        name: str,
        args: dict[str, Any],
        step,
        trace_id: str,
        keywords: set[str],
        confirm: Callable[[str], bool],
        _ev: Callable[[str, dict[str, Any]], None],
        _tool_result_to_message: Callable[[str, ToolResult, set[str] | None], str],
    ) -> tuple[ToolResult, bool]:
        return _exec_handle_tool_call_in_step(
            self, name, args, step, trace_id, keywords, confirm, _ev, _tool_result_to_message
        )

    def _execute_single_step_iteration(
        self,
        step,
        step_cursor: int,
        plan: Plan,
        iteration: int,
        trace_id: str,
        keywords: set[str],
        confirm: Callable[[str], bool],
        _ev: Callable[[str, dict[str, Any]], None],
        _llm_chat: Callable[[str, str | None], str],
        _try_parse_tool_call: Callable[[str], dict[str, Any] | None],
        _tool_result_to_message: Callable[[str, ToolResult, set[str] | None], str],
    ) -> tuple[str | None, bool, bool]:
        return _exec_execute_single_step_iteration(
            self,
            step,
            step_cursor,
            plan,
            iteration,
            trace_id,
            keywords,
            confirm,
            _ev,
            _llm_chat,
            _try_parse_tool_call,
            _tool_result_to_message,
        )

    def _handle_replanning(
        self,
        step,
        plan: Plan,
        replans_used: int,
        trace_id: str,
        tool_used: bool,
        _ev: Callable[[str, dict[str, Any]], None],
        _llm_chat: Callable[[str, str | None], str],
        _set_state: Callable[[AgentState, dict[str, Any] | None], None],
    ) -> tuple[Plan | None, int]:
        return _exec_handle_replanning(self, step, plan, replans_used, trace_id, tool_used, _ev, _llm_chat, _set_state)

    def _execute_final_verification(self, plan: Plan, did_modify_code: bool, trace_id: str, tool_used: bool, _ev: Callable[[str, dict[str, Any]], None], _set_state: Callable[[AgentState, dict[str, Any] | None], None]) -> AgentTurn | None:
        return _exec_execute_final_verification(self, plan, did_modify_code, trace_id, tool_used, _ev, _set_state)

    def _execute_react_fallback_loop(
        self,
        trace_id: str,
        keywords: set[str],
        confirm: Callable[[str], bool],
        events: list[dict[str, Any]],
        _ev: Callable[[str, dict[str, Any]], None],
        _llm_chat: Callable[[str, str | None], str],
        _try_parse_tool_call: Callable[[str], dict[str, Any] | None],
        _tool_result_to_message: Callable[[str, ToolResult, set[str] | None], str],
        _set_state: Callable[[AgentState, dict[str, Any] | None], None],
    ) -> AgentTurn:
        return _react_execute_react_fallback_loop(
            self,
            trace_id,
            keywords,
            confirm,
            events,
            _ev,
            _llm_chat,
            _try_parse_tool_call,
            _tool_result_to_message,
            _set_state,
        )

    def _execute_plan_steps(
        self,
        plan: Plan,
        trace_id: str,
        keywords: set[str],
        confirm: Callable[[str], bool],
        events: list[dict[str, Any]],
        _ev: Callable[[str, dict[str, Any]], None],
        _llm_chat: Callable[[str, str | None], str],
        _try_parse_tool_call: Callable[[str], dict[str, Any] | None],
        _tool_result_to_message: Callable[[str, ToolResult, set[str] | None], str],
        _set_state: Callable[[AgentState, dict[str, Any] | None], None],
    ) -> tuple[Plan | None, bool, bool]:
        return _exec_execute_plan_steps(
            self,
            plan,
            trace_id,
            keywords,
            confirm,
            events,
            _ev,
            _llm_chat,
            _try_parse_tool_call,
            _tool_result_to_message,
            _set_state,
        )

    def _trim_history(self, *, max_messages: int) -> None:
        """
        é«˜çº§å¯¹è¯å†å²è£å‰ªï¼Œä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å’Œtokené¢„ç®—ã€‚

        è£å‰ªç­–ç•¥ï¼š
        1. ä½¿ç”¨é«˜çº§ä¸Šä¸‹æ–‡ç®¡ç†å™¨è¿›è¡Œtoken-awareè£å‰ª
        2. ä¼˜å…ˆä¿ç•™é«˜ä¼˜å…ˆçº§å†…å®¹ï¼ˆç³»ç»Ÿæ¶ˆæ¯ã€å½“å‰ä»»åŠ¡ç›¸å…³ï¼‰
        3. æ™ºèƒ½å‹ç¼©é•¿å†…å®¹ä»¥é€‚åº”tokené¢„ç®—
        4. ä¿æŒå¯¹è¯çš„è¿è´¯æ€§å’Œè§’è‰²äº¤æ›¿

        å‚æ•°:
            max_messages: æœ€å¤§ä¿ç•™æ¶ˆæ¯æ•°ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼Œç°ä¸»è¦ä½¿ç”¨tokené¢„ç®—ï¼‰

        æµç¨‹å›¾: è§ `agent_loop_trim_history_flow.svg`
        """
        from clude_code.context.claude_standard import get_claude_context_manager, ContextPriority

        old_len = len(self.messages)
        if old_len <= 1:  # è‡³å°‘ä¿ç•™systemæ¶ˆæ¯
            return

        # ä½¿ç”¨Claude Codeæ ‡å‡†ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        context_manager = get_claude_context_manager(max_tokens=self.llm.max_tokens)

        # æ¸…ç©ºæ—§ä¸Šä¸‹æ–‡ï¼ˆé¿å…é‡å¤ï¼‰
        context_manager.clear_context(keep_protected=False)

        # æ·»åŠ systemæ¶ˆæ¯ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if self.messages and self.messages[0].role == "system":
            system_content = self.messages[0].content or ""
            # å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            if isinstance(system_content, list):
                system_content = "\n".join(
                    item.get("text", "") if isinstance(item, dict) and item.get("type") == "text" else "" 
                    for item in system_content
                )
            context_manager.add_system_context(system_content)

        # æ·»åŠ å¯¹è¯å†å²ï¼ˆæŒ‰ä¼˜å…ˆçº§åˆ†ç±»ï¼‰
        for i, message in enumerate(self.messages[1:], 1):  # è·³è¿‡systemæ¶ˆæ¯
            # æ ¹æ®ä½ç½®å’Œå†…å®¹ç¡®å®šä¼˜å…ˆçº§
            if i >= len(self.messages) - 5:  # æœ€è¿‘5æ¡æ¶ˆæ¯
                priority = ContextPriority.RECENT
            elif i >= len(self.messages) - 15:  # æœ€è¿‘15æ¡æ¶ˆæ¯
                priority = ContextPriority.WORKING
            else:
                priority = ContextPriority.RELEVANT
            
            context_manager.add_message(message, priority)

        # Claude Codeè‡ªåŠ¨å¤„ç†ï¼ˆå·²è§¦å‘auto-compactï¼‰
        optimized_items = context_manager.context_items

        # é‡å»ºæ¶ˆæ¯åˆ—è¡¨
        new_messages = []

        # æ·»åŠ systemæ¶ˆæ¯
        if self.messages and self.messages[0].role == "system":
            new_messages.append(self.messages[0])

        # ä»ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡é¡¹é‡å»ºæ¶ˆæ¯
        for item in optimized_items:
            if item.category == "system":
                continue  # systemæ¶ˆæ¯å·²æ·»åŠ 

            # ä»metadataæ¢å¤åŸå§‹æ¶ˆæ¯
            original_role = item.metadata.get("original_role", item.category)
            message = ChatMessage(role=original_role, content=item.content)
            new_messages.append(message)

        # ç¡®ä¿è‡³å°‘æœ‰æœ€å°æ¶ˆæ¯æ•°
        if len(new_messages) < 3 and len(self.messages) > 3:
            # ä¿ç•™system + æœ€åå‡ æ¡å¯¹è¯
            new_messages = [self.messages[0]] + self.messages[-3:] if len(self.messages) > 3 else self.messages

        self.messages = new_messages

        # è®°å½•è£å‰ªç»Ÿè®¡
        stats = context_manager.get_context_summary()
        self.logger.debug(
            f"[dim]Claude Codeæ ‡å‡†ä¸Šä¸‹æ–‡ä¼˜åŒ–: {old_len} â†’ {len(self.messages)} æ¡æ¶ˆæ¯, "
            f"{stats.get('current_tokens', 0)} tokens ({stats.get('usage_percent', 0):.1%})[/dim]"
        )

    def _format_args_summary(self, tool_name: str, args: dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–å·¥å…·å‚æ•°æ‘˜è¦ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰ã€‚
        
        æ ¹æ®å·¥å…·ç±»å‹æå–å…³é”®å‚æ•°ï¼Œé¿å…è¾“å‡ºè¿‡é•¿ã€‚
        """
        if tool_name == "read_file":
            path = args.get("path", "")
            offset = args.get("offset")
            limit = args.get("limit")
            parts = [f"path={path}"]
            if offset is not None:
                parts.append(f"offset={offset}")
            if limit is not None:
                parts.append(f"limit={limit}")
            return " ".join(parts)
        elif tool_name == "grep":
            pattern = args.get("pattern", "")[:60]
            path = args.get("path", ".")
            return f"pattern={pattern!r} path={path}"
        elif tool_name == "apply_patch":
            path = args.get("path", "")
            expected = args.get("expected_replacements", 1)
            fuzzy = args.get("fuzzy", False)
            return f"path={path} expected={expected} fuzzy={fuzzy}"
        elif tool_name == "write_file":
            path = args.get("path", "")
            text_len = len(args.get("text", ""))
            return f"path={path} text_len={text_len}"
        elif tool_name == "run_cmd":
            cmd = args.get("command", "")[:100]
            cwd = args.get("cwd", ".")
            return f"cmd={cmd!r} cwd={cwd}"
        elif tool_name == "list_dir":
            path = args.get("path", ".")
            return f"path={path}"
        elif tool_name == "glob_file_search":
            pattern = args.get("glob_pattern", "")
            target = args.get("target_directory", ".")
            return f"pattern={pattern} target={target}"
        else:
            # é€šç”¨ï¼šåªæ˜¾ç¤ºå‰ 3 ä¸ªå‚æ•°ï¼Œé¿å…è¿‡é•¿
            items = list(args.items())[:3]
            parts = [f"{k}={str(v)[:50]}" for k, v in items]
            if len(args) > 3:
                parts.append("...")
            return " ".join(parts)

    def _format_result_summary(self, tool_name: str, result: ToolResult) -> str:
        """
        æ ¼å¼åŒ–å·¥å…·æ‰§è¡Œç»“æœæ‘˜è¦ï¼ˆç”¨äºæ—¥å¿—è¾“å‡ºï¼‰ã€‚
        
        æ ¹æ®å·¥å…·ç±»å‹å’Œç»“æœæå–å…³é”®ä¿¡æ¯ï¼Œé¿å…è¾“å‡ºè¿‡é•¿ã€‚
        """
        if not result.ok:
            error_msg = result.error.get("message", str(result.error)) if isinstance(result.error, dict) else str(result.error)
            return f"å¤±è´¥: {error_msg[:100]}"
        
        if not result.payload:
            return "æˆåŠŸï¼ˆæ—  payloadï¼‰"
        
        payload = result.payload
        
        if tool_name == "read_file":
            text_len = len(payload.get("text", ""))
            return f"æˆåŠŸ: è¯»å– {text_len} å­—ç¬¦"
        elif tool_name == "grep":
            hits = payload.get("hits", [])
            count = len(hits)
            truncated = payload.get("truncated", False)
            return f"æˆåŠŸ: æ‰¾åˆ° {count} ä¸ªåŒ¹é…{'ï¼ˆå·²æˆªæ–­ï¼‰' if truncated else ''}"
        elif tool_name == "apply_patch":
            replacements = payload.get("replacements", 0)
            undo_id = payload.get("undo_id", "")
            return f"æˆåŠŸ: {replacements} å¤„æ›¿æ¢ undo_id={undo_id[:20]}"
        elif tool_name == "write_file":
            return "æˆåŠŸ: æ–‡ä»¶å·²å†™å…¥"
        elif tool_name == "run_cmd":
            exit_code = payload.get("exit_code", -1)
            stdout_len = len(payload.get("stdout", ""))
            stderr_len = len(payload.get("stderr", ""))
            return f"æˆåŠŸ: exit_code={exit_code} stdout={stdout_len}å­—ç¬¦ stderr={stderr_len}å­—ç¬¦"
        elif tool_name == "list_dir":
            items = payload.get("items", [])
            count = len(items)
            return f"æˆåŠŸ: {count} é¡¹"
        elif tool_name == "glob_file_search":
            matches = payload.get("matches", [])
            count = len(matches)
            return f"æˆåŠŸ: æ‰¾åˆ° {count} ä¸ªæ–‡ä»¶"
        elif tool_name == "search_semantic":
            hits = payload.get("hits", [])
            count = len(hits)
            return f"æˆåŠŸ: {count} ä¸ªè¯­ä¹‰åŒ¹é…"
        else:
            # é€šç”¨ï¼šæ˜¾ç¤º payload çš„é”®
            #keys = list(payload.keys())[:3]
            #return f"æˆåŠŸ: {', '.join(keys)}{'...' if len(payload) > 3 else ''}"
            keys = list(payload.keys()) if isinstance(payload, dict) else []
            keys_preview = keys[:8]
            more = "â€¦" if len(keys) > len(keys_preview) else ""
            return f"æˆåŠŸ: payload_keys={keys_preview}{more}"

    def _dispatch_tool(self, name: str, args: dict[str, Any]) -> ToolResult:
        """
        æ ¹æ®å·¥å…·åç§°åˆ†å‘åˆ°å¯¹åº”çš„å·¥å…·æ‰§è¡Œå‡½æ•°ã€‚
        
        æ”¯æŒçš„å·¥å…·ï¼š
        - list_dir: åˆ—å‡ºç›®å½•å†…å®¹
        - read_file: è¯»å–æ–‡ä»¶ï¼ˆæ”¯æŒ offset/limitï¼‰
        - glob_file_search: æŒ‰æ¨¡å¼æœç´¢æ–‡ä»¶
        - grep: æ–‡æœ¬æœç´¢ï¼ˆä¼˜å…ˆ ripgrepï¼Œé™çº§ Pythonï¼‰
        - apply_patch: åº”ç”¨ä»£ç è¡¥ä¸ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰
        - undo_patch: å›æ»šè¡¥ä¸ï¼ˆåŸºäº undo_idï¼‰
        - write_file: å†™å…¥æ–‡ä»¶
        - run_cmd: æ‰§è¡Œå‘½ä»¤
        - search_semantic: è¯­ä¹‰æœç´¢ï¼ˆå‘é‡ RAGï¼‰
        
        å‚æ•°:
            name: å·¥å…·åç§°
            args: å·¥å…·å‚æ•°å­—å…¸
        
        è¿”å›:
            ToolResult å¯¹è±¡ï¼ˆåŒ…å« ok/error/payloadï¼‰
        
        å¼‚å¸¸å¤„ç†:
            - KeyError: ç¼ºå°‘å¿…éœ€å‚æ•° â†’ è¿”å› E_INVALID_ARGS
            - å…¶ä»–å¼‚å¸¸: å·¥å…·æ‰§è¡Œå¤±è´¥ â†’ è¿”å› E_TOOL
        
        æµç¨‹å›¾: è§ `agent_loop_dispatch_tool_flow.svg`
        """
        return _dispatch_tool_fn(self, name, args)

    def _semantic_search(self, query: str) -> ToolResult:
        """
        æ‰§è¡Œè¯­ä¹‰æœç´¢ï¼ˆå‘é‡ RAGï¼‰ã€‚
        
        æµç¨‹ï¼š
        1. ä½¿ç”¨ CodeEmbedder å°†æŸ¥è¯¢æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        2. åœ¨ VectorStoreï¼ˆLanceDBï¼‰ä¸­æœç´¢æœ€ç›¸ä¼¼çš„ä»£ç å—ï¼ˆtop 5ï¼‰
        3. å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸º ToolResult
        
        å‚æ•°:
            query: æœç´¢æŸ¥è¯¢æ–‡æœ¬ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
        
        è¿”å›:
            ToolResult å¯¹è±¡ï¼Œpayload åŒ…å«ï¼š
            - query: åŸå§‹æŸ¥è¯¢
            - hits: æœç´¢ç»“æœåˆ—è¡¨ï¼ˆæ¯ä¸ªåŒ…å« path/start_line/end_line/textï¼‰
        
        å¼‚å¸¸å¤„ç†:
            ä»»ä½•å¼‚å¸¸éƒ½ä¼šè¿”å› E_SEMANTIC_SEARCH é”™è¯¯
        
        æµç¨‹å›¾: è§ `agent_loop_semantic_search_flow.svg`
        """
        return _semantic_search_fn(self, query)

    # ============================================================
    # P0-2: Question å·¥å…·é˜»å¡åè®® API
    # ============================================================
    
    def is_waiting_input(self) -> bool:
        """
        æ£€æŸ¥ Agent æ˜¯å¦æ­£åœ¨ç­‰å¾…ç”¨æˆ·è¾“å…¥ã€‚
        
        å½“ question å·¥å…·è¿”å› pending çŠ¶æ€æ—¶ï¼Œæ­¤æ–¹æ³•è¿”å› Trueã€‚
        è°ƒç”¨è€…ï¼ˆCLI/UIï¼‰åº”æ£€æµ‹æ­¤çŠ¶æ€å¹¶æ”¶é›†ç”¨æˆ·è¾“å…¥ã€‚
        """
        return self._waiting_user_input
    
    def get_pending_question(self) -> dict[str, Any] | None:
        """
        è·å–å¾…å›ç­”çš„é—®é¢˜æ•°æ®ã€‚
        
        è¿”å›:
            é—®é¢˜æ•°æ®å­—å…¸ï¼ˆåŒ…å« question/options/multiple/headerï¼‰ï¼Œæˆ– None
        """
        return self._pending_question
    
    def answer_question(self, answer: str | list[str]) -> None:
        """
        æä¾› question å·¥å…·çš„ç­”æ¡ˆï¼Œæ¢å¤ Agent æ‰§è¡Œã€‚
        
        å‚æ•°:
            answer: ç”¨æˆ·çš„å›ç­”ï¼ˆå•é€‰ä¸º strï¼Œå¤šé€‰ä¸º list[str]ï¼‰
        
        è¡Œä¸º:
            1. å°†ç­”æ¡ˆä½œä¸º user æ¶ˆæ¯æ³¨å…¥ messages
            2. æ¸…é™¤ç­‰å¾…æ ‡å¿—
            3. ä¸‹æ¬¡ run_turn æ—¶ Agent å°†çœ‹åˆ°ç­”æ¡ˆå¹¶ç»§ç»­
        """
        if not self._waiting_user_input:
            self.logger.warning("[yellow]answer_question è°ƒç”¨ä½†å½“å‰æœªåœ¨ç­‰å¾…è¾“å…¥[/yellow]")
            return
        
        # æ„å»ºç­”æ¡ˆæ¶ˆæ¯
        if isinstance(answer, list):
            answer_text = f"[ç”¨æˆ·å›ç­”] {', '.join(answer)}"
        else:
            answer_text = f"[ç”¨æˆ·å›ç­”] {answer}"
        
        # æ³¨å…¥åˆ°æ¶ˆæ¯å†å²
        self.messages.append(ChatMessage(role="user", content=answer_text))
        
        # æ¸…é™¤ç­‰å¾…çŠ¶æ€
        self._waiting_user_input = False
        self._pending_question = None
        
        self.logger.info(f"[green]âœ“ æ”¶åˆ°ç”¨æˆ·å›ç­”ï¼Œå·²æ¢å¤æ‰§è¡Œ[/green]")
        self.audit.write(
            trace_id=self._current_trace_id or "question_answer",
            event="question_answered",
            data={"answer": answer_text[:200]},
        )
    
    # ============================================================
    # åŠ¨æ€æ¨¡å‹åˆ‡æ¢ API
    # ============================================================
    
    def switch_model(self, model: str, validate: bool = True) -> tuple[bool, str]:
        """
        åˆ‡æ¢ LLM æ¨¡å‹ã€‚
        
        å‚æ•°:
            model: ç›®æ ‡æ¨¡å‹åç§°/ID
            validate: æ˜¯å¦éªŒè¯æ¨¡å‹å¯ç”¨æ€§ï¼ˆé»˜è®¤ Trueï¼‰
        
        è¿”å›:
            (success, message) å…ƒç»„
        """
        old_model = self.llm.model
        success, message = self._model_manager.switch_model(model, validate)
        
        if success:
            self.logger.info(f"[bold green]æ¨¡å‹å·²åˆ‡æ¢: {old_model} â†’ {model}[/bold green]")
            self.audit.write(
                trace_id="model_switch",
                event="model_switched",
                data={"old_model": old_model, "new_model": model},
            )
        else:
            self.logger.warning(f"[yellow]æ¨¡å‹åˆ‡æ¢å¤±è´¥: {message}[/yellow]")
        
        return success, message
    
    def get_current_model(self) -> str:
        """è·å–å½“å‰ä½¿ç”¨çš„æ¨¡å‹åç§°"""
        return self.llm.model
    
    def list_available_models(self) -> list[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return self._model_manager.list_models()
    
    def rollback_model(self) -> tuple[bool, str]:
        """å›æ»šåˆ°ä¸Šä¸€ä¸ªä½¿ç”¨çš„æ¨¡å‹"""
        return self._model_manager.rollback_model()

