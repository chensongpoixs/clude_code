from __future__ import annotations

import json
from typing import Any, Callable, TYPE_CHECKING

from clude_code.llm.llama_cpp_http import ChatMessage
from clude_code.orchestrator.state_m import AgentState
from clude_code.tooling.local_tools import ToolResult

if TYPE_CHECKING:
    from .agent_loop import AgentLoop
    from .models import AgentTurn


def execute_react_fallback_loop(
    loop: "AgentLoop",
    trace_id: str,
    keywords: set[str],
    confirm: Callable[[str], bool],
    events: list[dict[str, Any]],
    _ev: Callable[[str, dict[str, Any]], None],
    _llm_chat: Callable[[str, str | None], str],
    _try_parse_tool_call: Callable[[str], dict[str, Any] | None],
    _tool_result_to_message: Callable[[str, ToolResult, set[str] | None], str],
    _set_state: Callable[[AgentState, dict[str, Any] | None], None],
) -> "AgentTurn":
    """æ‰§è¡Œ ReAct fallback å¾ªç¯ï¼ˆå•çº§å¾ªç¯ï¼Œæ— è§„åˆ’ï¼‰ã€‚"""
    _set_state(AgentState.EXECUTING, {"mode": "react_fallback"})
    tool_used = False

    for iteration in range(20):  # hard stop to avoid infinite loops
        loop.logger.info(f"[bold yellow]â†’ ç¬¬ {iteration + 1} è½®ï¼šè¯·æ±‚ LLMï¼ˆæ¶ˆæ¯æ•°={len(loop.messages)}ï¼‰[/bold yellow]")
        _ev("llm_request", {"messages": len(loop.messages)})

        try:
            assistant = _llm_chat("react_fallback", None)
        except RuntimeError as e:
            error_msg = str(e)
            if "timeout" in error_msg.lower():
                _ev("llm_error", {"error": "timeout", "message": f"LLM è¯·æ±‚è¶…æ—¶ï¼ˆ{loop.llm.timeout_s}ç§’ï¼‰"})
                loop.logger.error(f"[red]LLM è¯·æ±‚è¶…æ—¶: {error_msg}[/red]")
                from .models import AgentTurn
                return AgentTurn(
                    assistant_text=f"LLM è¯·æ±‚è¶…æ—¶ï¼ˆ{loop.llm.timeout_s}ç§’ï¼‰ã€‚è¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œæˆ–å°è¯•é™ä½ max_tokensï¼ˆå½“å‰: {loop.llm.max_tokens}ï¼‰ã€‚",
                    tool_used=tool_used,
                    trace_id=trace_id,
                    events=events,
                )
            else:
                _ev("llm_error", {"error": "request_failed", "message": error_msg})
                loop.logger.error(f"[red]LLM è¯·æ±‚å¤±è´¥: {error_msg}[/red]")
                from .models import AgentTurn
                return AgentTurn(
                    assistant_text=f"LLM è¯·æ±‚å¤±è´¥: {error_msg}",
                    tool_used=tool_used,
                    trace_id=trace_id,
                    events=events,
                )

        if assistant.count("[") > 50 or assistant.count("{") > 50:
            loop.logger.warning("[red]æ£€æµ‹åˆ°æ¨¡å‹è¾“å‡ºå¼‚å¸¸ï¼ˆå¤è¯»å­—ç¬¦ï¼‰ï¼Œå·²å¼ºåˆ¶æˆªæ–­[/red]")
            assistant = "æ¨¡å‹è¾“å‡ºå¼‚å¸¸ï¼šæ£€æµ‹åˆ°è¿‡å¤šçš„é‡å¤å­—ç¬¦ï¼Œå·²å¼ºåˆ¶æˆªæ–­ã€‚è¯·é‡æ–°æè¿°ä½ çš„éœ€æ±‚æˆ–å°è¯•ç¼©å°ä»»åŠ¡èŒƒå›´ã€‚"
            _ev("stuttering_detected", {"length": len(assistant)})

        _ev("llm_response", {"text": assistant[:4000], "truncated": len(assistant) > 4000})
        loop.logger.debug(f"[dim]LLM å“åº”é•¿åº¦: {len(assistant)} å­—ç¬¦[/dim]")

        tool_call = _try_parse_tool_call(assistant)

        if tool_call is None:
            loop.logger.info("[bold green]âœ“ LLM è¿”å›æœ€ç»ˆå›å¤ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰[/bold green]")
            loop.messages.append(ChatMessage(role="assistant", content=assistant))
            loop.audit.write(trace_id=trace_id, event="assistant_text", data={"text": assistant})
            _ev("final_text", {"text": assistant[:4000], "truncated": len(assistant) > 4000})
            loop._trim_history(max_messages=30)
            from .models import AgentTurn

            return AgentTurn(assistant_text=assistant, tool_used=tool_used, trace_id=trace_id, events=events)

        name = tool_call["tool"]
        args = tool_call["args"]
        args_summary = loop._format_args_summary(name, args)
        loop.logger.info(f"[bold blue]ğŸ”§ è§£æåˆ°å·¥å…·è°ƒç”¨: {name}[/bold blue] [è½®æ¬¡] {iteration + 1}/20 [å‚æ•°] {args_summary}")
        loop.file_only_logger.info(f"å·¥å…·è°ƒç”¨è¯¦æƒ… [iteration={iteration + 1}] [tool={name}] [args={json.dumps(args, ensure_ascii=False)}]")
        _ev("tool_call_parsed", {"tool": name, "args": args})

        clean_assistant = json.dumps(tool_call, ensure_ascii=False)
        loop.messages.append(ChatMessage(role="assistant", content=clean_assistant))
        _ev("assistant_tool_call_recorded", {"tool": name})
        loop._trim_history(max_messages=30)

        result = loop._run_tool_lifecycle(name, args, trace_id, confirm, _ev)
        tool_used = True

        _ev("tool_result", {"tool": name, "ok": result.ok, "error": result.error, "payload": result.payload})

        result_msg = _tool_result_to_message(name, result, keywords=keywords)
        loop.messages.append(ChatMessage(role="user", content=result_msg))
        loop.logger.debug(f"[dim]å·¥å…·ç»“æœå·²å›å–‚[/dim] [å·¥å…·] {name}")
        loop.file_only_logger.debug(f"å·¥å…·ç»“æœå›å–‚ [tool={name}] [len={len(result_msg)}]")
        _ev("tool_result_fed_back", {"tool": name})
        loop._trim_history(max_messages=30)

    loop.logger.warning("[red]âš  è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼ˆ20ï¼‰ï¼Œåœæ­¢ä»¥é¿å…æ­»å¾ªç¯[/red]")
    _ev("stop_reason", {"reason": "max_tool_calls_reached", "limit": 20})
    from .models import AgentTurn

    return AgentTurn(
        assistant_text="è¾¾åˆ°æœ¬è½®æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼ˆ20ï¼‰ï¼Œå·²åœæ­¢ä»¥é¿å…æ­»å¾ªç¯ã€‚è¯·ç¼©å°ä»»åŠ¡æˆ–æä¾›æ›´å¤šçº¦æŸ/å…¥å£æ–‡ä»¶ã€‚",
        tool_used=tool_used,
        trace_id=trace_id,
        events=events,
    )


