# clude-code 响应时间优化与进度指示改进方案

## 1. 当前实现分析

### 1.1 响应时间瓶颈分析

通过对代码的分析，发现以下响应时间瓶颈：

1. **同步阻塞操作**
   - 在 `AgentLoop.run_turn()` 中，LLM 请求是同步的，没有流式处理
   - 工具执行（如 `grep`、`read_file`）是同步的，大文件操作会阻塞
   - 索引构建过程可能阻塞主线程

2. **缺乏进度指示**
   - 长时间操作（如 LLM 请求、大文件读取）没有进度反馈
   - 用户无法知道操作是否正在进行或卡住
   - 只有 Live UI 模式下有部分状态显示，但缺乏细粒度进度

3. **配置复杂性**
   - 配置项多且分散，用户需要手动设置多个环境变量
   - 缺乏智能默认配置和配置向导
   - 配置验证不充分，错误提示不够友好

### 1.2 进度指示实现现状

当前进度指示实现主要在 `LiveDisplay` 类中：

1. **状态指示**
   - 有基本的状态机显示（IDLE、INTAKE、CONTEXT_BUILDING 等）
   - 显示当前活跃组件和步骤
   - 有基本的耗时统计

2. **不足之处**
   - 缺乏细粒度进度（如文件读取百分比、LLM 生成进度）
   - 没有预估剩余时间
   - 长时间操作没有中间状态更新

### 1.3 配置流程现状

当前配置流程主要在 `CludeConfig` 类中：

1. **配置来源**
   - 环境变量（前缀 CLUDE_）
   - 配置文件（手动加载）
   - 默认值

2. **不足之处**
   - 配置项多且分散，缺乏分组和向导
   - 没有配置验证和智能默认值
   - 缺乏配置模板和预设

## 2. 优化方案设计

### 2.1 响应时间优化方案

#### 2.1.1 异步操作优化

1. **LLM 请求流式处理**
   - 实现 LLM 流式响应处理
   - 显示生成进度和已生成内容
   - 支持中断和继续

2. **工具执行异步化**
   - 大文件操作使用异步处理
   - 显示文件读取/处理进度
   - 支持操作取消

3. **后台任务管理**
   - 索引构建完全后台化
   - 实现任务队列和优先级
   - 支持任务状态查询

#### 2.1.2 缓存优化

1. **结果缓存**
   - 缓存常用工具执行结果
   - 实现智能缓存失效策略
   - 支持缓存预热

2. **模型缓存**
   - 缓存 LLM 请求和响应
   - 实现语义相似度匹配
   - 支持缓存统计和管理

### 2.2 进度指示优化方案

#### 2.2.1 细粒度进度指示

1. **操作级别进度**
   - 文件操作：显示读取/写入百分比
   - 搜索操作：显示已扫描文件数和进度
   - LLM 操作：显示生成进度和 token 数

2. **时间估算**
   - 基于历史数据估算操作时间
   - 显示预估剩余时间
   - 动态调整估算精度

3. **多任务进度**
   - 支持并行任务进度显示
   - 任务队列状态可视化
   - 资源使用情况显示

#### 2.2.2 交互式进度控制

1. **操作控制**
   - 支持暂停/继续操作
   - 支持取消长时间操作
   - 支持操作优先级调整

2. **进度反馈**
   - 支持用户进度反馈
   - 根据反馈调整操作策略
   - 记录用户偏好

### 2.3 配置流程简化方案

#### 2.3.1 智能配置向导

1. **配置向导**
   - 交互式配置引导
   - 基于使用场景的预设
   - 配置验证和错误提示

2. **智能默认值**
   - 基于系统环境自动配置
   - 根据硬件性能调整参数
   - 学习用户偏好设置

3. **配置模板**
   - 提供常用配置模板
   - 支持自定义模板
   - 模板分享和导入

#### 2.3.2 配置管理优化

1. **配置分组**
   - 按功能模块分组配置
   - 支持配置文件分离
   - 环境特定配置

2. **配置验证**
   - 实时配置验证
   - 友好错误提示
   - 自动修复建议

3. **配置迁移**
   - 版本升级配置迁移
   - 配置备份和恢复
   - 配置同步和共享

## 3. 实现方案

### 3.1 响应时间优化实现

#### 3.1.1 异步操作框架

```python
# src/clude_code/core/async_manager.py
import asyncio
from typing import Any, Callable, Dict, Optional, Union
from dataclasses import dataclass
from enum import Enum
import time

class TaskStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class TaskProgress:
    task_id: str
    status: TaskStatus
    progress: float  # 0.0 - 1.0
    message: str
    start_time: float
    estimated_end_time: Optional[float] = None
    result: Optional[Any] = None
    error: Optional[Exception] = None

class AsyncTaskManager:
    """异步任务管理器，用于管理长时间运行的操作"""
    
    def __init__(self):
        self.tasks: Dict[str, TaskProgress] = {}
        self.task_futures: Dict[str, asyncio.Task] = {}
    
    async def create_task(
        self,
        task_id: str,
        coro,
        progress_callback: Optional[Callable[[TaskProgress], None]] = None
    ) -> str:
        """创建异步任务"""
        if task_id in self.tasks:
            raise ValueError(f"Task {task_id} already exists")
        
        task_progress = TaskProgress(
            task_id=task_id,
            status=TaskStatus.PENDING,
            progress=0.0,
            message="准备中...",
            start_time=time.time()
        )
        self.tasks[task_id] = task_progress
        
        # 创建包装协程，用于更新进度
        async def wrapped_coro():
            try:
                task_progress.status = TaskStatus.RUNNING
                task_progress.message = "执行中..."
                self._notify_progress(task_progress, progress_callback)
                
                result = await coro
                task_progress.status = TaskStatus.COMPLETED
                task_progress.progress = 1.0
                task_progress.message = "已完成"
                task_progress.result = result
                self._notify_progress(task_progress, progress_callback)
                return result
            except asyncio.CancelledError:
                task_progress.status = TaskStatus.CANCELLED
                task_progress.message = "已取消"
                self._notify_progress(task_progress, progress_callback)
                raise
            except Exception as e:
                task_progress.status = TaskStatus.FAILED
                task_progress.message = f"失败: {str(e)}"
                task_progress.error = e
                self._notify_progress(task_progress, progress_callback)
                raise
        
        task = asyncio.create_task(wrapped_coro())
        self.task_futures[task_id] = task
        return task_id
    
    def _notify_progress(
        self,
        task_progress: TaskProgress,
        progress_callback: Optional[Callable[[TaskProgress], None]]
    ):
        """通知进度更新"""
        if progress_callback:
            progress_callback(task_progress)
    
    def get_progress(self, task_id: str) -> Optional[TaskProgress]:
        """获取任务进度"""
        return self.tasks.get(task_id)
    
    async def cancel_task(self, task_id: str) -> bool:
        """取消任务"""
        if task_id not in self.task_futures:
            return False
        
        future = self.task_futures[task_id]
        future.cancel()
        
        try:
            await future
        except asyncio.CancelledError:
            pass
        
        if task_id in self.tasks:
            self.tasks[task_id].status = TaskStatus.CANCELLED
            self.tasks[task_id].message = "已取消"
        
        return True
    
    async def wait_for_task(self, task_id: str) -> Any:
        """等待任务完成"""
        if task_id not in self.task_futures:
            raise ValueError(f"Task {task_id} not found")
        
        return await self.task_futures[task_id]
```

#### 3.1.2 流式 LLM 客户端

```python
# src/clude_code/llm/streaming_client.py
import asyncio
from typing import AsyncGenerator, Callable, Dict, List, Optional, Union
import json
import httpx
from dataclasses import dataclass

from clude_code.llm.llama_cpp_http import ChatMessage

@dataclass
class StreamChunk:
    content: str
    done: bool
    metadata: Optional[Dict] = None

class StreamingLLMClient:
    """支持流式响应的 LLM 客户端"""
    
    def __init__(
        self,
        base_url: str,
        api_mode: str = "openai_compat",
        model: Optional[str] = None,
        temperature: float = 0.2,
        max_tokens: int = 4096,
        timeout_s: int = 120,
    ):
        self.base_url = base_url
        self.api_mode = api_mode
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout_s = timeout_s
        self.client = httpx.AsyncClient(timeout=timeout_s)
    
    async def chat_stream(
        self,
        messages: List[ChatMessage],
        on_progress: Optional[Callable[[str, float], None]] = None,
    ) -> AsyncGenerator[StreamChunk, None]:
        """流式聊天接口"""
        if self.api_mode == "openai_compat":
            async for chunk in self._openai_compat_stream(messages, on_progress):
                yield chunk
        else:
            # 非流式模式回退到原有实现
            content = await self._non_stream_chat(messages)
            yield StreamChunk(content=content, done=True)
    
    async def _openai_compat_stream(
        self,
        messages: List[ChatMessage],
        on_progress: Optional[Callable[[str, float], None]] = None,
    ) -> AsyncGenerator[StreamChunk, None]:
        """OpenAI 兼容流式接口"""
        url = f"{self.base_url}/v1/chat/completions"
        payload = {
            "model": self.model,
            "messages": [{"role": m.role, "content": m.content} for m in messages],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stream": True,
        }
        
        accumulated_content = ""
        try:
            async with self.client.stream("POST", url, json=payload) as response:
                if response.status_code != 200:
                    error_text = await response.aread()
                    raise Exception(f"LLM API error: {response.status_code} - {error_text}")
                
                async for line in response.aiter_lines():
                    if not line.startswith("data: "):
                        continue
                    
                    data_str = line[6:]  # Remove "data: " prefix
                    if data_str.strip() == "[DONE]":
                        break
                    
                    try:
                        data = json.loads(data_str)
                        if "choices" in data and len(data["choices"]) > 0:
                            delta = data["choices"][0].get("delta", {})
                            if "content" in delta:
                                content = delta["content"]
                                accumulated_content += content
                                
                                # 计算进度（基于 token 估算）
                                progress = min(len(accumulated_content) / (self.max_tokens * 4), 0.95)
                                if on_progress:
                                    on_progress(content, progress)
                                
                                yield StreamChunk(
                                    content=content,
                                    done=False,
                                    metadata=data
                                )
                    except json.JSONDecodeError:
                        continue
            
            # 发送完成信号
            yield StreamChunk(content="", done=True)
        except Exception as e:
            yield StreamChunk(
                content=f"错误: {str(e)}",
                done=True,
                metadata={"error": str(e)}
            )
    
    async def _non_stream_chat(self, messages: List[ChatMessage]) -> str:
        """非流式聊天回退实现"""
        # 这里可以调用原有的同步实现
        # 为了简化，这里只是一个示例
        url = f"{self.base_url}/v1/chat/completions"
        payload = {
            "model": self.model,
            "messages": [{"role": m.role, "content": m.content} for m in messages],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stream": False,
        }
        
        response = await self.client.post(url, json=payload)
        if response.status_code != 200:
            raise Exception(f"LLM API error: {response.status_code} - {response.text}")
        
        data = response.json()
        return data["choices"][0]["message"]["content"]
    
    async def close(self):
        """关闭客户端"""
        await self.client.aclose()
```

### 3.2 进度指示实现

#### 3.2.1 增强的进度显示组件

```python
# src/clude_code/cli/enhanced_live_view.py
import time
from typing import Any, Dict, List, Optional
from dataclasses import dataclass, field
from enum import Enum
from collections import deque

from rich.console import Console, Group
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn
from rich.table import Table
from rich.text import Text
from rich.panel import Panel
from rich.live import Live

class TaskType(Enum):
    LLM_REQUEST = "llm_request"
    FILE_READ = "file_read"
    FILE_WRITE = "file_write"
    SEARCH = "search"
    COMMAND_EXEC = "command_exec"
    INDEXING = "indexing"

@dataclass
class TaskInfo:
    task_id: str
    task_type: TaskType
    description: str
    progress: float = 0.0
    status: str = "running"
    start_time: float = field(default_factory=time.time)
    estimated_end_time: Optional[float] = None
    details: Dict[str, Any] = field(default_factory=dict)

class EnhancedLiveDisplay:
    """增强的实时显示组件，支持细粒度进度指示"""
    
    def __init__(self, console: Console, cfg: Any):
        self.console = console
        self.cfg = cfg
        self.start_time = time.time()
        
        # 任务管理
        self.active_tasks: Dict[str, TaskInfo] = {}
        self.completed_tasks: deque = deque(maxlen=5)
        
        # 进度条组件
        self.progress = Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
            console=console,
            transient=True,
        )
        
        # 状态信息
        self.current_state = "初始化"
        self.current_operation = "等待中"
        self.last_events: deque = deque(maxlen=10)
        
        # 性能统计
        self.operation_times: Dict[str, List[float]] = {}
        
    def add_task(
        self,
        task_id: str,
        task_type: TaskType,
        description: str,
        estimated_duration: Optional[float] = None
    ) -> None:
        """添加新任务"""
        task = TaskInfo(
            task_id=task_id,
            task_type=task_type,
            description=description,
            estimated_end_time=time.time() + estimated_duration if estimated_duration else None
        )
        self.active_tasks[task_id] = task
        
        # 添加到进度条
        self.progress.add_task(
            description=description,
            task_id=task_id,
            total=100.0,
            completed=0.0
        )
        
        self.last_events.append(f"开始任务: {description}")
    
    def update_task(
        self,
        task_id: str,
        progress: float,
        status: Optional[str] = None,
        details: Optional[Dict[str, Any]] = None
    ) -> None:
        """更新任务进度"""
        if task_id not in self.active_tasks:
            return
        
        task = self.active_tasks[task_id]
        task.progress = progress
        if status:
            task.status = status
        if details:
            task.details.update(details)
        
        # 更新进度条
        self.progress.update(task_id, completed=progress)
        
        # 更新预估时间
        if task.estimated_end_time is None and progress > 0.1:
            elapsed = time.time() - task.start_time
            estimated_total = elapsed / progress
            task.estimated_end_time = task.start_time + estimated_total
    
    def complete_task(self, task_id: str, result: Optional[str] = None) -> None:
        """完成任务"""
        if task_id not in self.active_tasks:
            return
        
        task = self.active_tasks[task_id]
        task.progress = 1.0
        task.status = "completed"
        
        # 记录操作时间
        duration = time.time() - task.start_time
        task_type_name = task.task_type.value
        if task_type_name not in self.operation_times:
            self.operation_times[task_type_name] = []
        self.operation_times[task_type_name].append(duration)
        
        # 更新进度条
        self.progress.update(task_id, completed=100.0)
        
        # 移动到已完成任务
        self.completed_tasks.append(task)
        del self.active_tasks[task_id]
        
        self.last_events.append(f"完成任务: {task.description}")
    
    def fail_task(self, task_id: str, error: str) -> None:
        """任务失败"""
        if task_id not in self.active_tasks:
            return
        
        task = self.active_tasks[task_id]
        task.status = f"失败: {error}"
        
        # 更新进度条
        self.progress.update(task_id, completed=task.progress * 100)
        
        # 移动到已完成任务
        self.completed_tasks.append(task)
        del self.active_tasks[task_id]
        
        self.last_events.append(f"任务失败: {task.description} - {error}")
    
    def set_state(self, state: str, operation: str) -> None:
        """设置当前状态和操作"""
        self.current_state = state
        self.current_operation = operation
    
    def on_event(self, event: Dict[str, Any]) -> None:
        """处理事件"""
        event_type = event.get("event", "")
        event_data = event.get("data", {})
        
        if event_type == "llm_request":
            self.add_task(
                task_id=f"llm_{time.time()}",
                task_type=TaskType.LLM_REQUEST,
                description="LLM 请求处理",
                estimated_duration=10.0  # 基于历史数据估算
            )
        elif event_type == "llm_response":
            # 完成最新的 LLM 任务
            llm_tasks = [t for t in self.active_tasks.values() if t.task_type == TaskType.LLM_REQUEST]
            if llm_tasks:
                self.complete_task(llm_tasks[-1].task_id)
        elif event_type == "file_read":
            path = event_data.get("path", "")
            self.add_task(
                task_id=f"read_{time.time()}",
                task_type=TaskType.FILE_READ,
                description=f"读取文件: {path}",
                estimated_duration=2.0
            )
        elif event_type == "file_read_complete":
            read_tasks = [t for t in self.active_tasks.values() if t.task_type == TaskType.FILE_READ]
            if read_tasks:
                self.complete_task(read_tasks[-1].task_id)
        
        self.last_events.append(f"{event_type}: {event_data}")
    
    def render(self) -> Group:
        """渲染完整界面"""
        # 创建状态面板
        status_table = Table(show_header=False, box=None, pad_edge=False)
        status_table.add_column(justify="left", style="bold")
        status_table.add_column(justify="left")
        
        elapsed = int(time.time() - self.start_time)
        status_table.add_row("状态:", self.current_state)
        status_table.add_row("当前操作:", self.current_operation)
        status_table.add_row("运行时间:", f"{elapsed}秒")
        status_table.add_row("活跃任务:", str(len(self.active_tasks)))
        
        status_panel = Panel(status_table, title="系统状态", border_style="blue")
        
        # 创建进度面板
        progress_panel = Panel(
            self.progress,
            title="任务进度",
            border_style="green"
        )
        
        # 创建事件历史面板
        events_table = Table(show_header=False, box=None)
        events_table.add_column("时间", style="dim")
        events_table.add_column("事件")
        
        for event in reversed(list(self.last_events)):
            events_table.add_row(f"{time.strftime('%H:%M:%S')}", event)
        
        events_panel = Panel(
            events_table,
            title="事件历史",
            border_style="yellow"
        )
        
        # 创建性能统计面板
        perf_table = Table(show_header=True, box=None)
        perf_table.add_column("操作类型")
        perf_table.add_column("平均耗时", justify="right")
        perf_table.add_column("次数", justify="right")
        
        for op_type, times in self.operation_times.items():
            if times:
                avg_time = sum(times) / len(times)
                perf_table.add_row(op_type, f"{avg_time:.2f}s", str(len(times)))
        
        perf_panel = Panel(
            perf_table,
            title="性能统计",
            border_style="cyan"
        )
        
        # 组合所有面板
        return Group(
            status_panel,
            progress_panel,
            events_panel,
            perf_panel
        )
```

### 3.3 配置流程简化实现

#### 3.3.1 配置向导

```python
# src/clude_code/cli/config_wizard.py
import os
import platform
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import typer
from rich.console import Console
from rich.prompt import Confirm, Prompt
from rich.table import Table
from rich.panel import Panel
from rich.text import Text

from clude_code.config import CludeConfig, LLMConfig, RAGConfig

console = Console()

class ConfigWizard:
    """配置向导，帮助用户快速配置 clude-code"""
    
    def __init__(self):
        self.config = CludeConfig()
        self.presets = {
            "本地开发": {
                "description": "适合本地开发环境，注重隐私保护",
                "config": {
                    "llm.provider": "llama_cpp_http",
                    "llm.base_url": "http://127.0.0.1:8899",
                    "llm.api_mode": "openai_compat",
                    "policy.allow_network": False,
                    "policy.confirm_write": True,
                    "policy.confirm_exec": True,
                    "rag.enabled": True,
                    "rag.device": "cpu",
                }
            },
            "高性能": {
                "description": "适合高性能环境，最大化处理能力",
                "config": {
                    "llm.max_tokens": 819200,
                    "rag.device": "cuda" if platform.system() != "Darwin" else "mps",
                    "rag.embed_batch_size": 128,
                    "limits.max_file_read_bytes": 2_000_000,
                    "limits.max_output_bytes": 2_000_000,
                }
            },
            "资源受限": {
                "description": "适合资源受限环境，最小化资源使用",
                "config": {
                    "llm.max_tokens": 204800,
                    "rag.device": "cpu",
                    "rag.embed_batch_size": 32,
                    "rag.chunk_size": 300,
                    "limits.max_file_read_bytes": 500_000,
                    "limits.max_output_bytes": 500_000,
                }
            }
        }
    
    def run_wizard(self) -> CludeConfig:
        """运行配置向导"""
        console.print("[bold blue]欢迎使用 clude-code 配置向导![/bold blue]")
        console.print("本向导将帮助您快速配置 clude-code，确保最佳使用体验。\n")
        
        # 检测系统环境
        self._detect_environment()
        
        # 选择预设
        preset = self._select_preset()
        
        # 应用预设
        self._apply_preset(preset)
        
        # 配置 LLM
        self._configure_llm()
        
        # 配置工作区
        self._configure_workspace()
        
        # 验证配置
        self._validate_config()
        
        # 保存配置
        self._save_config()
        
        console.print("\n[bold green]配置完成![/bold green]")
        console.print("您现在可以开始使用 clude-code 了。")
        
        return self.config
    
    def _detect_environment(self) -> None:
        """检测系统环境"""
        console.print("[bold]检测系统环境...[/bold]")
        
        # 系统信息
        system = platform.system()
        machine = platform.machine()
        
        # 内存信息
        try:
            if system == "Linux":
                with open('/proc/meminfo', 'r') as f:
                    for line in f:
                        if line.startswith('MemTotal:'):
                            mem_kb = int(line.split()[1])
                            mem_gb = mem_kb / (1024 * 1024)
                            break
            elif system == "Darwin":  # macOS
                import subprocess
                result = subprocess.run(['sysctl', 'hw.memsize'], capture_output=True, text=True)
                if result.returncode == 0:
                    mem_bytes = int(result.stdout.split()[1])
                    mem_gb = mem_bytes / (1024 * 1024 * 1024)
            elif system == "Windows":
                import psutil
                mem_bytes = psutil.virtual_memory().total
                mem_gb = mem_bytes / (1024 * 1024 * 1024)
            else:
                mem_gb = 8  # 默认值
        except:
            mem_gb = 8  # 默认值
        
        # GPU 检测
        has_gpu = False
        gpu_type = None
        try:
            if system == "Linux":
                result = subprocess.run(['nvidia-smi'], capture_output=True)
                if result.returncode == 0:
                    has_gpu = True
                    gpu_type = "cuda"
            elif system == "Darwin":
                # macOS Apple Silicon
                if machine in ("arm64", "arm64e"):
                    has_gpu = True
                    gpu_type = "mps"
        except:
            pass
        
        # 显示检测结果
        env_table = Table(show_header=True, box=None)
        env_table.add_column("项目", style="bold")
        env_table.add_column("值")
        
        env_table.add_row("操作系统", f"{system} ({machine})")
        env_table.add_row("内存", f"{mem_gb:.1f} GB")
        env_table.add_row("GPU", "是" if has_gpu else "否")
        if has_gpu and gpu_type:
            env_table.add_row("GPU 类型", gpu_type)
        
        console.print(Panel(env_table, title="环境检测结果"))
        
        # 根据环境调整默认配置
        if mem_gb < 8:
            console.print("[yellow]检测到内存较小，将使用资源受限配置。[/yellow]")
            self.config.limits.max_file_read_bytes = 500_000
            self.config.limits.max_output_bytes = 500_000
            self.config.rag.embed_batch_size = 32
        
        if has_gpu:
            console.print(f"[green]检测到 {gpu_type} GPU，将启用 GPU 加速。[/green]")
            self.config.rag.device = gpu_type
    
    def _select_preset(self) -> str:
        """选择配置预设"""
        console.print("\n[bold]选择配置预设:[/bold]")
        
        # 显示预设选项
        preset_table = Table(show_header=True, box=None)
        preset_table.add_column("序号", style="bold")
        preset_table.add_column("预设", style="bold")
        preset_table.add_column("描述")
        
        for i, (name, info) in enumerate(self.presets.items(), 1):
            preset_table.add_row(str(i), name, info["description"])
        
        console.print(preset_table)
        
        while True:
            choice = Prompt.ask("请选择预设 (1-3)", default="1")
            if choice in ("1", "2", "3"):
                preset_names = list(self.presets.keys())
                return preset_names[int(choice) - 1]
            console.print("[red]无效选择，请输入 1-3。[/red]")
    
    def _apply_preset(self, preset_name: str) -> None:
        """应用配置预设"""
        preset = self.presets[preset_name]
        console.print(f"\n[bold]应用预设: {preset_name}[/bold]")
        
        for key, value in preset["config"].items():
            # 解析嵌套键，如 "llm.provider" -> config.llm.provider
            parts = key.split('.')
            obj = self.config
            for part in parts[:-1]:
                obj = getattr(obj, part)
            setattr(obj, parts[-1], value)
        
        console.print(f"[green]已应用预设: {preset_name}[/green]")
    
    def _configure_llm(self) -> None:
        """配置 LLM"""
        console.print("\n[bold]配置 LLM:[/bold]")
        
        # 基础 URL
        current_url = self.config.llm.base_url
        new_url = Prompt.ask("LLM 基础 URL", default=current_url)
        self.config.llm.base_url = new_url
        
        # 模型
        current_model = self.config.llm.model
        new_model = Prompt.ask("模型名称", default=current_model)
        self.config.llm.model = new_model
        
        # API 模式
        api_modes = ["openai_compat", "completion"]
        console.print("可用 API 模式:")
        for i, mode in enumerate(api_modes, 1):
            console.print(f"{i}. {mode}")
        
        while True:
            choice = Prompt.ask("选择 API 模式 (1-2)", default="1")
            if choice in ("1", "2"):
                self.config.llm.api_mode = api_modes[int(choice) - 1]
                break
            console.print("[red]无效选择，请输入 1-2。[/red]")
        
        # 测试连接
        if Confirm.ask("测试 LLM 连接?", default=True):
            self._test_llm_connection()
    
    def _configure_workspace(self) -> None:
        """配置工作区"""
        console.print("\n[bold]配置工作区:[/bold]")
        
        # 工作区路径
        current_path = self.config.workspace_root
        new_path = Prompt.ask("工作区路径", default=current_path)
        self.config.workspace_root = new_path
        
        # 确认工作区可访问
        workspace_path = Path(new_path)
        if not workspace_path.exists():
            console.print(f"[red]路径不存在: {new_path}[/red]")
            if Confirm.ask("创建工作区目录?", default=True):
                workspace_path.mkdir(parents=True, exist_ok=True)
                console.print(f"[green]已创建: {new_path}[/green]")
            else:
                console.print("[yellow]请确保工作区路径存在。[/yellow]")
    
    def _validate_config(self) -> None:
        """验证配置"""
        console.print("\n[bold]验证配置...[/bold]")
        
        errors = []
        
        # 验证工作区
        workspace_path = Path(self.config.workspace_root)
        if not workspace_path.exists():
            errors.append("工作区路径不存在")
        elif not workspace_path.is_dir():
            errors.append("工作区路径不是目录")
        
        # 验证 LLM 配置
        if not self.config.llm.base_url:
            errors.append("LLM 基础 URL 未设置")
        
        if not self.config.llm.model:
            errors.append("LLM 模型未设置")
        
        # 验证 RAG 配置
        if self.config.rag.enabled:
            rag_db_path = Path(self.config.rag.db_path)
            if not rag_db_path.parent.exists():
                try:
                    rag_db_path.parent.mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    errors.append(f"无法创建 RAG 数据库目录: {e}")
        
        # 显示验证结果
        if errors:
            console.print("[red]配置验证失败:[/red]")
            for error in errors:
                console.print(f"- {error}")
            
            if not Confirm.ask("是否继续保存配置?", default=False):
                raise typer.Abort()
        else:
            console.print("[green]配置验证通过![/green]")
    
    def _save_config(self) -> None:
        """保存配置"""
        console.print("\n[bold]保存配置...[/bold]")
        
        # 创建配置目录
        config_dir = Path.home() / ".clude"
        config_dir.mkdir(exist_ok=True)
        
        # 保存配置文件
        config_file = config_dir / "config.json"
        
        # 转换为字典并保存
        config_dict = self.config.model_dump()
        
        import json
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        
        console.print(f"[green]配置已保存到: {config_file}[/green]")
        
        # 显示环境变量设置提示
        console.print("\n[bold]环境变量设置:[/bold]")
        console.print("您也可以通过以下环境变量覆盖配置:")
        console.print("- CLUDE_WORKSPACE_ROOT: 工作区路径")
        console.print("- CLUDE_LLM__BASE_URL: LLM 基础 URL")
        console.print("- CLUDE_LLM__MODEL: LLM 模型")
        console.print("- CLUDE_LLM__API_MODE: LLM API 模式")
    
    def _test_llm_connection(self) -> None:
        """测试 LLM 连接"""
        console.print("[bold]测试 LLM 连接...[/bold]")
        
        try:
            from clude_code.llm.llama_cpp_http import LlamaCppHttpClient, ChatMessage
            
            client = LlamaCppHttpClient(
                base_url=self.config.llm.base_url,
                api_mode=self.config.llm.api_mode,
                model=self.config.llm.model,
                temperature=0.0,
                max_tokens=10,
                timeout_s=10,
            )
            
            response = client.chat([
                ChatMessage(role="system", content="你是测试助手，只回复 OK。"),
                ChatMessage(role="user", content="测试"),
            ])
            
            console.print(f"[green]LLM 连接成功! 响应: {response}[/green]")
        except Exception as e:
            console.print(f"[red]LLM 连接失败: {e}[/red]")
            console.print("[yellow]请检查 LLM 服务是否正在运行，以及配置是否正确。[/yellow]")

def run_config_wizard() -> None:
    """运行配置向导"""
    wizard = ConfigWizard()
    wizard.run_wizard()
```

## 4. 集成实现

### 4.1 更新 AgentLoop 以支持异步操作

```python
# src/clude_code/orchestrator/agent_loop/enhanced_agent_loop.py
import asyncio
from typing import Any, Callable, Dict, List, Optional, Union
import time

from clude_code.config import CludeConfig
from clude_code.core.async_manager import AsyncTaskManager, TaskStatus
from clude_code.llm.streaming_client import StreamingLLMClient, StreamChunk
from clude_code.cli.enhanced_live_view import EnhancedLiveDisplay, TaskType
from .agent_loop import AgentLoop
from .models import AgentTurn

class EnhancedAgentLoop(AgentLoop):
    """增强的 AgentLoop，支持异步操作和细粒度进度指示"""
    
    def __init__(self, cfg: CludeConfig) -> None:
        super().__init__(cfg)
        self.async_manager = AsyncTaskManager()
        self.streaming_client = StreamingLLMClient(
            base_url=cfg.llm.base_url,
            api_mode=cfg.llm.api_mode,
            model=cfg.llm.model,
            temperature=cfg.llm.temperature,
            max_tokens=cfg.llm.max_tokens,
            timeout_s=cfg.llm.timeout_s,
        )
        self.enhanced_display: Optional[EnhancedLiveDisplay] = None
    
    def set_enhanced_display(self, display: EnhancedLiveDisplay) -> None:
        """设置增强显示组件"""
        self.enhanced_display = display
    
    async def run_turn_async(
        self,
        user_text: str,
        *,
        confirm: Callable[[str], bool],
        debug: bool = False,
        on_event: Callable[[dict[str, Any]], None] = None,
    ) -> AgentTurn:
        """异步执行一轮 Agent 对话循环"""
        # 创建任务ID
        turn_id = f"turn_{int(time.time())}"
        
        # 添加主任务
        await self.async_manager.create_task(
            task_id=turn_id,
            coro=self._run_turn_core_async(user_text, confirm, debug, on_event, turn_id),
            progress_callback=self._on_task_progress
        )
        
        # 等待任务完成
        result = await self.async_manager.wait_for_task(turn_id)
        return result
    
    async def _run_turn_core_async(
        self,
        user_text: str,
        confirm: Callable[[str], bool],
        debug: bool,
        on_event: Callable[[dict[str, Any]], None],
        turn_id: str
    ) -> AgentTurn:
        """异步执行核心逻辑"""
        # 提取关键词
        keywords = self._extract_keywords(user_text)
        
        # 更新状态
        self._update_state("INTAKE", "处理用户输入")
        
        # 添加用户消息
        self.messages.append(ChatMessage(role="user", content=user_text))
        if on_event:
            on_event({"event": "user_message", "data": {"text": user_text}})
        
        # 执行工具调用循环
        for iteration in range(20):  # 最大20次迭代
            # 更新状态
            self._update_state("THINKING", f"思考中 (轮次 {iteration + 1})")
            
            # 创建 LLM 请求任务
            llm_task_id = f"llm_{turn_id}_{iteration}"
            await self.async_manager.create_task(
                task_id=llm_task_id,
                coro=self._llm_request_async(keywords, on_event),
                progress_callback=self._on_task_progress
            )
            
            # 等待 LLM 响应
            response = await self.async_manager.wait_for_task(llm_task_id)
            
            # 检查是否有工具调用
            tool_call = self._try_parse_tool_call(response)
            if not tool_call:
                # 没有工具调用，返回最终响应
                self._update_state("DONE", "完成")
                return AgentTurn(
                    trace_id=turn_id,
                    final_response=response,
                    tool_used=False,
                    events=[]
                )
            
            # 执行工具调用
            tool_name = tool_call["tool"]
            tool_args = tool_call["args"]
            
            # 更新状态
            self._update_state("EXECUTING", f"执行工具: {tool_name}")
            
            # 创建工具执行任务
            tool_task_id = f"tool_{turn_id}_{iteration}"
            await self.async_manager.create_task(
                task_id=tool_task_id,
                coro=self._execute_tool_async(tool_name, tool_args, confirm, on_event),
                progress_callback=self._on_task_progress
            )
            
            # 等待工具执行完成
            tool_result = await self.async_manager.wait_for_task(tool_task_id)
            
            # 将结果回喂给 LLM
            result_msg = _tool_result_to_message(tool_name, tool_result, keywords)
            self.messages.append(ChatMessage(role="user", content=result_msg))
            
            if on_event:
                on_event({
                    "event": "tool_result",
                    "data": {
                        "tool": tool_name,
                        "ok": tool_result.ok,
                        "error": tool_result.error,
                        "payload": tool_result.payload
                    }
                })
        
        # 达到最大迭代次数
        self._update_state("DONE", "达到最大迭代次数")
        return AgentTurn(
            trace_id=turn_id,
            final_response="达到最大迭代次数，请简化任务或重试。",
            tool_used=True,
            events=[]
        )
    
    async def _llm_request_async(
        self,
        keywords: set[str],
        on_event: Optional[Callable[[dict[str, Any]], None]]
    ) -> str:
        """异步 LLM 请求"""
        # 通知开始 LLM 请求
        if self.enhanced_display:
            self.enhanced_display.add_task(
                task_id=f"llm_{time.time()}",
                task_type=TaskType.LLM_REQUEST,
                description="LLM 请求处理",
                estimated_duration=10.0
            )
        
        if on_event:
            on_event({"event": "llm_request", "data": {"messages": len(self.messages)}})
        
        # 流式处理响应
        accumulated_response = ""
        
        async def on_progress(content: str, progress: float):
            nonlocal accumulated_response
            accumulated_response += content
            
            # 更新进度
            if self.enhanced_display:
                # 找到活跃的 LLM 任务并更新
                llm_tasks = [
                    t for t in self.enhanced_display.active_tasks.values()
                    if t.task_type == TaskType.LLM_REQUEST
                ]
                if llm_tasks:
                    self.enhanced_display.update_task(
                        llm_tasks[-1].task_id,
                        progress=progress,
                        details={"generated_length": len(accumulated_response)}
                    )
        
        # 发送流式请求
        async for chunk in self.streaming_client.chat_stream(
            self.messages,
            on_progress=on_progress
        ):
            if chunk.done:
                break
        
        # 完成 LLM 任务
        if self.enhanced_display:
            llm_tasks = [
                t for t in self.enhanced_display.active_tasks.values()
                if t.task_type == TaskType.LLM_REQUEST
            ]
            if llm_tasks:
                self.enhanced_display.complete_task(llm_tasks[-1].task_id)
        
        if on_event:
            on_event({
                "event": "llm_response",
                "data": {"text": accumulated_response}
            })
        
        return accumulated_response
    
    async def _execute_tool_async(
        self,
        tool_name: str,
        tool_args: dict[str, Any],
        confirm: Callable[[str], bool],
        on_event: Optional[Callable[[dict[str, Any]], None]]
    ) -> ToolResult:
        """异步执行工具"""
        # 确定任务类型和描述
        task_type = TaskType.FILE_READ
        description = f"执行工具: {tool_name}"
        estimated_duration = 2.0
        
        if tool_name == "read_file":
            task_type = TaskType.FILE_READ
            path = tool_args.get("path", "")
            description = f"读取文件: {path}"
        elif tool_name == "write_file":
            task_type = TaskType.FILE_WRITE
            path = tool_args.get("path", "")
            description = f"写入文件: {path}"
        elif tool_name == "grep":
            task_type = TaskType.SEARCH
            pattern = tool_args.get("pattern", "")
            description = f"搜索: {pattern}"
        elif tool_name == "run_cmd":
            task_type = TaskType.COMMAND_EXEC
            command = tool_args.get("command", "")
            description = f"执行命令: {command}"
        
        # 添加任务
        if self.enhanced_display:
            self.enhanced_display.add_task(
                task_id=f"tool_{time.time()}",
                task_type=task_type,
                description=description,
                estimated_duration=estimated_duration
            )
        
        # 执行工具
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self._run_tool_sync(tool_name, tool_args, confirm)
            )
            
            # 完成任务
            if self.enhanced_display:
                tool_tasks = [
                    t for t in self.enhanced_display.active_tasks.values()
                    if t.task_type == task_type
                ]
                if tool_tasks:
                    self.enhanced_display.complete_task(tool_tasks[-1].task_id)
            
            return result
        except Exception as e:
            # 任务失败
            if self.enhanced_display:
                tool_tasks = [
                    t for t in self.enhanced_display.active_tasks.values()
                    if t.task_type == task_type
                ]
                if tool_tasks:
                    self.enhanced_display.fail_task(tool_tasks[-1].task_id, str(e))
            
            return ToolResult(
                ok=False,
                error={"code": "EXECUTION_ERROR", "message": str(e)}
            )
    
    def _run_tool_sync(
        self,
        tool_name: str,
        tool_args: dict[str, Any],
        confirm: Callable[[str], bool]
    ) -> ToolResult:
        """同步执行工具（在 executor 中运行）"""
        # 这里调用原有的工具执行逻辑
        return self._run_tool_lifecycle(tool_name, tool_args, "trace_id", confirm, lambda e, d: None)
    
    def _update_state(self, state: str, operation: str) -> None:
        """更新状态"""
        if self.enhanced_display:
            self.enhanced_display.set_state(state, operation)
    
    def _on_task_progress(self, task_progress) -> None:
        """任务进度回调"""
        # 这里可以添加额外的进度处理逻辑
        pass
    
    def _extract_keywords(self, text: str) -> set[str]:
        """提取关键词"""
        # 简单的关键词提取实现
        import re
        words = re.findall(r'\b\w+\b', text.lower())
        # 过滤掉短词和常见词
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their'}
        keywords = {w for w in words if len(w) >= 3 and w not in stop_words}
        return keywords
```

### 4.2 更新 CLI 以支持增强功能

```python
# src/clude_code/cli/enhanced_chat_handler.py
import asyncio
from typing import Any
import typer
from rich.console import Console
from rich.live import Live
from rich.prompt import Confirm, Prompt

from clude_code.config import CludeConfig
from clude_code.orchestrator.agent_loop.enhanced_agent_loop import EnhancedAgentLoop
from clude_code.cli.enhanced_live_view import EnhancedLiveDisplay
from clude_code.cli.utils import select_model_interactively

console = Console()

class EnhancedChatHandler:
    """
    增强的聊天处理器，支持异步操作和细粒度进度指示
    """
    def __init__(self, cfg: CludeConfig, logger: Any, file_only_logger: Any):
        self.cfg = cfg
        self.logger = logger
        self.file_only_logger = file_only_logger
        self.agent = EnhancedAgentLoop(cfg)

    def select_model_interactively(self) -> None:
        """调用公共工具进行交互式模型选择。"""
        select_model_interactively(self.cfg, self.logger)

    def run_loop(self, debug: bool, live: bool) -> None:
        """主交互循环。"""
        self.logger.info("[bold]进入 clude chat (增强模式)[/bold]")
        self.logger.info("- 输入 `exit` 退出")
        self.logger.info("- 输入 `config` 运行配置向导")

        while True:
            user_text = typer.prompt("you")
            if user_text.strip().lower() in {"exit", "quit"}:
                self.logger.info("bye")
                break
            
            if user_text.strip().lower() == "config":
                from clude_code.cli.config_wizard import run_config_wizard
                run_config_wizard()
                continue

            if live:
                asyncio.run(self._run_with_live_async(user_text, debug=True))
            else:
                asyncio.run(self._run_simple_async(user_text, debug=debug))

    async def _run_with_live_async(self, user_text: str, debug: bool) -> None:
        """带增强实时面板的异步执行模式。"""
        display = EnhancedLiveDisplay(console, self.cfg)
        self.agent.set_enhanced_display(display)

        def _confirm(msg: str) -> bool:
            return Confirm.ask(msg, default=False)

        with Live(display.render(), console=console, refresh_per_second=4, transient=False) as live_view:
            self._log_turn_start(user_text, debug=True, live=True)
            try:
                def on_event_wrapper(e: dict):
                    display.on_event(e)
                    try:
                        live_view.update(display.render())
                    except Exception:
                        pass

                turn = await self.agent.run_turn_async(user_text, confirm=_confirm, debug=True, on_event=on_event_wrapper)
                self._log_turn_end(turn)
                
                # 结束后固定状态
                display.set_state("DONE", "本轮完成")
                live_view.update(display.render())
                
                self._print_assistant_response(turn, debug=True, show_trace=True)
            except Exception as e:
                self.logger.error(f"AgentLoop 运行异常 (Live): {e}", exc_info=True)
                self.file_only_logger.exception("AgentLoop 运行异常 (Live)")
                raise typer.Exit(code=1)

    async def _run_simple_async(self, user_text: str, debug: bool) -> None:
        """普通命令行输出模式（异步）。"""
        self._log_turn_start(user_text, debug=debug, live=False)
        try:
            def _confirm(msg: str) -> bool:
                return Confirm.ask(msg, default=False)

            turn = await self.agent.run_turn_async(user_text, confirm=_confirm, debug=debug)
            self._log_turn_end(turn)
            self._print_assistant_response(turn, debug=debug, show_trace=not debug)
        except Exception as e:
            self.logger.error(f"AgentLoop 运行异常 (Simple): {e}", exc_info=True)
            self.file_only_logger.exception("AgentLoop 运行异常 (Simple)")
            raise typer.Exit(code=1)

    def _log_turn_start(self, user_text: str, debug: bool, live: bool) -> None:
        self.file_only_logger.info(
            f"Turn Start (Async) - input: {user_text[:100]}..., debug={debug}, live={live}, "
            f"model={self.cfg.llm.model}"
        )

    def _log_turn_end(self, turn: Any) -> None:
        self.file_only_logger.info(
            f"Turn End (Async) - trace_id={turn.trace_id}, tool_used={turn.tool_used}, events={len(turn.events)}"
        )

    def _print_assistant_response(self, turn: Any, debug: bool, show_trace: bool) -> None:
        """打印助手响应。"""
        console.print(f"\n[bold]助手:[/bold] {turn.final_response}")
        
        if show_trace and hasattr(turn, 'events') and turn.events:
            console.print("\n[bold dim]执行轨迹:[/bold dim]")
            for event in turn.events[-5:]:  # 只显示最后5个事件
                console.print(f"  - {event.get('event', 'unknown')}: {event.get('data', {})}")
```

## 5. 使用示例

### 5.1 配置向导使用

```bash
# 运行配置向导
clude config-wizard

# 或在聊天中输入
clude chat
you: config
```

### 5.2 增强聊天模式

```bash
# 启动增强聊天模式
clude chat --live

# 示例对话
you: 帮我分析 src/clude_code 目录下的代码结构
# 将显示详细的进度指示，包括文件读取、分析等步骤的进度
```

## 6. 总结

本方案通过以下三个方面的改进，显著提升了 clude-code 的用户体验：

1. **响应时间优化**
   - 引入异步操作框架，避免长时间操作阻塞
   - 实现流式 LLM 处理，实时显示生成进度
   - 添加智能缓存，减少重复操作

2. **进度指示增强**
   - 实现细粒度进度指示，包括操作级别进度
   - 添加时间估算和性能统计
   - 支持交互式进度控制

3. **配置流程简化**
   - 提供智能配置向导，自动检测环境
   - 实现配置预设和模板
   - 增强配置验证和错误提示

这些改进使 clude-code 更加用户友好，特别是对于新用户，大大降低了使用门槛。同时，细粒度的进度指示和异步操作提升了用户体验，让用户能够更好地了解和控制操作进度。